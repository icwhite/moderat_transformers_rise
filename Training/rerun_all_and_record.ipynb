{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classification_classes import * \n",
    "\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "import wandb\n",
    "import argparse\n",
    "import re\n",
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "MODEL_NAME_OR_PATH = \"bert-base-german-cased\"\n",
    "DATASETS = [\"RP-Crowd-3\", \"RP-Crowd-2\", \"RP-Mod\"]\n",
    "DATASET_PATHS = [\"./Datasets/RP-Crowd-3-folds.csv\", \"./Datasets/RP-Crowd-2-folds.csv\", \"./Datasets/RP-Mod-folds.csv\", \\\n",
    "    \"./Datasets/resampled/200shap-folds.csv\"]\n",
    "# DATASET_PATH = \"/home/dobby/Datasets/resampled/200shap-folds.csv\"\n",
    "# WANDB_PROJECT_NAME = f\"{MODEL_NAME_OR_PATH}-all-datasets\"\n",
    "OUTPUT_DIR = f\"./german-bert-results/\"\n",
    "\n",
    "TUNING_LEARNING_RATE = False\n",
    "model_name = \"german-bert\"\n",
    "model_class = BertFineTuner\n",
    "\n",
    "CONFIGS = [\n",
    "    {\n",
    "        \"model_class\": Enc1T5,\n",
    "        \"learning_rate\": 0.0001,\n",
    "        \"weight_decay\": 0.1, \n",
    "        \"model_name_or_path\": \"GermanT5/t5-efficient-oscar-german-small-el32\"\n",
    "    }, \n",
    "    {\n",
    "        \"model_class\": BertFineTuner,\n",
    "        \"learning_rate\": 0.0001, \n",
    "        \"weight_decay\": 0.1, \n",
    "        \"model_name_or_path\": \"bert-base-german-cased\"\n",
    "    }, \n",
    "    {\n",
    "        \"model_class\": T5FineTuner, \n",
    "        \"learning_rate\": 0.0001,\n",
    "        \"weight_decay\": 0.1,\n",
    "        \"model_name_or_path\": \"GermanT5/t5-efficient-oscar-german-small-el32\"\n",
    "    }, \n",
    "    {\n",
    "        \"model_class\": T5FineTuner, \n",
    "        \"learning_rate\": 0.0001,\n",
    "        \"weight_decay\": 0.1,\n",
    "        \"model_name_or_path\": \"google/mt5-small\"\n",
    "    },\n",
    "    {\n",
    "        \"model_class\": T5FineTuner, \n",
    "        \"learning_rate\": 0.0001,\n",
    "        \"weight_decay\": 0.1,\n",
    "        \"model_name_or_path\": \"google/mt5-base\"\n",
    "    },\n",
    "    {\n",
    "        \"model_class\": T5FineTuner, \n",
    "        \"learning_rate\": 0.0001,\n",
    "        \"weight_decay\": 0.1,\n",
    "        \"model_name_or_path\": \"GermanT5/german-t5-oscar-ep1-prompted-germanquad\"\n",
    "    },\n",
    "    {\n",
    "        \"model_class\": BertFineTuner,\n",
    "        #TODO: change these\n",
    "        \"learning_rate\": 0.0001, \n",
    "        \"weight_decay\": 0.1,\n",
    "        \"model_name_or_path\": \"xlm-roberta-base\"\n",
    "    }\n",
    "]\n",
    "\n",
    "for config in CONFIGS:\n",
    "    # pick output_dir\n",
    "    # set args to the right values\n",
    "    # initialize wandb project names\n",
    "    # and wandb logger & checkpoint logger & early stop callback\n",
    "    # set summary metrics\n",
    "\n",
    "    # for i in range(num_times):\n",
    "    # train model\n",
    "    # keep track of max accuracy, f1, recall, precision\n",
    "    # figure out how to get the run time of the model\n",
    "\n",
    "\n",
    "\n",
    "#TODO: set up the configs for running on different datasets with different classes\n",
    "#TODO: how do we get the summaries for each\n",
    "#TODO: save checkpoints of the models as well in a folder?\n",
    "#TODO: somehow keep track of the time that each model took to train?\n",
    "for source in DATASET_PATHS:\n",
    "        train_inputs, train_targets, val_inputs, val_targets = get_folds_classification(source)\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME_OR_PATH)\n",
    "\n",
    "        train_dataset = RPClassificationDataset(tokenizer, train_inputs, train_targets)\n",
    "        valid_dataset = RPClassificationDataset(tokenizer, val_inputs, val_targets)\n",
    "        args_dict = dict(\n",
    "                model_name_or_path=MODEL_NAME_OR_PATH,\n",
    "                gradient_accumulation_steps=16,\n",
    "                weight_decay=0.1,\n",
    "                learning_rate=1e-5,\n",
    "                adam_epsilon=1e-8,\n",
    "                adam_betas=(0.9,0.999),\n",
    "                num_train_epochs=30,\n",
    "                n_gpu=1,\n",
    "                train_batch_size=8,\n",
    "                eval_batch_size=8,\n",
    "                data_dir=\"\", # path for data files\n",
    "                output_dir=OUTPUT_DIR, # path to save the checkpoints\n",
    "                dataset_name=dataset,\n",
    "                max_seq_length=512,\n",
    "                early_stop_callback=True,\n",
    "                fp_16=False, # if you want to enable 16-bit training then install apex and set this to true\n",
    "                opt_level='O1', # you can find out more on optimisation levels here https://nvidia.github.io/apex/amp.html#opt-levels-and-properties\n",
    "                max_grad_norm=0.5, # if you enable 16-bit training then set this to a sensible value, 0.5 is a good default\n",
    "                seed=42,\n",
    "                train_dataset=train_dataset,\n",
    "                val_dataset=valid_dataset,\n",
    "                warmup_steps=0\n",
    "                )\n",
    "        args = argparse.Namespace(**args_dict)\n",
    "        args.auto_lr_find = \"learning_rate\"\n",
    "\n",
    "        train_params = dict(\n",
    "                accumulate_grad_batches=args.gradient_accumulation_steps,\n",
    "                auto_lr_find=True,\n",
    "                gpus=args.n_gpu,\n",
    "                max_epochs=args.num_train_epochs,\n",
    "                precision= 16 if args.fp_16 else 32,\n",
    "                amp_level=args.opt_level,\n",
    "                gradient_clip_val=args.max_grad_norm,\n",
    "                # enable_checkpointing=checkpoint_callback,\n",
    "                callbacks=[],\n",
    "                # callbacks=[EarlyStopping(monitor=\"val/accuracy\", patience=5, mode=\"max\")],\n",
    "                # callbacks=[raytuner_callback],\n",
    "                # callbacks=[LoggingCallback()],\n",
    "                amp_backend=\"apex\"\n",
    "                )\n",
    "        wandb_project_name = f\"{dataset}-hyperparameter-search-{model_name}\"\n",
    "        possible_weight_decays = [0.1]\n",
    "        early_stop_callback = EarlyStopping(monitor=\"val_accuracy\", patience=3, mode=\"max\")\n",
    "        for wd in possible_weight_decays:\n",
    "                if TUNING_LEARNING_RATE:\n",
    "                        \n",
    "                        args.weight_decay = wd\n",
    "                        model = BertFineTuner(args)\n",
    "                        init_trainer = pl.Trainer(**train_params)\n",
    "                        print(\"*\" * 100)\n",
    "                        print(f\"{dataset} Learning Rate Tuning\")\n",
    "                        lr_finder = init_trainer.tuner.lr_find(model)\n",
    "                        # print(lr_finder.results)\n",
    "                        fig = lr_finder.plot(suggest=True)\n",
    "                        fig.show()\n",
    "                        new_lr = lr_finder.suggestion()\n",
    "                        print(f\"Best Learning Rate is: {new_lr}\")\n",
    "\n",
    "                        # update with the best learning rate\n",
    "                        possible_learning_rates = [1e-4, new_lr, 1e-5]\n",
    "                        # possible_learning_rates = np.power(10, rand.uniform(-6, np.log10(new_lr) + 1, 3))\n",
    "                else:\n",
    "                        possible_learning_rates = [1e-4]\n",
    "                \n",
    "                for lr in possible_learning_rates:\n",
    "                        config = {\n",
    "                                \"learning_rate\": lr,\n",
    "                                \"weight_decay\": wd, \n",
    "                                \"num_train_epochs\": 20\n",
    "                        }\n",
    "\n",
    "                        run_name = \"\"\n",
    "                        for key in config.keys():\n",
    "                                run_name += f\"-{key}-{config[key]}\"\n",
    "                                args_dict[key] = config[key]\n",
    "                        args = argparse.Namespace(**args_dict)\n",
    "\n",
    "                        # get train params and update with wandb logger, checkpoint callback, and early stopping callback\n",
    "                        early_stop_callback = EarlyStopping(monitor=\"val_accuracy\", patience=1, mode=\"max\")\n",
    "                        wandb.finish()\n",
    "                                                \n",
    "                        wandb_logger = WandbLogger(project=wandb_project_name, \n",
    "                                name=run_name)\n",
    "                        wandb.define_metric(\"val_accuracy\", summary=\"max\")\n",
    "\n",
    "                        checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "                                                        dirpath=args.output_dir + \"/\" + run_name, \n",
    "                                                        filename=\"{epoch}-{val_accuracy:.2f}-{val_loss:.2f}\", \n",
    "                                                        monitor=\"val_accuracy\", mode=\"max\", save_top_k=5\n",
    "                                                        )\n",
    "                        \n",
    "                        train_params[\"logger\"] = wandb_logger\n",
    "                        train_params[\"callbacks\"] = [early_stop_callback, checkpoint_callback]\n",
    "\n",
    "                        model = model_class(args)\n",
    "                        trainer = pl.Trainer(**train_params)\n",
    "                        trainer.fit(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jun 30 14:40:19 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 511.79       Driver Version: 511.79       CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   49C    P8    13W /  N/A |   1607MiB /  8192MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      8080      C   ...nda3\\envs\\gpu1\\python.exe    N/A      |\n",
      "|    0   N/A  N/A     23952      C   ...nda3\\envs\\gpu1\\python.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'11.3'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.version.cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1.], device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_ones = torch.ones(3)\n",
    "x_ones.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/dobby/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "[nltk_data] Downloading package punkt to /home/dobby/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from T5FineTuner import RPDataset\n",
    "from utils import get_folds\n",
    "import torch\n",
    "import argparse\n",
    "from transformers import T5Tokenizer, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import wandb\n",
    "\n",
    "DATASETS = [\"RP-Crowd-3\", \"RP-Crowd-2\", \"RP-Mod\"]\n",
    "# suffix = \"german-t5-oscar-ep1-prompted-germanquad\"\n",
    "model_names = [\"t5-efficient-gc4-german-small-el32\", \"t5-base-german-3e\"]\n",
    "# MODEL_NAME_OR_PATH = f\"GermanT5/{suffix}\"\n",
    "# WANDB_PROJECT_NAME = f\"all-datasets-{suffix}\"\n",
    "# OUTPUT_DIR = f\"./{suffix}\"\n",
    "\n",
    "# auto_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME_OR_PATH)\n",
    "# auto_model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME_OR_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/dobby/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import random\n",
    "import re\n",
    "from itertools import chain\n",
    "from string import punctuation\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import csv\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# eval packages\n",
    "import textwrap\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn import metrics\n",
    "\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from transformers import (\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "\n",
    "def set_seed(seed):\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  torch.manual_seed(seed)\n",
    "  if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "\n",
    "\n",
    "class T5FineTuner(pl.LightningModule):\n",
    "  def __init__(self, hparams):\n",
    "    super(T5FineTuner, self).__init__()\n",
    "    # self.hparams.save_hyperparameters(hparams)\n",
    "    self.save_hyperparameters(hparams)\n",
    "    \n",
    "    self.model = hparams.model\n",
    "    self.tokenizer = hparams.tokenizer\n",
    "    # self.train_dataset = train_dataset\n",
    "    # self.val_dataset = val_dataset\n",
    "    \n",
    "  def is_logger(self):\n",
    "    return self.trainer.global_rank <= 0\n",
    "  \n",
    "\n",
    "  def forward(\n",
    "      self, input_ids, attention_mask=None, decoder_input_ids=None, decoder_attention_mask=None, labels=None\n",
    "  ):\n",
    "    return self.model(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        decoder_input_ids=decoder_input_ids,\n",
    "        decoder_attention_mask=decoder_attention_mask,\n",
    "        labels=labels,\n",
    "        # lm_labels=lm_labels,\n",
    "    )\n",
    "\n",
    "  def _step(self, batch):\n",
    "    labels = batch[\"target_ids\"]\n",
    "    labels[labels[:, :] == self.tokenizer.pad_token_id] = -100\n",
    "\n",
    "    outputs = self(\n",
    "        input_ids=batch[\"source_ids\"],\n",
    "        attention_mask=batch[\"source_mask\"],\n",
    "        labels=labels,\n",
    "        # lm_labels=lm_labels,\n",
    "        decoder_attention_mask=batch['target_mask']\n",
    "    )\n",
    "\n",
    "    loss = outputs[0]\n",
    "\n",
    "    return loss\n",
    "  \n",
    "  def get_accuracy(self, batch):\n",
    "    labels = batch[\"target_ids\"]\n",
    "\n",
    "    outs = self.model.generate(input_ids=batch[\"source_ids\"], \n",
    "                                attention_mask=batch[\"source_mask\"], \n",
    "                                max_length=2)\n",
    "\n",
    "    dec = [self.tokenizer.decode(ids) for ids in outs]\n",
    "    target = [self.tokenizer.decode(label) for label in labels]\n",
    "\n",
    "    new_outputs = [s[6:] for s in dec]\n",
    "    new_targets = [s[:-4] for s in target]\n",
    "\n",
    "    accuracy_score = metrics.accuracy_score(new_targets, new_outputs)\n",
    "    f1 = metrics.f1_score(new_targets, new_outputs, labels=[\"problematisch\"], average=None)\n",
    "    recall = metrics.recall_score(new_targets, new_outputs, labels=[\"problematisch\"], average=None)\n",
    "    precision = metrics.precision_score(new_targets, new_outputs, labels=[\"problematisch\"], average=None)\n",
    "    # rec = metrics.recall_score(new_targets, new_outputs, average=\"micros\")\n",
    "    return accuracy_score, f1, recall, precision\n",
    "  # def computer_accuracy(self):\n",
    "  #   self.model.model.eval()\n",
    "\n",
    "  def training_step(self, batch, batch_idx):\n",
    "    loss = self._step(batch)\n",
    "    self.log(\"train/loss\", loss)\n",
    "    return {\"loss\": loss}\n",
    "  \n",
    "  def training_epoch_end(self, outputs):\n",
    "    avg_train_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n",
    "    tensorboard_logs = {\"avg_train_loss\": avg_train_loss}\n",
    "    self.log(\"avg_train_loss\", avg_train_loss)\n",
    "    \n",
    "  def validation_step(self, batch, batch_idx):\n",
    "    \n",
    "    loss = self._step(batch)\n",
    "    accuracy, f1, recall, precision = [torch.tensor(val) for val in self.get_accuracy(batch)]\n",
    "    self.log(\"val/loss\", loss, logger=True)\n",
    "    self.log(\"val/accuracy\", accuracy, logger=True)\n",
    "    self.log(\"val/f1\", f1, logger=True)\n",
    "    self.log(\"val/recall\", recall, logger=True)\n",
    "    self.log(\"val/precision\", precision, logger=True)\n",
    "    return {\"val/loss\": loss, \"val/accuracy\": accuracy, \"val/f1\": f1, \"val/recall\": recall, \"val/precision\": precision}\n",
    "  \n",
    "  def validation_epoch_end(self, outputs):\n",
    "    avg_loss = torch.stack([x[\"val/loss\"] for x in outputs]).mean()\n",
    "    avg_accuracy = torch.stack([x[\"val/accuracy\"] for x in outputs]).mean()\n",
    "    avg_f1 = torch.stack([x[\"val/f1\"] for x in outputs]).mean()\n",
    "    avg_recall = torch.stack([x[\"val/recall\"] for x in outputs]).mean()\n",
    "    avg_precision = torch.stack([x[\"val/precision\"] for x in outputs]).mean()\n",
    "    # tensorboard_logs = {\"val_loss\": avg_loss}\n",
    "    self.log(\"avg_val_loss\", avg_loss)\n",
    "    self.log(\"avg_val_accuracy\", avg_accuracy)\n",
    "    self.log(\"avg_val_f1\", avg_f1)\n",
    "    self.log(\"avg_val_recall\", avg_recall)\n",
    "    self.log(\"avg_val_precision\", avg_precision)\n",
    "    # self.log(\"log\", tensorboard_logs)\n",
    "    # self.log(\"progress_bar\", tensorboard_logs)\n",
    "    # self.log({\"avg_val_loss\": avg_loss, \n",
    "    #           \"log\": tensorboard_logs,\n",
    "    #           'progress_bar': tensorboard_logs}, logger=True, prog_bar=True)\n",
    "    return {\"avg_val_loss\": avg_loss, \"avg_val_accuracy\": avg_accuracy}\n",
    "\n",
    "  def configure_optimizers(self):\n",
    "    \"Prepare optimizer and schedule (linear warmup and decay)\"\n",
    "\n",
    "    model = self.model\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": self.hparams.weight_decay,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=self.hparams.learning_rate, \\\n",
    "                      eps=self.hparams.adam_epsilon, betas=self.hparams.adam_betas)\n",
    "    self.opt = optimizer\n",
    "    return [optimizer]\n",
    "  \n",
    "  def optimizer_step(self, \n",
    "                      epoch,\n",
    "                      batch_idx,\n",
    "                      optimizer,\n",
    "                      optimizer_idx,\n",
    "                      second_order_closure=None,\n",
    "                      on_tpu=None,\n",
    "                      using_native_amp=None,\n",
    "                      using_lbfgs=None):\n",
    "    # if self.trainer.use_tpu:\n",
    "    #   xm.optimizer_step(optimizer)\n",
    "    # else:\n",
    "    optimizer.step(closure=second_order_closure)\n",
    "    optimizer.zero_grad()\n",
    "    self.lr_scheduler.step()\n",
    "  \n",
    "  # def closure(self):\n",
    "  #   return \"closure\"\n",
    "  \n",
    "  def get_tqdm_dict(self):\n",
    "    tqdm_dict = {\"loss\": \"{:.3f}\".format(self.trainer.avg_loss), \"lr\": self.lr_scheduler.get_last_lr()[-1]}\n",
    "\n",
    "    return tqdm_dict\n",
    "\n",
    "  def train_dataloader(self):\n",
    "    # train_dataset = get_dataset(tokenizer=self.tokenizer, type_path=\"train\", args=self.hparams)\n",
    "    dataloader = DataLoader(self.hparams.train_dataset, batch_size=self.hparams.train_batch_size, drop_last=True, shuffle=True, num_workers=4)\n",
    "    t_total = (\n",
    "        (len(dataloader.dataset) // (self.hparams.train_batch_size * max(1, self.hparams.n_gpu)))\n",
    "        // self.hparams.gradient_accumulation_steps\n",
    "        * float(self.hparams.num_train_epochs)\n",
    "    )\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        self.opt, num_warmup_steps=self.hparams.warmup_steps, num_training_steps=t_total\n",
    "    )\n",
    "    self.lr_scheduler = scheduler\n",
    "    return dataloader\n",
    "\n",
    "  def val_dataloader(self):\n",
    "    # val_dataset = get_dataset(tokenizer=self.tokenizer, type_path=\"val\", args=self.hparams)\n",
    "    return DataLoader(self.hparams.val_dataset, batch_size=self.hparams.eval_batch_size, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up Automatic Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading a Flax model in PyTorch, requires both PyTorch and Flax to be installed. Please see https://pytorch.org/ and https://flax.readthedocs.io/en/latest/installation.html for installation instructions.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "jax requires jaxlib to be installed. See https://github.com/google/jax#installation for installation instructions.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/jax/_src/lib/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m   \u001b[0;32mimport\u001b[0m \u001b[0mjaxlib\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mjaxlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mModuleNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'jaxlib'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8276/1244964071.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m             )\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mauto_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForSeq2SeqLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_NAME_OR_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_flax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             args_dict = dict(\n\u001b[1;32m     37\u001b[0m                     \u001b[0mdata_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# path for data files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    444\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m             \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m         raise ValueError(\n\u001b[1;32m    448\u001b[0m             \u001b[0;34mf\"Unrecognized configuration class {config.__class__} for this kind of AutoModel: {cls.__name__}.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2042\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mfrom_flax\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2043\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2044\u001b[0;31m                 \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmodeling_flax_pytorch_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_flax_checkpoint_in_pytorch_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2046\u001b[0m                 \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_flax_checkpoint_in_pytorch_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresolved_archive_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/modeling_flax_pytorch_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/jax/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# We want the exported object to be the class, so we first import the module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# to make sure a later import doesn't overwrite the class.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_config_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0m_config_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/jax/config.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# TODO(phawkins): fix users of this alias and delete this file.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/jax/_src/config.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mabsl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjax_jit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransfer_guard_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/jax/_src/lib/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m   \u001b[0;32mimport\u001b[0m \u001b[0mjaxlib\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mjaxlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mModuleNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m   raise ModuleNotFoundError(\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0;34m'jax requires jaxlib to be installed. See '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;34m'https://github.com/google/jax#installation for installation instructions.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: jax requires jaxlib to be installed. See https://github.com/google/jax#installation for installation instructions."
     ]
    }
   ],
   "source": [
    "DATASETS = [\"RP-Crowd-3\", \"RP-Crowd-2\", \"RP-Mod\"]\n",
    "# suffix = \"german-t5-oscar-ep1-prompted-germanquad\"\n",
    "model_names = [\"t5-efficient-oscar-german-small-el32\"]\n",
    "learning_rates = [1e-4, 4e-4, 5.6e-5]\n",
    "\n",
    "for suffix in model_names:\n",
    "    MODEL_NAME_OR_PATH = \"GermanT5/t5-base-german-3e\"\n",
    "    WANDB_PROJECT_NAME = f\"all-datasets-{suffix}\"\n",
    "    OUTPUT_DIR = f\"./{suffix}\"\n",
    "\n",
    "    auto_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME_OR_PATH)\n",
    "    \n",
    "    for dataset in DATASETS:\n",
    "        source = f\"./Datasets/{dataset}-folds.csv\"\n",
    "        train_inputs, train_targets, val_inputs, val_targets = get_folds(source)\n",
    "\n",
    "        train_dataset = RPDataset(auto_tokenizer, train_inputs, train_targets)\n",
    "        valid_dataset = RPDataset(auto_tokenizer, val_inputs, val_targets)\n",
    "        for lr in learning_rates:\n",
    "            run_name = f\"{dataset}-lr-{lr}\"\n",
    "\n",
    "            # wandb.finish()\n",
    "\n",
    "            \n",
    "            # wandb_logger = WandbLogger(project=WANDB_PROJECT_NAME, \n",
    "            # name=run_name)\n",
    "\n",
    "            # wandb.define_metric(\"val/accuracy\", summary=\"max\")\n",
    "            # wandb.define_metric(\"val/f1\", summary=\"max\")\n",
    "\n",
    "            checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "                dirpath=OUTPUT_DIR + run_name, filename=\"{epoch}-{val/accuracy:.2f}-{val/loss:.2f}\", monitor=\"val/accuracy\", mode=\"max\", save_top_k=5\n",
    "            )\n",
    "\n",
    "            auto_model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME_OR_PATH, from_flax=True)\n",
    "            args_dict = dict(\n",
    "                    data_dir=\"\", # path for data files\n",
    "                    output_dir=f\"./GermanT5-RP-Mod/t5-efficient-oscar-german-small-el32/\", # path to save the checkpoints\n",
    "                    model_name_or_path=MODEL_NAME_OR_PATH,\n",
    "                    tokenizer_name_or_path=MODEL_NAME_OR_PATH,\n",
    "                    dataset_name=dataset,\n",
    "                    max_seq_length=512,\n",
    "                    learning_rate=lr,\n",
    "                    weight_decay=0.1,\n",
    "                    adam_epsilon=1e-8,\n",
    "                    adam_betas=(0.9,0.999),\n",
    "                    warmup_steps=0,\n",
    "                    train_batch_size=4,\n",
    "                    eval_batch_size=2,\n",
    "                    num_train_epochs=10,\n",
    "                    gradient_accumulation_steps=1,\n",
    "                    n_gpu=1,\n",
    "                    early_stop_callback=False,\n",
    "                    fp_16=False, # if you want to enable 16-bit training then install apex and set this to true\n",
    "                    opt_level='O1', # you can find out more on optimisation levels here https://nvidia.github.io/apex/amp.html#opt-levels-and-properties\n",
    "                    max_grad_norm=0.5, # if you enable 16-bit training then set this to a sensible value, 0.5 is a good default\n",
    "                    seed=42,\n",
    "                    train_dataset=train_dataset, \n",
    "                    val_dataset=valid_dataset, \n",
    "                    model=auto_model,\n",
    "                    tokenizer=auto_tokenizer\n",
    "                )\n",
    "            \n",
    "            args = argparse.Namespace(**args_dict)\n",
    "\n",
    "            train_params = dict(\n",
    "                        accumulate_grad_batches=args.gradient_accumulation_steps,\n",
    "                        auto_lr_find=True,\n",
    "                        gpus=args.n_gpu,\n",
    "                        max_epochs=args.num_train_epochs,\n",
    "                        default_root_dir=f\"/home/dobby/{MODEL_NAME_OR_PATH}\",\n",
    "                        # early_stop_callback=False,\n",
    "                        precision= 16 if args.fp_16 else 32,\n",
    "                        amp_level=args.opt_level,\n",
    "                        gradient_clip_val=args.max_grad_norm,\n",
    "                        # checkpoint_callback=checkpoint_callback,\n",
    "                        logger=wandb_logger,\n",
    "                        enable_checkpointing=checkpoint_callback,\n",
    "                        callbacks=[checkpoint_callback],\n",
    "                        # callbacks=[raytuner_callback],\n",
    "                        # callbacks=[LoggingCallback()],\n",
    "                        amp_backend=\"apex\"\n",
    "                    )\n",
    "            \n",
    "            model = T5FineTuner(args)\n",
    "            trainer = pl.Trainer(**train_params)\n",
    "\n",
    "            trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting jax\n",
      "  Downloading jax-0.3.14.tar.gz (990 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m990.1/990.1 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: absl-py in ./.local/lib/python3.9/site-packages (from jax) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.9/dist-packages (from jax) (1.22.3)\n",
      "Collecting opt_einsum\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.9/dist-packages (from jax) (1.8.0)\n",
      "Requirement already satisfied: typing_extensions in ./.local/lib/python3.9/site-packages (from jax) (4.2.0)\n",
      "Collecting etils[epath]\n",
      "  Downloading etils-0.6.0-py3-none-any.whl (98 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.1/98.1 kB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: zipp in ./.local/lib/python3.9/site-packages (from etils[epath]->jax) (3.8.0)\n",
      "Collecting importlib_resources\n",
      "  Downloading importlib_resources-5.8.0-py3-none-any.whl (28 kB)\n",
      "Building wheels for collected packages: jax\n",
      "  Building wheel for jax (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jax: filename=jax-0.3.14-py3-none-any.whl size=1147584 sha256=263e8aae2fa1cea6edb19a7e9163c4337d2c6ad9cdf4df7dae932271e1bcb97a\n",
      "  Stored in directory: /home/dobby/.cache/pip/wheels/32/21/2b/29f2d0dba28673825c67ce8451e44b07ca7bbf8e68964a82db\n",
      "Successfully built jax\n",
      "Installing collected packages: opt_einsum, importlib_resources, etils, jax\n",
      "Successfully installed etils-0.6.0 importlib_resources-5.8.0 jax-0.3.14 opt_einsum-3.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.2\n"
     ]
    }
   ],
   "source": [
    "print(torch.version.cuda)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "045d91fbc43d47ff793b1f8ca33e5dab7b2806f668aee7cb8968f9c49233b7cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jun 20 20:32:53 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.103.01   Driver Version: 470.103.01   CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Quadro RTX 6000     Off  | 00000000:D8:00.0 Off |                  Off |\n",
      "| 35%   32C    P8     7W / 260W |   6120MiB / 24220MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T5 for RP Mod Dataset :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/dobby/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import random\n",
    "import re\n",
    "from itertools import chain\n",
    "from string import punctuation\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# eval packages\n",
    "import textwrap\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "\n",
    "def set_seed(seed):\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  torch.manual_seed(seed)\n",
    "  if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class T5FineTuner(pl.LightningModule):\n",
    "  def __init__(self, hparams, train_dataset, val_dataset):\n",
    "    super(T5FineTuner, self).__init__()\n",
    "    # self.hparams.save_hyperparameters(hparams)\n",
    "    self.save_hyperparameters(hparams)\n",
    "    \n",
    "    self.model = T5ForConditionalGeneration.from_pretrained(hparams.model_name_or_path)\n",
    "    self.tokenizer = T5Tokenizer.from_pretrained(hparams.tokenizer_name_or_path)\n",
    "    self.train_dataset = train_dataset\n",
    "    self.val_dataset = val_dataset\n",
    "    \n",
    "  def is_logger(self):\n",
    "    return self.trainer.global_rank <= 0\n",
    "  \n",
    "\n",
    "  def forward(\n",
    "      self, input_ids, attention_mask=None, decoder_input_ids=None, decoder_attention_mask=None, labels=None\n",
    "  ):\n",
    "    return self.model(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        decoder_input_ids=decoder_input_ids,\n",
    "        decoder_attention_mask=decoder_attention_mask,\n",
    "        labels=labels,\n",
    "        # lm_labels=lm_labels,\n",
    "    )\n",
    "\n",
    "  def _step(self, batch):\n",
    "    labels = batch[\"target_ids\"]\n",
    "    labels[labels[:, :] == self.tokenizer.pad_token_id] = -100\n",
    "\n",
    "    outputs = self(\n",
    "        input_ids=batch[\"source_ids\"],\n",
    "        attention_mask=batch[\"source_mask\"],\n",
    "        labels=labels,\n",
    "        # lm_labels=lm_labels,\n",
    "        decoder_attention_mask=batch['target_mask']\n",
    "    )\n",
    "\n",
    "    loss = outputs[0]\n",
    "\n",
    "    return loss\n",
    "  \n",
    "  def get_accuracy(self, batch):\n",
    "    labels = batch[\"target_ids\"]\n",
    "\n",
    "    outs = self.model.generate(input_ids=batch[\"source_ids\"], \n",
    "                                attention_mask=batch[\"source_mask\"], \n",
    "                                max_length=2)\n",
    "\n",
    "    dec = [self.tokenizer.decode(ids) for ids in outs]\n",
    "    target = [self.tokenizer.decode(label) for label in labels]\n",
    "\n",
    "    new_outputs = [s[6:] for s in dec]\n",
    "    new_targets = [s[:-4] for s in target]\n",
    "\n",
    "    accuracy_score = metrics.accuracy_score(new_targets, new_outputs)\n",
    "    f1 = metrics.f1_score(new_targets, new_outputs, average=\"micro\")\n",
    "    # rec = metrics.recall_score(new_targets, new_outputs, average=\"micros\")\n",
    "    return accuracy_score, f1\n",
    "  # def computer_accuracy(self):\n",
    "  #   self.model.model.eval()\n",
    "\n",
    "  def training_step(self, batch, batch_idx):\n",
    "    loss = self._step(batch)\n",
    "    self.log(\"train_loss\", loss)\n",
    "    return {\"loss\": loss}\n",
    "  \n",
    "  def training_epoch_end(self, outputs):\n",
    "    avg_train_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n",
    "    tensorboard_logs = {\"avg_train_loss\": avg_train_loss}\n",
    "    self.log(\"avg_train_loss\", avg_train_loss)\n",
    "    \n",
    "  def validation_step(self, batch, batch_idx):\n",
    "    loss = self._step(batch)\n",
    "    accuracy, f1 = torch.tensor(self.get_accuracy(batch))\n",
    "    self.log(\"val_loss\", loss, logger=True)\n",
    "    self.log(\"val_accuracy\", accuracy, logger=True)\n",
    "    return {\"val_loss\": loss, \"val_accuracy\": accuracy}\n",
    "  \n",
    "  def validation_epoch_end(self, outputs):\n",
    "    avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "    avg_accuracy = torch.stack([x[\"val_accuracy\"] for x in outputs]).mean()\n",
    "    # tensorboard_logs = {\"val_loss\": avg_loss}\n",
    "    self.log(\"avg_val_loss\", avg_loss)\n",
    "    self.log(\"avg_val_accuracy\", avg_accuracy)\n",
    "    # self.log(\"log\", tensorboard_logs)\n",
    "    # self.log(\"progress_bar\", tensorboard_logs)\n",
    "    # self.log({\"avg_val_loss\": avg_loss, \n",
    "    #           \"log\": tensorboard_logs,\n",
    "    #           'progress_bar': tensorboard_logs}, logger=True, prog_bar=True)\n",
    "    return {\"avg_val_loss\": avg_loss, \"avg_val_accuracy\": avg_accuracy}\n",
    "\n",
    "  def configure_optimizers(self):\n",
    "    \"Prepare optimizer and schedule (linear warmup and decay)\"\n",
    "\n",
    "    model = self.model\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": self.hparams.weight_decay,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=self.hparams.learning_rate, eps=self.hparams.adam_epsilon)\n",
    "    self.opt = optimizer\n",
    "    return [optimizer]\n",
    "  \n",
    "  def optimizer_step(self, \n",
    "                      epoch,\n",
    "                      batch_idx,\n",
    "                      optimizer,\n",
    "                      optimizer_idx,\n",
    "                      second_order_closure=None,\n",
    "                      on_tpu=None,\n",
    "                      using_native_amp=None,\n",
    "                      using_lbfgs=None):\n",
    "    # if self.trainer.use_tpu:\n",
    "    #   xm.optimizer_step(optimizer)\n",
    "    # else:\n",
    "    optimizer.step(closure=second_order_closure)\n",
    "    optimizer.zero_grad()\n",
    "    self.lr_scheduler.step()\n",
    "  \n",
    "  # def closure(self):\n",
    "  #   return \"closure\"\n",
    "  \n",
    "  def get_tqdm_dict(self):\n",
    "    tqdm_dict = {\"loss\": \"{:.3f}\".format(self.trainer.avg_loss), \"lr\": self.lr_scheduler.get_last_lr()[-1]}\n",
    "\n",
    "    return tqdm_dict\n",
    "\n",
    "  def train_dataloader(self):\n",
    "    # train_dataset = get_dataset(tokenizer=self.tokenizer, type_path=\"train\", args=self.hparams)\n",
    "    dataloader = DataLoader(self.train_dataset, batch_size=self.hparams.train_batch_size, drop_last=True, shuffle=True, num_workers=4)\n",
    "    t_total = (\n",
    "        (len(dataloader.dataset) // (self.hparams.train_batch_size * max(1, self.hparams.n_gpu)))\n",
    "        // self.hparams.gradient_accumulation_steps\n",
    "        * float(self.hparams.num_train_epochs)\n",
    "    )\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        self.opt, num_warmup_steps=self.hparams.warmup_steps, num_training_steps=t_total\n",
    "    )\n",
    "    self.lr_scheduler = scheduler\n",
    "    return dataloader\n",
    "\n",
    "  def val_dataloader(self):\n",
    "    # val_dataset = get_dataset(tokenizer=self.tokenizer, type_path=\"val\", args=self.hparams)\n",
    "    return DataLoader(self.val_dataset, batch_size=self.hparams.eval_batch_size, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['source_ids', 'source_mask', 'target_ids', 'target_mask'])\n"
     ]
    }
   ],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained(args.model_name_or_path)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8)\n",
    "for batch in list(enumerate(val_dataloader))[:1]:\n",
    "    print(batch[1].keys())\n",
    "    outputs = model.generate(\n",
    "        input_ids=batch[1][\"source_ids\"],\n",
    "        attention_mask=batch[1][\"source_mask\"],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_dict = dict(\n",
    "    data_dir=\"\", # path for data files\n",
    "    output_dir=\"/GermanT5-RP-Mod/t5-efficient-oscar-german-small-el32/\", # path to save the checkpoints\n",
    "    model_name_or_path=\"GermanT5/t5-efficient-oscar-german-small-el32\",\n",
    "    tokenizer_name_or_path=\"GermanT5/t5-efficient-oscar-german-small-el32\",\n",
    "    dataset_name=\"RP-Mod\",\n",
    "    max_seq_length=512,\n",
    "    learning_rate=1.9e-3,\n",
    "    weight_decay=0.1,\n",
    "    adam_epsilon=1e-8,\n",
    "    warmup_steps=0,\n",
    "    train_batch_size=8,\n",
    "    eval_batch_size=8,\n",
    "    num_train_epochs=10,\n",
    "    gradient_accumulation_steps=16,\n",
    "    n_gpu=1,\n",
    "    early_stop_callback=False,\n",
    "    fp_16=False, # if you want to enable 16-bit training then install apex and set this to true\n",
    "    opt_level='O1', # you can find out more on optimisation levels here https://nvidia.github.io/apex/amp.html#opt-levels-and-properties\n",
    "    max_grad_norm=0.5, # if you enable 16-bit training then set this to a sensible value, 0.5 is a good default\n",
    "    seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(data_dir='', output_dir='/GermanT5-RP-Mod/t5-efficient-oscar-german-small-el32/', model_name_or_path='GermanT5/t5-efficient-oscar-german-small-el32', tokenizer_name_or_path='GermanT5/t5-efficient-oscar-german-small-el32', dataset_name='RP-Mod', max_seq_length=512, learning_rate=0.0019, weight_decay=0.1, adam_epsilon=1e-08, warmup_steps=0, train_batch_size=8, eval_batch_size=8, num_train_epochs=10, gradient_accumulation_steps=16, n_gpu=1, early_stop_callback=False, fp_16=False, opt_level='O1', max_grad_norm=0.5, seed=42)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = argparse.Namespace(**args_dict)\n",
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set-up Wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "WANDB_NOTEBOOK_NAME=\"t5_fine_tuning_rp_mod.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b7d4686a1344d60af3cb0240485a5cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">run-hyperparameter-tuning</strong>: <a href=\"https://wandb.ai/isadoraw/RP-Mod-GermanT5-oscar-german-small-el32/runs/29v4l0p3\" target=\"_blank\">https://wandb.ai/isadoraw/RP-Mod-GermanT5-oscar-german-small-el32/runs/29v4l0p3</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220620_085141-29v4l0p3/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.18"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/dobby/wandb/run-20220620_085340-1p3v8m11</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/isadoraw/RP-Mod-GermanT5-oscar-german-small-el32/runs/1p3v8m11\" target=\"_blank\">run-hyperparameter-tuning</a></strong> to <a href=\"https://wandb.ai/isadoraw/RP-Mod-GermanT5-oscar-german-small-el32\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import wandb\n",
    "wandb.finish()\n",
    "\n",
    "# wandb_logger = WandbLogger(project=f\"{args.dataset_name}-GermanT5-oscar-german-small-el32\", \n",
    "#                             name=f\"run-lr-{args.learning_rate}-wd-{args.weight_decay}\" + \n",
    "#                             f\"-gradient-accumulation-steps-{args.gradient_accumulation_steps}-no-swa\")\n",
    "\n",
    "wandb_logger = WandbLogger(project=f\"{args.dataset_name}-GermanT5-oscar-german-small-el32\", \n",
    "                            name=\"run-hyperparameter-tuning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure other training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import RichProgressBar\n",
    "from pytorch_lightning.callbacks import StochasticWeightAveraging\n",
    "from ray.tune.integration.pytorch_lightning import TuneReportCallback\n",
    "\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    dirpath=args.output_dir, filename=\"checkpoint\", monitor=\"val_loss\", mode=\"min\", save_top_k=5\n",
    ")\n",
    "\n",
    "met = {\"acc\": \"val_accuracy\"}\n",
    "raytuner_callback = TuneReportCallback(met, on=\"validation_end\")\n",
    "\n",
    "train_params = dict(\n",
    "    accumulate_grad_batches=args.gradient_accumulation_steps,\n",
    "    auto_lr_find=True,\n",
    "    gpus=args.n_gpu,\n",
    "    max_epochs=args.num_train_epochs,\n",
    "    default_root_dir=f\"/{args.max_seq_length}-RP-Mod/lr-{args.learning_rate}\",\n",
    "    # early_stop_callback=False,\n",
    "    precision= 16 if args.fp_16 else 32,\n",
    "    amp_level=args.opt_level,\n",
    "    gradient_clip_val=args.max_grad_norm,\n",
    "    # checkpoint_callback=checkpoint_callback,\n",
    "    logger=wandb_logger,\n",
    "    enable_checkpointing=checkpoint_callback,\n",
    "    callbacks=[raytuner_callback],\n",
    "    # callbacks=[LoggingCallback()],\n",
    "    amp_backend=\"apex\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare RP-Mod Dataset for Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(args.model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('classification Sag mal liebe RP was wollt ihr eigentlich Lieber am Ende jammern dass die Polizei versagt hat wenn dann doch ein Anschlag von Islamisten passiert Diese linksgrüne MainstreamMeinungsmache ist wirklich furchtbar Das hat doch nichts mehr mit vernünftigen Journalismus zu tun Ein richtiger Journalist wägt bei Themen vernünftig ab ist kritisch führt Vor und Nachteile auf aber bringt auf keinen Fall zuerst einmal seine persönliche Einstellung zu Papiert',\n",
       " 'unproblematisch')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import random\n",
    "\n",
    "def build_examples_from_files(csv_path):\n",
    "    inputs = []\n",
    "    targets = []\n",
    "\n",
    "    REPLACE_NO_SPACE = re.compile(\"[.;:!\\'?,\\\"()\\[\\]]\")\n",
    "    REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "\n",
    "    LABELS = [\"unproblematisch\", \"problematisch\"]\n",
    "\n",
    "    with open(csv_path, encoding=\"utf-8\") as f_source:\n",
    "        reader = csv.DictReader(f_source)\n",
    "        for row in reader:\n",
    "            line = \"classification: \" + row[\"text\"]\n",
    "            line = REPLACE_NO_SPACE.sub(\"\", line)\n",
    "            line = REPLACE_WITH_SPACE.sub(\"\", line)\n",
    "            line = line\n",
    "\n",
    "            target = LABELS[int(float(row['label']))]\n",
    "\n",
    "            inputs.append(line)\n",
    "            targets.append(target)\n",
    "        \n",
    "        temp = list(zip(inputs, targets))\n",
    "        random.shuffle(temp)\n",
    "        res1, res2 = zip(*temp)\n",
    "\n",
    "        inputs, targets = list(res1), list(res2)\n",
    "\n",
    "        val_inputs, val_targets = inputs[:1000], targets[:1000]\n",
    "        test_inputs, test_targets = inputs[1000:2000], targets[1000:2000]\n",
    "        train_inputs, train_targets = inputs[2000:], targets[2000:] \n",
    "\n",
    "    return train_inputs, train_targets, val_inputs, val_targets, test_inputs, test_targets\n",
    "\n",
    "source = \"./Datasets/RP-Mod-folds.csv\"\n",
    "train_inputs, train_targets, val_inputs, val_targets, test_inputs, test_targets = build_examples_from_files(source)\n",
    "train_inputs[0], train_targets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "#TODO: reorganize this as a lightning data module?\n",
    "class RPDataset(Dataset):\n",
    "    def __init__(self, tokenizer, inputs, outputs, max_len=512):\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.inputs = inputs\n",
    "        self.outputs = outputs\n",
    "\n",
    "        self.tokenized_inputs = tokenizer.batch_encode_plus(\n",
    "            inputs, max_length=max_len, padding=True, truncation=True, return_tensors=\"pt\"\n",
    "        )\n",
    "        self.tokenized_targets = tokenizer.batch_encode_plus(\n",
    "            outputs, max_length=2, padding=True, truncation=True, return_tensors=\"pt\"\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # tokenize input and output\n",
    "        \n",
    "        source_ids = self.tokenized_inputs[\"input_ids\"][index].squeeze()\n",
    "        target_ids = self.tokenized_targets[\"input_ids\"][index].squeeze()\n",
    "\n",
    "        src_mask    = self.tokenized_inputs[\"attention_mask\"][index].squeeze()  # might need to squeeze\n",
    "        target_mask = self.tokenized_targets[\"attention_mask\"][index].squeeze()  # might need to squeeze\n",
    "\n",
    "        return {\"source_ids\": source_ids, \"source_mask\": src_mask, \"target_ids\": target_ids, \"target_mask\": target_mask}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = RPDataset(tokenizer, train_inputs, train_targets)\n",
    "val_dataset = RPDataset(tokenizer, val_inputs, val_targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12282"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = RPDataset(tokenizer, test_inputs, test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification Richtig so Der öffentliche Raum gehört nicht nur den Autobesitzern die so oder so schon viel zu viel Raum illegal durch Gehwerk oder Radwegparken nehmen</s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "unproblematisch</s>\n"
     ]
    }
   ],
   "source": [
    "data = train_dataset.__getitem__(50)\n",
    "print(tokenizer.decode(data['source_ids']))\n",
    "print(tokenizer.decode(data['target_ids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.integration.wandb import wandb_mixin\n",
    "\n",
    "@wandb_mixin\n",
    "def train_rp(config, num_epochs=4):\n",
    "    model = T5FineTuner(args, train_dataset, val_dataset)\n",
    "    tuning_params = dict(\n",
    "        accumulate_grad_batches=args.gradient_accumulation_steps,\n",
    "        auto_lr_find=True,\n",
    "        gpus=args.n_gpu,\n",
    "        #TODO: change num_epochs\n",
    "        max_epochs=num_epochs,\n",
    "        default_root_dir=f\"/{args.max_seq_length}-RP-Mod/lr-{args.learning_rate}\",\n",
    "        # early_stop_callback=False,\n",
    "        precision= 16 if args.fp_16 else 32,\n",
    "        amp_level=args.opt_level,\n",
    "        gradient_clip_val=args.max_grad_norm,\n",
    "        # checkpoint_callback=checkpoint_callback,\n",
    "        logger=wandb_logger,\n",
    "        enable_checkpointing=checkpoint_callback,\n",
    "        callbacks=[raytuner_callback],\n",
    "        # callbacks=[LoggingCallback()],\n",
    "        amp_backend=\"apex\"\n",
    "    )\n",
    "    trainer = pl.Trainer(**tuning_params)\n",
    "    # do I need to update the train_params here with another value for the callback\n",
    "    trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-20 12:00:07,792\tWARNING worker.py:1404 -- Warning: The actor ImplicitFunc is very large (32 MiB). Check that its definition is not implicitly capturing a large array or other object in scope. Tip: use ray.put() to put large objects in the Ray object store.\n",
      "2022-06-20 12:00:07,873\tWARNING util.py:214 -- The `start_trial` operation took 0.564 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (3.0.4) doesn't match a supported version!\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (3.0.4) doesn't match a supported version!\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=17844)\u001b[0m 2022-06-20 12:00:11,109\tINFO wandb.py:172 -- Already logged into W&B.\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=17844)\u001b[0m wandb: Currently logged in as: isadoraw. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2022-06-20 12:00:12 (running for 00:00:05.32)\n",
      "Memory usage on this node: 26.9/376.5 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1.0/8 CPUs, 1.0/1 GPUs, 0.0/34.4 GiB heap, 0.0/17.2 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/dobby/ray_results/tune_rp\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+----------------------+----------+------------------+-----------------+----------------+\n",
      "| Trial name           | status   | loc              |   learning_rate |   weight_decay |\n",
      "|----------------------+----------+------------------+-----------------+----------------|\n",
      "| train_rp_86c32_00000 | RUNNING  | 172.17.0.5:17844 |     0.000368944 |            0.1 |\n",
      "+----------------------+----------+------------------+-----------------+----------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ImplicitFunc pid=17844)\u001b[0m wandb: Tracking run with wandb version 0.12.18\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=17844)\u001b[0m wandb: Run data is saved locally in /home/dobby/ray_results/tune_rp/train_rp_86c32_00000_0_learning_rate=0.0004,weight_decay=0.1000_2022-06-20_12-00-07/wandb/run-20220620_120011-86c32_00000\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=17844)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=17844)\u001b[0m wandb: Syncing run train_rp_86c32_00000\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=17844)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/isadoraw/Optimization_Project\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=17844)\u001b[0m wandb: 🚀 View run at https://wandb.ai/isadoraw/Optimization_Project/runs/86c32_00000\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m GPU available: True, used: True\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m /home/dobby/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:117: UserWarning: When using `Trainer(accumulate_grad_batches != 1)` and overriding `LightningModule.optimizer_{step,zero_grad}`, the hooks will not be called on every batch (rather, they are called on every optimization step).\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m   rank_zero_warn(\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m /home/dobby/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:335: LightningDeprecationWarning: The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7. Please use the `on_exception` callback hook instead.\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m /home/dobby/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:347: LightningDeprecationWarning: The `on_init_start` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m /home/dobby/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:351: LightningDeprecationWarning: The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m   rank_zero_deprecation(\"The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\")\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m /home/dobby/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:376: LightningDeprecationWarning: The `Callback.on_batch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_start` instead.\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m /home/dobby/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:376: LightningDeprecationWarning: The `Callback.on_batch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_end` instead.\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m /home/dobby/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:385: LightningDeprecationWarning: The `Callback.on_epoch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_start` instead.\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m /home/dobby/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:385: LightningDeprecationWarning: The `Callback.on_epoch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_end` instead.\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m   rank_zero_deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2022-06-20 12:00:17 (running for 00:00:10.32)\n",
      "Memory usage on this node: 28.1/376.5 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1.0/8 CPUs, 1.0/1 GPUs, 0.0/34.4 GiB heap, 0.0/17.2 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/dobby/ray_results/tune_rp\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+----------------------+----------+------------------+-----------------+----------------+\n",
      "| Trial name           | status   | loc              |   learning_rate |   weight_decay |\n",
      "|----------------------+----------+------------------+-----------------+----------------|\n",
      "| train_rp_86c32_00000 | RUNNING  | 172.17.0.5:17844 |     0.000368944 |            0.1 |\n",
      "+----------------------+----------+------------------+-----------------+----------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m /home/dobby/.local/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m   | Name  | Type                       | Params\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m -----------------------------------------------------\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m 0 | model | T5ForConditionalGeneration | 142 M \n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m -----------------------------------------------------\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m 142 M     Trainable params\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m 142 M     Total params\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m 569.289   Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m /home/dobby/.local/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:347: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m   rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: 0it [00:00, ?it/s]\n",
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 10.49it/s]\n",
      "Epoch 0:   0%|          | 0/1660 [00:00<?, ?it/s]                          \n",
      "Epoch 0:   0%|          | 1/1660 [00:00<16:20,  1.69it/s, loss=22, v_num=0000]\n",
      "Epoch 0:   0%|          | 2/1660 [00:00<09:59,  2.76it/s, loss=21.3, v_num=0000]\n",
      "== Status ==\n",
      "Current time: 2022-06-20 12:00:22 (running for 00:00:15.33)\n",
      "Memory usage on this node: 29.7/376.5 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1.0/8 CPUs, 1.0/1 GPUs, 0.0/34.4 GiB heap, 0.0/17.2 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/dobby/ray_results/tune_rp\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+----------------------+----------+------------------+-----------------+----------------+\n",
      "| Trial name           | status   | loc              |   learning_rate |   weight_decay |\n",
      "|----------------------+----------+------------------+-----------------+----------------|\n",
      "| train_rp_86c32_00000 | RUNNING  | 172.17.0.5:17844 |     0.000368944 |            0.1 |\n",
      "+----------------------+----------+------------------+-----------------+----------------+\n",
      "\n",
      "\n",
      "Epoch 0:   0%|          | 3/1660 [00:00<07:41,  3.59it/s, loss=21.5, v_num=0000]\n",
      "Epoch 0:   0%|          | 4/1660 [00:00<06:35,  4.19it/s, loss=22.1, v_num=0000]\n",
      "Epoch 0:   0%|          | 5/1660 [00:01<05:53,  4.68it/s, loss=22.3, v_num=0000]\n",
      "Epoch 0:   0%|          | 6/1660 [00:01<05:25,  5.07it/s, loss=21.7, v_num=0000]\n",
      "Epoch 0:   0%|          | 7/1660 [00:01<05:05,  5.41it/s, loss=21.8, v_num=0000]\n",
      "Epoch 0:   0%|          | 8/1660 [00:01<04:50,  5.68it/s, loss=21.9, v_num=0000]\n",
      "Epoch 0:   1%|          | 9/1660 [00:01<04:40,  5.89it/s, loss=22, v_num=0000]  \n",
      "Epoch 0:   1%|          | 10/1660 [00:01<04:30,  6.09it/s, loss=21.8, v_num=0000]\n",
      "Epoch 0:   1%|          | 11/1660 [00:01<04:23,  6.27it/s, loss=22, v_num=0000]  \n",
      "Epoch 0:   1%|          | 12/1660 [00:01<04:16,  6.42it/s, loss=22, v_num=0000]\n",
      "Epoch 0:   1%|          | 13/1660 [00:01<04:11,  6.56it/s, loss=22.2, v_num=0000]\n",
      "Epoch 0:   1%|          | 14/1660 [00:02<04:06,  6.68it/s, loss=22.3, v_num=0000]\n",
      "Epoch 0:   1%|          | 15/1660 [00:02<04:02,  6.79it/s, loss=22.3, v_num=0000]\n",
      "Epoch 0:   1%|          | 16/1660 [00:02<04:05,  6.69it/s, loss=22, v_num=0000]  \n",
      "Epoch 0:   1%|          | 17/1660 [00:02<04:03,  6.76it/s, loss=21.5, v_num=0000]\n",
      "Epoch 0:   1%|          | 18/1660 [00:02<03:59,  6.85it/s, loss=21.1, v_num=0000]\n",
      "Epoch 0:   1%|          | 19/1660 [00:02<03:56,  6.94it/s, loss=20.7, v_num=0000]\n",
      "Epoch 0:   1%|          | 20/1660 [00:02<03:53,  7.02it/s, loss=20.4, v_num=0000]\n",
      "Epoch 0:   1%|▏         | 21/1660 [00:02<03:51,  7.09it/s, loss=20.1, v_num=0000]\n",
      "Epoch 0:   1%|▏         | 22/1660 [00:03<03:48,  7.15it/s, loss=19.6, v_num=0000]\n",
      "Epoch 0:   1%|▏         | 23/1660 [00:03<03:46,  7.22it/s, loss=19.2, v_num=0000]\n",
      "Epoch 0:   1%|▏         | 24/1660 [00:03<03:44,  7.27it/s, loss=18.6, v_num=0000]\n",
      "Epoch 0:   2%|▏         | 25/1660 [00:03<03:43,  7.33it/s, loss=18.2, v_num=0000]\n",
      "Epoch 0:   2%|▏         | 26/1660 [00:03<03:41,  7.37it/s, loss=17.9, v_num=0000]\n",
      "Epoch 0:   2%|▏         | 27/1660 [00:03<03:40,  7.41it/s, loss=17.5, v_num=0000]\n",
      "Epoch 0:   2%|▏         | 28/1660 [00:03<03:39,  7.45it/s, loss=17.1, v_num=0000]\n",
      "Epoch 0:   2%|▏         | 29/1660 [00:03<03:37,  7.48it/s, loss=16.7, v_num=0000]\n",
      "Epoch 0:   2%|▏         | 30/1660 [00:03<03:36,  7.51it/s, loss=16.2, v_num=0000]\n",
      "Epoch 0:   2%|▏         | 31/1660 [00:04<03:35,  7.54it/s, loss=15.7, v_num=0000]\n",
      "Epoch 0:   2%|▏         | 32/1660 [00:04<03:37,  7.49it/s, loss=15.3, v_num=0000]\n",
      "Epoch 0:   2%|▏         | 33/1660 [00:04<03:36,  7.52it/s, loss=14.6, v_num=0000]\n",
      "Epoch 0:   2%|▏         | 34/1660 [00:04<03:35,  7.55it/s, loss=14.1, v_num=0000]\n",
      "Epoch 0:   2%|▏         | 35/1660 [00:04<03:34,  7.58it/s, loss=13.6, v_num=0000]\n",
      "Epoch 0:   2%|▏         | 36/1660 [00:04<03:33,  7.61it/s, loss=13.4, v_num=0000]\n",
      "Epoch 0:   2%|▏         | 37/1660 [00:04<03:32,  7.63it/s, loss=13.3, v_num=0000]\n",
      "Epoch 0:   2%|▏         | 38/1660 [00:04<03:31,  7.66it/s, loss=13.2, v_num=0000]\n",
      "Epoch 0:   2%|▏         | 39/1660 [00:05<03:31,  7.68it/s, loss=13.2, v_num=0000]\n",
      "Epoch 0:   2%|▏         | 40/1660 [00:05<03:30,  7.70it/s, loss=13, v_num=0000]  \n",
      "Epoch 0:   2%|▏         | 41/1660 [00:05<03:29,  7.72it/s, loss=12.8, v_num=0000]\n",
      "Epoch 0:   3%|▎         | 42/1660 [00:05<03:28,  7.74it/s, loss=12.9, v_num=0000]\n",
      "Epoch 0:   3%|▎         | 43/1660 [00:05<03:28,  7.77it/s, loss=12.6, v_num=0000]\n",
      "Epoch 0:   3%|▎         | 44/1660 [00:05<03:27,  7.79it/s, loss=12.5, v_num=0000]\n",
      "== Status ==\n",
      "Current time: 2022-06-20 12:00:27 (running for 00:00:20.33)\n",
      "Memory usage on this node: 29.7/376.5 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1.0/8 CPUs, 1.0/1 GPUs, 0.0/34.4 GiB heap, 0.0/17.2 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/dobby/ray_results/tune_rp\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+----------------------+----------+------------------+-----------------+----------------+\n",
      "| Trial name           | status   | loc              |   learning_rate |   weight_decay |\n",
      "|----------------------+----------+------------------+-----------------+----------------|\n",
      "| train_rp_86c32_00000 | RUNNING  | 172.17.0.5:17844 |     0.000368944 |            0.1 |\n",
      "+----------------------+----------+------------------+-----------------+----------------+\n",
      "\n",
      "\n",
      "Epoch 0:   3%|▎         | 45/1660 [00:05<03:26,  7.80it/s, loss=12.3, v_num=0000]\n",
      "Epoch 0:   3%|▎         | 46/1660 [00:05<03:26,  7.82it/s, loss=12.1, v_num=0000]\n",
      "Epoch 0:   3%|▎         | 47/1660 [00:05<03:25,  7.84it/s, loss=12, v_num=0000]  \n",
      "Epoch 0:   3%|▎         | 48/1660 [00:06<03:26,  7.79it/s, loss=11.7, v_num=0000]\n",
      "Epoch 0:   3%|▎         | 49/1660 [00:06<03:26,  7.81it/s, loss=11.7, v_num=0000]\n",
      "Epoch 0:   3%|▎         | 49/1660 [00:06<03:26,  7.81it/s, loss=11.5, v_num=0000]\n",
      "Epoch 0:   3%|▎         | 50/1660 [00:06<03:25,  7.82it/s, loss=11.4, v_num=0000]\n",
      "Epoch 0:   3%|▎         | 51/1660 [00:06<03:25,  7.84it/s, loss=11.1, v_num=0000]\n",
      "Epoch 0:   3%|▎         | 52/1660 [00:06<03:24,  7.85it/s, loss=10.8, v_num=0000]\n",
      "Epoch 0:   3%|▎         | 53/1660 [00:06<03:24,  7.86it/s, loss=10.8, v_num=0000]\n",
      "Epoch 0:   3%|▎         | 54/1660 [00:06<03:23,  7.87it/s, loss=10.6, v_num=0000]\n",
      "Epoch 0:   3%|▎         | 55/1660 [00:06<03:23,  7.89it/s, loss=10.5, v_num=0000]\n",
      "Epoch 0:   3%|▎         | 56/1660 [00:07<03:23,  7.89it/s, loss=10.3, v_num=0000]\n",
      "Epoch 0:   3%|▎         | 57/1660 [00:07<03:22,  7.90it/s, loss=10.1, v_num=0000]\n",
      "Epoch 0:   3%|▎         | 58/1660 [00:07<03:22,  7.92it/s, loss=9.95, v_num=0000]\n",
      "Epoch 0:   4%|▎         | 59/1660 [00:07<03:21,  7.93it/s, loss=9.72, v_num=0000]\n",
      "Epoch 0:   4%|▎         | 60/1660 [00:07<03:21,  7.94it/s, loss=9.62, v_num=0000]\n",
      "Epoch 0:   4%|▎         | 61/1660 [00:07<03:21,  7.95it/s, loss=9.55, v_num=0000]\n",
      "Epoch 0:   4%|▎         | 62/1660 [00:07<03:20,  7.96it/s, loss=9.4, v_num=0000] \n",
      "Epoch 0:   4%|▍         | 63/1660 [00:07<03:20,  7.98it/s, loss=9.39, v_num=0000]\n",
      "Epoch 0:   4%|▍         | 64/1660 [00:08<03:21,  7.94it/s, loss=9.46, v_num=0000]\n",
      "Epoch 0:   4%|▍         | 65/1660 [00:08<03:20,  7.95it/s, loss=9.25, v_num=0000]\n",
      "Epoch 0:   4%|▍         | 66/1660 [00:08<03:20,  7.96it/s, loss=8.93, v_num=0000]\n",
      "Epoch 0:   4%|▍         | 67/1660 [00:08<03:19,  7.97it/s, loss=8.62, v_num=0000]\n",
      "Epoch 0:   4%|▍         | 68/1660 [00:08<03:19,  7.98it/s, loss=8.44, v_num=0000]\n",
      "Epoch 0:   4%|▍         | 69/1660 [00:08<03:19,  7.99it/s, loss=8.27, v_num=0000]\n",
      "Epoch 0:   4%|▍         | 70/1660 [00:08<03:18,  8.00it/s, loss=8.05, v_num=0000]\n",
      "Epoch 0:   4%|▍         | 71/1660 [00:08<03:18,  8.01it/s, loss=7.83, v_num=0000]\n",
      "Epoch 0:   4%|▍         | 72/1660 [00:08<03:18,  8.02it/s, loss=7.76, v_num=0000]\n",
      "Epoch 0:   4%|▍         | 73/1660 [00:09<03:17,  8.02it/s, loss=7.43, v_num=0000]\n",
      "Epoch 0:   4%|▍         | 74/1660 [00:09<03:17,  8.03it/s, loss=7.23, v_num=0000]\n",
      "Epoch 0:   5%|▍         | 75/1660 [00:09<03:17,  8.04it/s, loss=7.04, v_num=0000]\n",
      "Epoch 0:   5%|▍         | 76/1660 [00:09<03:16,  8.05it/s, loss=6.86, v_num=0000]\n",
      "Epoch 0:   5%|▍         | 77/1660 [00:09<03:16,  8.06it/s, loss=6.72, v_num=0000]\n",
      "Epoch 0:   5%|▍         | 78/1660 [00:09<03:15,  8.07it/s, loss=6.54, v_num=0000]\n",
      "Epoch 0:   5%|▍         | 79/1660 [00:09<03:15,  8.08it/s, loss=6.33, v_num=0000]\n",
      "Epoch 0:   5%|▍         | 80/1660 [00:09<03:16,  8.05it/s, loss=6.14, v_num=0000]\n",
      "Epoch 0:   5%|▍         | 81/1660 [00:10<03:16,  8.05it/s, loss=5.79, v_num=0000]\n",
      "Epoch 0:   5%|▍         | 82/1660 [00:10<03:15,  8.06it/s, loss=5.38, v_num=0000]\n",
      "Epoch 0:   5%|▌         | 83/1660 [00:10<03:15,  8.07it/s, loss=5.03, v_num=0000]\n",
      "Epoch 0:   5%|▌         | 84/1660 [00:10<03:15,  8.07it/s, loss=4.6, v_num=0000] \n",
      "Epoch 0:   5%|▌         | 85/1660 [00:10<03:14,  8.08it/s, loss=4.45, v_num=0000]\n",
      "Epoch 0:   5%|▌         | 86/1660 [00:10<03:14,  8.08it/s, loss=4.34, v_num=0000]\n",
      "Epoch 0:   5%|▌         | 87/1660 [00:10<03:14,  8.09it/s, loss=4.2, v_num=0000] \n",
      "== Status ==\n",
      "Current time: 2022-06-20 12:00:32 (running for 00:00:25.33)\n",
      "Memory usage on this node: 29.7/376.5 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1.0/8 CPUs, 1.0/1 GPUs, 0.0/34.4 GiB heap, 0.0/17.2 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/dobby/ray_results/tune_rp\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+----------------------+----------+------------------+-----------------+----------------+\n",
      "| Trial name           | status   | loc              |   learning_rate |   weight_decay |\n",
      "|----------------------+----------+------------------+-----------------+----------------|\n",
      "| train_rp_86c32_00000 | RUNNING  | 172.17.0.5:17844 |     0.000368944 |            0.1 |\n",
      "+----------------------+----------+------------------+-----------------+----------------+\n",
      "\n",
      "\n",
      "Epoch 0:   5%|▌         | 88/1660 [00:10<03:14,  8.10it/s, loss=3.99, v_num=0000]\n",
      "Epoch 0:   5%|▌         | 89/1660 [00:10<03:13,  8.10it/s, loss=3.87, v_num=0000]\n",
      "Epoch 0:   5%|▌         | 90/1660 [00:11<03:13,  8.11it/s, loss=3.76, v_num=0000]\n",
      "Epoch 0:   5%|▌         | 91/1660 [00:11<03:13,  8.11it/s, loss=3.63, v_num=0000]\n",
      "Epoch 0:   6%|▌         | 92/1660 [00:11<03:13,  8.12it/s, loss=3.46, v_num=0000]\n",
      "Epoch 0:   6%|▌         | 93/1660 [00:11<03:12,  8.13it/s, loss=3.34, v_num=0000]\n",
      "Epoch 0:   6%|▌         | 94/1660 [00:11<03:12,  8.13it/s, loss=3.15, v_num=0000]\n",
      "Epoch 0:   6%|▌         | 95/1660 [00:11<03:12,  8.14it/s, loss=2.94, v_num=0000]\n",
      "Epoch 0:   6%|▌         | 96/1660 [00:11<03:12,  8.11it/s, loss=2.78, v_num=0000]\n",
      "Epoch 0:   6%|▌         | 97/1660 [00:11<03:12,  8.12it/s, loss=2.58, v_num=0000]\n",
      "Epoch 0:   6%|▌         | 98/1660 [00:12<03:12,  8.12it/s, loss=2.36, v_num=0000]\n",
      "Epoch 0:   6%|▌         | 99/1660 [00:12<03:12,  8.13it/s, loss=2.15, v_num=0000]\n",
      "Epoch 0:   6%|▌         | 100/1660 [00:12<03:11,  8.13it/s, loss=1.94, v_num=0000]\n",
      "Epoch 0:   6%|▌         | 101/1660 [00:12<03:11,  8.14it/s, loss=1.89, v_num=0000]\n",
      "Epoch 0:   6%|▌         | 102/1660 [00:12<03:11,  8.14it/s, loss=1.81, v_num=0000]\n",
      "Epoch 0:   6%|▌         | 103/1660 [00:12<03:11,  8.15it/s, loss=1.76, v_num=0000]\n",
      "Epoch 0:   6%|▋         | 104/1660 [00:12<03:10,  8.15it/s, loss=1.7, v_num=0000] \n",
      "Epoch 0:   6%|▋         | 105/1660 [00:12<03:10,  8.15it/s, loss=1.64, v_num=0000]\n",
      "Epoch 0:   6%|▋         | 106/1660 [00:12<03:10,  8.15it/s, loss=1.58, v_num=0000]\n",
      "Epoch 0:   6%|▋         | 107/1660 [00:13<03:10,  8.16it/s, loss=1.51, v_num=0000]\n",
      "Epoch 0:   7%|▋         | 108/1660 [00:13<03:10,  8.16it/s, loss=1.45, v_num=0000]\n",
      "Epoch 0:   7%|▋         | 109/1660 [00:13<03:09,  8.17it/s, loss=1.38, v_num=0000]\n",
      "Epoch 0:   7%|▋         | 110/1660 [00:13<03:09,  8.17it/s, loss=1.3, v_num=0000] \n",
      "Epoch 0:   7%|▋         | 111/1660 [00:13<03:09,  8.17it/s, loss=1.22, v_num=0000]\n",
      "Epoch 0:   7%|▋         | 112/1660 [00:13<03:09,  8.15it/s, loss=1.13, v_num=0000]\n",
      "Epoch 0:   7%|▋         | 113/1660 [00:13<03:09,  8.15it/s, loss=1.08, v_num=0000]\n",
      "Epoch 0:   7%|▋         | 114/1660 [00:13<03:09,  8.16it/s, loss=1.05, v_num=0000]\n",
      "Epoch 0:   7%|▋         | 115/1660 [00:14<03:09,  8.16it/s, loss=0.991, v_num=0000]\n",
      "Epoch 0:   7%|▋         | 116/1660 [00:14<03:09,  8.16it/s, loss=0.937, v_num=0000]\n",
      "Epoch 0:   7%|▋         | 117/1660 [00:14<03:08,  8.17it/s, loss=0.98, v_num=0000] \n",
      "Epoch 0:   7%|▋         | 118/1660 [00:14<03:08,  8.17it/s, loss=1.05, v_num=0000]\n",
      "Epoch 0:   7%|▋         | 119/1660 [00:14<03:08,  8.17it/s, loss=1.09, v_num=0000]\n",
      "Epoch 0:   7%|▋         | 120/1660 [00:14<03:08,  8.18it/s, loss=1.1, v_num=0000] \n",
      "Epoch 0:   7%|▋         | 121/1660 [00:14<03:08,  8.18it/s, loss=1.12, v_num=0000]\n",
      "Epoch 0:   7%|▋         | 122/1660 [00:14<03:07,  8.18it/s, loss=1.15, v_num=0000]\n",
      "Epoch 0:   7%|▋         | 123/1660 [00:15<03:07,  8.19it/s, loss=1.22, v_num=0000]\n",
      "Epoch 0:   7%|▋         | 124/1660 [00:15<03:07,  8.19it/s, loss=1.28, v_num=0000]\n",
      "Epoch 0:   8%|▊         | 125/1660 [00:15<03:07,  8.19it/s, loss=1.31, v_num=0000]\n",
      "Epoch 0:   8%|▊         | 126/1660 [00:15<03:07,  8.20it/s, loss=1.32, v_num=0000]\n",
      "Epoch 0:   8%|▊         | 127/1660 [00:15<03:06,  8.20it/s, loss=1.38, v_num=0000]\n",
      "Epoch 0:   8%|▊         | 128/1660 [00:15<03:07,  8.18it/s, loss=1.41, v_num=0000]\n",
      "Epoch 0:   8%|▊         | 129/1660 [00:15<03:07,  8.18it/s, loss=1.43, v_num=0000]\n",
      "== Status ==\n",
      "Current time: 2022-06-20 12:00:37 (running for 00:00:30.34)\n",
      "Memory usage on this node: 29.7/376.5 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1.0/8 CPUs, 1.0/1 GPUs, 0.0/34.4 GiB heap, 0.0/17.2 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/dobby/ray_results/tune_rp\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+----------------------+----------+------------------+-----------------+----------------+\n",
      "| Trial name           | status   | loc              |   learning_rate |   weight_decay |\n",
      "|----------------------+----------+------------------+-----------------+----------------|\n",
      "| train_rp_86c32_00000 | RUNNING  | 172.17.0.5:17844 |     0.000368944 |            0.1 |\n",
      "+----------------------+----------+------------------+-----------------+----------------+\n",
      "\n",
      "\n",
      "Epoch 0:   8%|▊         | 130/1660 [00:15<03:06,  8.18it/s, loss=1.4, v_num=0000] \n",
      "Epoch 0:   8%|▊         | 131/1660 [00:15<03:06,  8.19it/s, loss=1.41, v_num=0000]\n",
      "Epoch 0:   8%|▊         | 132/1660 [00:16<03:06,  8.19it/s, loss=1.47, v_num=0000]\n",
      "Epoch 0:   8%|▊         | 133/1660 [00:16<03:06,  8.20it/s, loss=1.45, v_num=0000]\n",
      "Epoch 0:   8%|▊         | 134/1660 [00:16<03:06,  8.20it/s, loss=1.46, v_num=0000]\n",
      "Epoch 0:   8%|▊         | 135/1660 [00:16<03:05,  8.21it/s, loss=1.43, v_num=0000]\n",
      "Epoch 0:   8%|▊         | 136/1660 [00:16<03:05,  8.21it/s, loss=1.41, v_num=0000]\n",
      "Epoch 0:   8%|▊         | 137/1660 [00:16<03:05,  8.21it/s, loss=1.33, v_num=0000]\n",
      "Epoch 0:   8%|▊         | 138/1660 [00:16<03:05,  8.22it/s, loss=1.28, v_num=0000]\n",
      "Epoch 0:   8%|▊         | 139/1660 [00:16<03:05,  8.22it/s, loss=1.22, v_num=0000]\n",
      "Epoch 0:   8%|▊         | 140/1660 [00:17<03:04,  8.22it/s, loss=1.17, v_num=0000]\n",
      "Epoch 0:   8%|▊         | 141/1660 [00:17<03:04,  8.23it/s, loss=1.16, v_num=0000]\n",
      "Epoch 0:   9%|▊         | 142/1660 [00:17<03:04,  8.23it/s, loss=1.14, v_num=0000]\n",
      "Epoch 0:   9%|▊         | 143/1660 [00:17<03:04,  8.24it/s, loss=1.06, v_num=0000]\n",
      "Epoch 0:   9%|▊         | 144/1660 [00:17<03:04,  8.22it/s, loss=1.01, v_num=0000]\n",
      "Epoch 0:   9%|▊         | 145/1660 [00:17<03:04,  8.23it/s, loss=0.976, v_num=0000]\n",
      "Epoch 0:   9%|▉         | 146/1660 [00:17<03:03,  8.23it/s, loss=0.943, v_num=0000]\n",
      "Epoch 0:   9%|▉         | 147/1660 [00:17<03:03,  8.24it/s, loss=0.868, v_num=0000]\n",
      "Epoch 0:   9%|▉         | 148/1660 [00:17<03:03,  8.24it/s, loss=0.825, v_num=0000]\n",
      "Epoch 0:   9%|▉         | 149/1660 [00:18<03:03,  8.25it/s, loss=0.804, v_num=0000]\n",
      "Epoch 0:   9%|▉         | 150/1660 [00:18<03:03,  8.25it/s, loss=0.822, v_num=0000]\n",
      "Epoch 0:   9%|▉         | 151/1660 [00:18<03:02,  8.25it/s, loss=0.794, v_num=0000]\n",
      "Epoch 0:   9%|▉         | 152/1660 [00:18<03:02,  8.25it/s, loss=0.722, v_num=0000]\n",
      "Epoch 0:   9%|▉         | 153/1660 [00:18<03:02,  8.25it/s, loss=0.701, v_num=0000]\n",
      "Epoch 0:   9%|▉         | 154/1660 [00:18<03:02,  8.26it/s, loss=0.657, v_num=0000]\n",
      "Epoch 0:   9%|▉         | 155/1660 [00:18<03:02,  8.26it/s, loss=0.646, v_num=0000]\n",
      "Epoch 0:   9%|▉         | 156/1660 [00:18<03:02,  8.26it/s, loss=0.638, v_num=0000]\n",
      "Epoch 0:   9%|▉         | 157/1660 [00:19<03:01,  8.26it/s, loss=0.66, v_num=0000] \n",
      "Epoch 0:  10%|▉         | 158/1660 [00:19<03:01,  8.26it/s, loss=0.629, v_num=0000]\n",
      "Epoch 0:  10%|▉         | 159/1660 [00:19<03:01,  8.26it/s, loss=0.62, v_num=0000] \n",
      "Epoch 0:  10%|▉         | 160/1660 [00:19<03:01,  8.25it/s, loss=0.614, v_num=0000]\n",
      "Epoch 0:  10%|▉         | 161/1660 [00:19<03:01,  8.25it/s, loss=0.573, v_num=0000]\n",
      "Epoch 0:  10%|▉         | 162/1660 [00:19<03:01,  8.25it/s, loss=0.541, v_num=0000]\n",
      "Epoch 0:  10%|▉         | 163/1660 [00:19<03:01,  8.25it/s, loss=0.538, v_num=0000]\n",
      "Epoch 0:  10%|▉         | 164/1660 [00:19<03:01,  8.26it/s, loss=0.5, v_num=0000]  \n",
      "Epoch 0:  10%|▉         | 165/1660 [00:19<03:01,  8.26it/s, loss=0.463, v_num=0000]\n",
      "Epoch 0:  10%|█         | 166/1660 [00:20<03:00,  8.26it/s, loss=0.454, v_num=0000]\n",
      "Epoch 0:  10%|█         | 167/1660 [00:20<03:00,  8.26it/s, loss=0.434, v_num=0000]\n",
      "Epoch 0:  10%|█         | 168/1660 [00:20<03:00,  8.26it/s, loss=0.418, v_num=0000]\n",
      "Epoch 0:  10%|█         | 169/1660 [00:20<03:00,  8.26it/s, loss=0.403, v_num=0000]\n",
      "Epoch 0:  10%|█         | 170/1660 [00:20<03:00,  8.26it/s, loss=0.392, v_num=0000]\n",
      "Epoch 0:  10%|█         | 171/1660 [00:20<03:00,  8.27it/s, loss=0.386, v_num=0000]\n",
      "Epoch 0:  10%|█         | 172/1660 [00:20<02:59,  8.27it/s, loss=0.397, v_num=0000]\n",
      "== Status ==\n",
      "Current time: 2022-06-20 12:00:42 (running for 00:00:35.34)\n",
      "Memory usage on this node: 29.8/376.5 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1.0/8 CPUs, 1.0/1 GPUs, 0.0/34.4 GiB heap, 0.0/17.2 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/dobby/ray_results/tune_rp\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+----------------------+----------+------------------+-----------------+----------------+\n",
      "| Trial name           | status   | loc              |   learning_rate |   weight_decay |\n",
      "|----------------------+----------+------------------+-----------------+----------------|\n",
      "| train_rp_86c32_00000 | RUNNING  | 172.17.0.5:17844 |     0.000368944 |            0.1 |\n",
      "+----------------------+----------+------------------+-----------------+----------------+\n",
      "\n",
      "\n",
      "Epoch 0:  10%|█         | 173/1660 [00:20<02:59,  8.27it/s, loss=0.399, v_num=0000]\n",
      "Epoch 0:  10%|█         | 174/1660 [00:21<02:59,  8.27it/s, loss=0.402, v_num=0000]\n",
      "Epoch 0:  11%|█         | 175/1660 [00:21<02:59,  8.27it/s, loss=0.401, v_num=0000]\n",
      "Epoch 0:  11%|█         | 176/1660 [00:21<02:59,  8.26it/s, loss=0.384, v_num=0000]\n",
      "Epoch 0:  11%|█         | 177/1660 [00:21<02:59,  8.27it/s, loss=0.386, v_num=0000]\n",
      "Epoch 0:  11%|█         | 178/1660 [00:21<02:59,  8.27it/s, loss=0.39, v_num=0000] \n",
      "Epoch 0:  11%|█         | 179/1660 [00:21<02:59,  8.27it/s, loss=0.419, v_num=0000]\n",
      "Epoch 0:  11%|█         | 180/1660 [00:21<02:58,  8.27it/s, loss=0.439, v_num=0000]\n",
      "Epoch 0:  11%|█         | 181/1660 [00:21<02:58,  8.28it/s, loss=0.451, v_num=0000]\n",
      "Epoch 0:  11%|█         | 182/1660 [00:21<02:58,  8.28it/s, loss=0.46, v_num=0000] \n",
      "Epoch 0:  11%|█         | 183/1660 [00:22<02:58,  8.28it/s, loss=0.447, v_num=0000]\n",
      "Epoch 0:  11%|█         | 184/1660 [00:22<02:58,  8.28it/s, loss=0.457, v_num=0000]\n",
      "Epoch 0:  11%|█         | 185/1660 [00:22<02:58,  8.29it/s, loss=0.455, v_num=0000]\n",
      "Epoch 0:  11%|█         | 186/1660 [00:22<02:57,  8.29it/s, loss=0.478, v_num=0000]\n",
      "Epoch 0:  11%|█▏        | 187/1660 [00:22<02:57,  8.29it/s, loss=0.486, v_num=0000]\n",
      "Epoch 0:  11%|█▏        | 188/1660 [00:22<02:57,  8.29it/s, loss=0.5, v_num=0000]  \n",
      "Epoch 0:  11%|█▏        | 189/1660 [00:22<02:57,  8.30it/s, loss=0.515, v_num=0000]\n",
      "Epoch 0:  11%|█▏        | 190/1660 [00:22<02:57,  8.30it/s, loss=0.522, v_num=0000]\n",
      "Epoch 0:  12%|█▏        | 191/1660 [00:23<02:56,  8.30it/s, loss=0.527, v_num=0000]\n",
      "Epoch 0:  12%|█▏        | 192/1660 [00:23<02:57,  8.29it/s, loss=0.532, v_num=0000]\n",
      "Epoch 0:  12%|█▏        | 193/1660 [00:23<02:56,  8.29it/s, loss=0.547, v_num=0000]\n",
      "Epoch 0:  12%|█▏        | 194/1660 [00:23<02:56,  8.29it/s, loss=0.544, v_num=0000]\n",
      "Epoch 0:  12%|█▏        | 195/1660 [00:23<02:56,  8.29it/s, loss=0.548, v_num=0000]\n",
      "Epoch 0:  12%|█▏        | 196/1660 [00:23<02:56,  8.30it/s, loss=0.542, v_num=0000]\n",
      "Epoch 0:  12%|█▏        | 197/1660 [00:23<02:56,  8.30it/s, loss=0.567, v_num=0000]\n",
      "Epoch 0:  12%|█▏        | 198/1660 [00:23<02:56,  8.30it/s, loss=0.556, v_num=0000]\n",
      "Epoch 0:  12%|█▏        | 199/1660 [00:23<02:56,  8.30it/s, loss=0.553, v_num=0000]\n",
      "Epoch 0:  12%|█▏        | 200/1660 [00:24<02:55,  8.30it/s, loss=0.538, v_num=0000]\n",
      "Epoch 0:  12%|█▏        | 201/1660 [00:24<02:55,  8.31it/s, loss=0.534, v_num=0000]\n",
      "Epoch 0:  12%|█▏        | 202/1660 [00:24<02:55,  8.31it/s, loss=0.542, v_num=0000]\n",
      "Epoch 0:  12%|█▏        | 203/1660 [00:24<02:55,  8.31it/s, loss=0.54, v_num=0000] \n",
      "Epoch 0:  12%|█▏        | 204/1660 [00:24<02:55,  8.31it/s, loss=0.539, v_num=0000]\n",
      "Epoch 0:  12%|█▏        | 205/1660 [00:24<02:54,  8.31it/s, loss=0.536, v_num=0000]\n",
      "Epoch 0:  12%|█▏        | 206/1660 [00:24<02:54,  8.32it/s, loss=0.529, v_num=0000]\n",
      "Epoch 0:  12%|█▏        | 207/1660 [00:24<02:54,  8.32it/s, loss=0.53, v_num=0000] \n",
      "Epoch 0:  13%|█▎        | 208/1660 [00:25<02:54,  8.31it/s, loss=0.54, v_num=0000]\n",
      "Epoch 0:  13%|█▎        | 209/1660 [00:25<02:54,  8.31it/s, loss=0.54, v_num=0000]\n",
      "Epoch 0:  13%|█▎        | 210/1660 [00:25<02:54,  8.31it/s, loss=0.531, v_num=0000]\n",
      "Epoch 0:  13%|█▎        | 211/1660 [00:25<02:54,  8.32it/s, loss=0.529, v_num=0000]\n",
      "Epoch 0:  13%|█▎        | 212/1660 [00:25<02:54,  8.32it/s, loss=0.522, v_num=0000]\n",
      "Epoch 0:  13%|█▎        | 213/1660 [00:25<02:53,  8.32it/s, loss=0.502, v_num=0000]\n",
      "Epoch 0:  13%|█▎        | 214/1660 [00:25<02:53,  8.32it/s, loss=0.503, v_num=0000]\n",
      "== Status ==\n",
      "Current time: 2022-06-20 12:00:47 (running for 00:00:40.34)\n",
      "Memory usage on this node: 29.8/376.5 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1.0/8 CPUs, 1.0/1 GPUs, 0.0/34.4 GiB heap, 0.0/17.2 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/dobby/ray_results/tune_rp\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+----------------------+----------+------------------+-----------------+----------------+\n",
      "| Trial name           | status   | loc              |   learning_rate |   weight_decay |\n",
      "|----------------------+----------+------------------+-----------------+----------------|\n",
      "| train_rp_86c32_00000 | RUNNING  | 172.17.0.5:17844 |     0.000368944 |            0.1 |\n",
      "+----------------------+----------+------------------+-----------------+----------------+\n",
      "\n",
      "\n",
      "Epoch 0:  13%|█▎        | 215/1660 [00:25<02:53,  8.32it/s, loss=0.497, v_num=0000]\n",
      "Epoch 0:  13%|█▎        | 216/1660 [00:25<02:53,  8.33it/s, loss=0.503, v_num=0000]\n",
      "Epoch 0:  13%|█▎        | 217/1660 [00:26<02:53,  8.33it/s, loss=0.464, v_num=0000]\n",
      "Epoch 0:  13%|█▎        | 218/1660 [00:26<02:53,  8.33it/s, loss=0.461, v_num=0000]\n",
      "Epoch 0:  13%|█▎        | 219/1660 [00:26<02:52,  8.33it/s, loss=0.444, v_num=0000]\n",
      "Epoch 0:  13%|█▎        | 220/1660 [00:26<02:52,  8.33it/s, loss=0.454, v_num=0000]\n",
      "Epoch 0:  13%|█▎        | 221/1660 [00:26<02:52,  8.34it/s, loss=0.441, v_num=0000]\n",
      "Epoch 0:  13%|█▎        | 222/1660 [00:26<02:52,  8.34it/s, loss=0.434, v_num=0000]\n",
      "Epoch 0:  13%|█▎        | 223/1660 [00:26<02:52,  8.34it/s, loss=0.436, v_num=0000]\n",
      "Epoch 0:  13%|█▎        | 224/1660 [00:26<02:52,  8.33it/s, loss=0.445, v_num=0000]\n",
      "Epoch 0:  14%|█▎        | 225/1660 [00:27<02:52,  8.33it/s, loss=0.473, v_num=0000]\n",
      "Epoch 0:  14%|█▎        | 226/1660 [00:27<02:52,  8.33it/s, loss=0.492, v_num=0000]\n",
      "Epoch 0:  14%|█▎        | 227/1660 [00:27<02:51,  8.34it/s, loss=0.507, v_num=0000]\n",
      "Epoch 0:  14%|█▎        | 228/1660 [00:27<02:51,  8.34it/s, loss=0.513, v_num=0000]\n",
      "Epoch 0:  14%|█▍        | 229/1660 [00:27<02:51,  8.34it/s, loss=0.51, v_num=0000] \n",
      "Epoch 0:  14%|█▍        | 230/1660 [00:27<02:51,  8.34it/s, loss=0.529, v_num=0000]\n",
      "Epoch 0:  14%|█▍        | 231/1660 [00:27<02:51,  8.34it/s, loss=0.53, v_num=0000] \n",
      "Epoch 0:  14%|█▍        | 232/1660 [00:27<02:51,  8.34it/s, loss=0.567, v_num=0000]\n",
      "Epoch 0:  14%|█▍        | 233/1660 [00:27<02:50,  8.35it/s, loss=0.575, v_num=0000]\n",
      "Epoch 0:  14%|█▍        | 234/1660 [00:28<02:50,  8.35it/s, loss=0.585, v_num=0000]\n",
      "Epoch 0:  14%|█▍        | 235/1660 [00:28<02:50,  8.35it/s, loss=0.623, v_num=0000]\n",
      "Epoch 0:  14%|█▍        | 236/1660 [00:28<02:50,  8.35it/s, loss=0.654, v_num=0000]\n",
      "Epoch 0:  14%|█▍        | 237/1660 [00:28<02:50,  8.35it/s, loss=0.656, v_num=0000]\n",
      "Epoch 0:  14%|█▍        | 238/1660 [00:28<02:50,  8.35it/s, loss=0.679, v_num=0000]\n",
      "Epoch 0:  14%|█▍        | 239/1660 [00:28<02:50,  8.36it/s, loss=0.694, v_num=0000]\n",
      "Epoch 0:  14%|█▍        | 240/1660 [00:28<02:50,  8.35it/s, loss=0.692, v_num=0000]\n",
      "Epoch 0:  15%|█▍        | 241/1660 [00:28<02:49,  8.35it/s, loss=0.7, v_num=0000]  \n",
      "Epoch 0:  15%|█▍        | 242/1660 [00:28<02:49,  8.35it/s, loss=0.738, v_num=0000]\n",
      "Epoch 0:  15%|█▍        | 243/1660 [00:29<02:49,  8.35it/s, loss=0.777, v_num=0000]\n",
      "Epoch 0:  15%|█▍        | 244/1660 [00:29<02:49,  8.35it/s, loss=0.789, v_num=0000]\n",
      "Epoch 0:  15%|█▍        | 245/1660 [00:29<02:49,  8.35it/s, loss=0.761, v_num=0000]\n",
      "Epoch 0:  15%|█▍        | 246/1660 [00:29<02:49,  8.36it/s, loss=0.753, v_num=0000]\n",
      "Epoch 0:  15%|█▍        | 247/1660 [00:29<02:49,  8.36it/s, loss=0.77, v_num=0000] \n",
      "Epoch 0:  15%|█▍        | 248/1660 [00:29<02:48,  8.36it/s, loss=0.768, v_num=0000]\n",
      "Epoch 0:  15%|█▌        | 249/1660 [00:29<02:48,  8.36it/s, loss=0.793, v_num=0000]\n",
      "Epoch 0:  15%|█▌        | 250/1660 [00:29<02:48,  8.36it/s, loss=0.801, v_num=0000]\n",
      "Epoch 0:  15%|█▌        | 251/1660 [00:30<02:48,  8.36it/s, loss=0.811, v_num=0000]\n",
      "Epoch 0:  15%|█▌        | 252/1660 [00:30<02:48,  8.36it/s, loss=0.803, v_num=0000]\n",
      "Epoch 0:  15%|█▌        | 253/1660 [00:30<02:48,  8.36it/s, loss=0.801, v_num=0000]\n",
      "Epoch 0:  15%|█▌        | 254/1660 [00:30<02:48,  8.37it/s, loss=0.813, v_num=0000]\n",
      "Epoch 0:  15%|█▌        | 255/1660 [00:30<02:47,  8.37it/s, loss=0.804, v_num=0000]\n",
      "Epoch 0:  15%|█▌        | 256/1660 [00:30<02:47,  8.36it/s, loss=0.781, v_num=0000]\n",
      "Epoch 0:  15%|█▌        | 257/1660 [00:30<02:47,  8.36it/s, loss=0.776, v_num=0000]\n",
      "== Status ==\n",
      "Current time: 2022-06-20 12:00:52 (running for 00:00:45.35)\n",
      "Memory usage on this node: 29.8/376.5 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1.0/8 CPUs, 1.0/1 GPUs, 0.0/34.4 GiB heap, 0.0/17.2 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/dobby/ray_results/tune_rp\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+----------------------+----------+------------------+-----------------+----------------+\n",
      "| Trial name           | status   | loc              |   learning_rate |   weight_decay |\n",
      "|----------------------+----------+------------------+-----------------+----------------|\n",
      "| train_rp_86c32_00000 | RUNNING  | 172.17.0.5:17844 |     0.000368944 |            0.1 |\n",
      "+----------------------+----------+------------------+-----------------+----------------+\n",
      "\n",
      "\n",
      "Epoch 0:  16%|█▌        | 258/1660 [00:30<02:47,  8.36it/s, loss=0.747, v_num=0000]\n",
      "Epoch 0:  16%|█▌        | 259/1660 [00:30<02:47,  8.36it/s, loss=0.732, v_num=0000]\n",
      "Epoch 0:  16%|█▌        | 260/1660 [00:31<02:47,  8.36it/s, loss=0.727, v_num=0000]\n",
      "Epoch 0:  16%|█▌        | 261/1660 [00:31<02:47,  8.37it/s, loss=0.736, v_num=0000]\n",
      "Epoch 0:  16%|█▌        | 262/1660 [00:31<02:47,  8.37it/s, loss=0.691, v_num=0000]\n",
      "Epoch 0:  16%|█▌        | 263/1660 [00:31<02:46,  8.37it/s, loss=0.67, v_num=0000] \n",
      "Epoch 0:  16%|█▌        | 264/1660 [00:31<02:46,  8.37it/s, loss=0.659, v_num=0000]\n",
      "Epoch 0:  16%|█▌        | 265/1660 [00:31<02:46,  8.37it/s, loss=0.685, v_num=0000]\n",
      "Epoch 0:  16%|█▌        | 266/1660 [00:31<02:46,  8.37it/s, loss=0.666, v_num=0000]\n",
      "Epoch 0:  16%|█▌        | 267/1660 [00:31<02:46,  8.37it/s, loss=0.645, v_num=0000]\n",
      "Epoch 0:  16%|█▌        | 268/1660 [00:32<02:46,  8.37it/s, loss=0.614, v_num=0000]\n",
      "Epoch 0:  16%|█▌        | 269/1660 [00:32<02:46,  8.38it/s, loss=0.579, v_num=0000]\n",
      "Epoch 0:  16%|█▋        | 270/1660 [00:32<02:45,  8.38it/s, loss=0.559, v_num=0000]\n",
      "Epoch 0:  16%|█▋        | 271/1660 [00:32<02:45,  8.38it/s, loss=0.545, v_num=0000]\n",
      "Epoch 0:  16%|█▋        | 272/1660 [00:32<02:45,  8.37it/s, loss=0.516, v_num=0000]\n",
      "Epoch 0:  16%|█▋        | 273/1660 [00:32<02:45,  8.37it/s, loss=0.519, v_num=0000]\n",
      "Epoch 0:  17%|█▋        | 274/1660 [00:32<02:45,  8.37it/s, loss=0.495, v_num=0000]\n",
      "Epoch 0:  17%|█▋        | 275/1660 [00:32<02:45,  8.37it/s, loss=0.469, v_num=0000]\n",
      "Epoch 0:  17%|█▋        | 276/1660 [00:32<02:45,  8.37it/s, loss=0.462, v_num=0000]\n",
      "Epoch 0:  17%|█▋        | 277/1660 [00:33<02:45,  8.37it/s, loss=0.476, v_num=0000]\n",
      "Epoch 0:  17%|█▋        | 278/1660 [00:33<02:45,  8.38it/s, loss=0.482, v_num=0000]\n",
      "Epoch 0:  17%|█▋        | 279/1660 [00:33<02:44,  8.38it/s, loss=0.48, v_num=0000] \n",
      "Epoch 0:  17%|█▋        | 280/1660 [00:33<02:44,  8.38it/s, loss=0.494, v_num=0000]\n",
      "Epoch 0:  17%|█▋        | 281/1660 [00:33<02:44,  8.38it/s, loss=0.477, v_num=0000]\n",
      "Epoch 0:  17%|█▋        | 282/1660 [00:33<02:44,  8.38it/s, loss=0.479, v_num=0000]\n",
      "Epoch 0:  17%|█▋        | 283/1660 [00:33<02:44,  8.38it/s, loss=0.472, v_num=0000]\n",
      "Epoch 0:  17%|█▋        | 284/1660 [00:33<02:44,  8.38it/s, loss=0.471, v_num=0000]\n",
      "Epoch 0:  17%|█▋        | 285/1660 [00:33<02:44,  8.38it/s, loss=0.471, v_num=0000]\n",
      "Epoch 0:  17%|█▋        | 285/1660 [00:33<02:44,  8.38it/s, loss=0.454, v_num=0000]\n",
      "Epoch 0:  17%|█▋        | 286/1660 [00:34<02:43,  8.39it/s, loss=0.45, v_num=0000] \n",
      "Epoch 0:  17%|█▋        | 287/1660 [00:34<02:43,  8.39it/s, loss=0.435, v_num=0000]\n",
      "Epoch 0:  17%|█▋        | 288/1660 [00:34<02:43,  8.38it/s, loss=0.449, v_num=0000]\n",
      "Epoch 0:  17%|█▋        | 289/1660 [00:34<02:43,  8.38it/s, loss=0.449, v_num=0000]\n",
      "Epoch 0:  17%|█▋        | 290/1660 [00:34<02:43,  8.38it/s, loss=0.45, v_num=0000] \n",
      "Epoch 0:  18%|█▊        | 291/1660 [00:34<02:43,  8.38it/s, loss=0.469, v_num=0000]\n",
      "Epoch 0:  18%|█▊        | 292/1660 [00:34<02:43,  8.38it/s, loss=0.461, v_num=0000]\n",
      "Epoch 0:  18%|█▊        | 293/1660 [00:34<02:43,  8.38it/s, loss=0.474, v_num=0000]\n",
      "Epoch 0:  18%|█▊        | 294/1660 [00:35<02:42,  8.39it/s, loss=0.489, v_num=0000]\n",
      "Epoch 0:  18%|█▊        | 295/1660 [00:35<02:42,  8.39it/s, loss=0.495, v_num=0000]\n",
      "Epoch 0:  18%|█▊        | 296/1660 [00:35<02:42,  8.39it/s, loss=0.499, v_num=0000]\n",
      "Epoch 0:  18%|█▊        | 297/1660 [00:35<02:42,  8.39it/s, loss=0.484, v_num=0000]\n",
      "Epoch 0:  18%|█▊        | 298/1660 [00:35<02:42,  8.39it/s, loss=0.478, v_num=0000]\n",
      "Epoch 0:  18%|█▊        | 299/1660 [00:35<02:42,  8.39it/s, loss=0.486, v_num=0000]\n",
      "Epoch 0:  18%|█▊        | 300/1660 [00:35<02:42,  8.39it/s, loss=0.474, v_num=0000]\n",
      "== Status ==\n",
      "Current time: 2022-06-20 12:00:57 (running for 00:00:50.35)\n",
      "Memory usage on this node: 29.5/376.5 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1.0/8 CPUs, 1.0/1 GPUs, 0.0/34.4 GiB heap, 0.0/17.2 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/dobby/ray_results/tune_rp\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+----------------------+----------+------------------+-----------------+----------------+\n",
      "| Trial name           | status   | loc              |   learning_rate |   weight_decay |\n",
      "|----------------------+----------+------------------+-----------------+----------------|\n",
      "| train_rp_86c32_00000 | RUNNING  | 172.17.0.5:17844 |     0.000368944 |            0.1 |\n",
      "+----------------------+----------+------------------+-----------------+----------------+\n",
      "\n",
      "\n",
      "Epoch 0:  18%|█▊        | 301/1660 [00:35<02:41,  8.39it/s, loss=0.487, v_num=0000]\n",
      "Epoch 0:  18%|█▊        | 302/1660 [00:35<02:41,  8.39it/s, loss=0.497, v_num=0000]\n",
      "Epoch 0:  18%|█▊        | 303/1660 [00:36<02:41,  8.39it/s, loss=0.499, v_num=0000]\n",
      "Epoch 0:  18%|█▊        | 304/1660 [00:36<02:41,  8.39it/s, loss=0.488, v_num=0000]\n",
      "Epoch 0:  18%|█▊        | 305/1660 [00:36<02:41,  8.39it/s, loss=0.489, v_num=0000]\n",
      "Epoch 0:  18%|█▊        | 306/1660 [00:36<02:41,  8.39it/s, loss=0.495, v_num=0000]\n",
      "Epoch 0:  18%|█▊        | 307/1660 [00:36<02:41,  8.39it/s, loss=0.493, v_num=0000]\n",
      "Epoch 0:  19%|█▊        | 308/1660 [00:36<02:41,  8.39it/s, loss=0.497, v_num=0000]\n",
      "Epoch 0:  19%|█▊        | 309/1660 [00:36<02:40,  8.39it/s, loss=0.491, v_num=0000]\n",
      "Epoch 0:  19%|█▊        | 310/1660 [00:36<02:40,  8.39it/s, loss=0.487, v_num=0000]\n",
      "Epoch 0:  19%|█▊        | 311/1660 [00:37<02:40,  8.39it/s, loss=0.473, v_num=0000]\n",
      "Epoch 0:  19%|█▉        | 312/1660 [00:37<02:40,  8.40it/s, loss=0.483, v_num=0000]\n",
      "Epoch 0:  19%|█▉        | 313/1660 [00:37<02:40,  8.40it/s, loss=0.478, v_num=0000]\n",
      "Epoch 0:  19%|█▉        | 314/1660 [00:37<02:40,  8.40it/s, loss=0.457, v_num=0000]\n",
      "Epoch 0:  19%|█▉        | 315/1660 [00:37<02:40,  8.40it/s, loss=0.456, v_num=0000]\n",
      "Epoch 0:  19%|█▉        | 316/1660 [00:37<02:40,  8.40it/s, loss=0.465, v_num=0000]\n",
      "Epoch 0:  19%|█▉        | 317/1660 [00:37<02:39,  8.40it/s, loss=0.466, v_num=0000]\n",
      "Epoch 0:  19%|█▉        | 318/1660 [00:37<02:39,  8.40it/s, loss=0.465, v_num=0000]\n",
      "Epoch 0:  19%|█▉        | 319/1660 [00:37<02:39,  8.40it/s, loss=0.449, v_num=0000]\n",
      "Epoch 0:  19%|█▉        | 320/1660 [00:38<02:39,  8.39it/s, loss=0.459, v_num=0000]\n",
      "Epoch 0:  19%|█▉        | 321/1660 [00:38<02:39,  8.40it/s, loss=0.447, v_num=0000]\n",
      "Epoch 0:  19%|█▉        | 322/1660 [00:38<02:39,  8.40it/s, loss=0.432, v_num=0000]\n",
      "Epoch 0:  19%|█▉        | 323/1660 [00:38<02:39,  8.40it/s, loss=0.431, v_num=0000]\n",
      "Epoch 0:  20%|█▉        | 324/1660 [00:38<02:39,  8.40it/s, loss=0.434, v_num=0000]\n",
      "Epoch 0:  20%|█▉        | 325/1660 [00:38<02:38,  8.40it/s, loss=0.434, v_num=0000]\n",
      "Epoch 0:  20%|█▉        | 325/1660 [00:38<02:38,  8.40it/s, loss=0.444, v_num=0000]\n",
      "Epoch 0:  20%|█▉        | 326/1660 [00:38<02:38,  8.40it/s, loss=0.44, v_num=0000] \n",
      "Epoch 0:  20%|█▉        | 327/1660 [00:38<02:38,  8.40it/s, loss=0.437, v_num=0000]\n",
      "Epoch 0:  20%|█▉        | 328/1660 [00:39<02:38,  8.40it/s, loss=0.458, v_num=0000]\n",
      "Epoch 0:  20%|█▉        | 329/1660 [00:39<02:38,  8.40it/s, loss=0.466, v_num=0000]\n",
      "Epoch 0:  20%|█▉        | 330/1660 [00:39<02:38,  8.40it/s, loss=0.477, v_num=0000]\n",
      "Epoch 0:  20%|█▉        | 331/1660 [00:39<02:38,  8.40it/s, loss=0.477, v_num=0000]\n",
      "Epoch 0:  20%|██        | 332/1660 [00:39<02:38,  8.40it/s, loss=0.471, v_num=0000]\n",
      "Epoch 0:  20%|██        | 333/1660 [00:39<02:37,  8.40it/s, loss=0.463, v_num=0000]\n",
      "Epoch 0:  20%|██        | 334/1660 [00:39<02:37,  8.40it/s, loss=0.468, v_num=0000]\n",
      "Epoch 0:  20%|██        | 335/1660 [00:39<02:37,  8.41it/s, loss=0.471, v_num=0000]\n",
      "Epoch 0:  20%|██        | 336/1660 [00:40<02:37,  8.40it/s, loss=0.466, v_num=0000]\n",
      "Epoch 0:  20%|██        | 337/1660 [00:40<02:37,  8.40it/s, loss=0.486, v_num=0000]\n",
      "Epoch 0:  20%|██        | 338/1660 [00:40<02:37,  8.40it/s, loss=0.496, v_num=0000]\n",
      "Epoch 0:  20%|██        | 339/1660 [00:40<02:37,  8.40it/s, loss=0.508, v_num=0000]\n",
      "Epoch 0:  20%|██        | 340/1660 [00:40<02:37,  8.40it/s, loss=0.5, v_num=0000]  \n",
      "Epoch 0:  21%|██        | 341/1660 [00:40<02:36,  8.40it/s, loss=0.52, v_num=0000]\n",
      "Epoch 0:  21%|██        | 342/1660 [00:40<02:36,  8.40it/s, loss=0.528, v_num=0000]\n",
      "Epoch 0:  21%|██        | 343/1660 [00:40<02:36,  8.40it/s, loss=0.531, v_num=0000]\n",
      "== Status ==\n",
      "Current time: 2022-06-20 12:01:02 (running for 00:00:55.35)\n",
      "Memory usage on this node: 29.5/376.5 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1.0/8 CPUs, 1.0/1 GPUs, 0.0/34.4 GiB heap, 0.0/17.2 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/dobby/ray_results/tune_rp\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+----------------------+----------+------------------+-----------------+----------------+\n",
      "| Trial name           | status   | loc              |   learning_rate |   weight_decay |\n",
      "|----------------------+----------+------------------+-----------------+----------------|\n",
      "| train_rp_86c32_00000 | RUNNING  | 172.17.0.5:17844 |     0.000368944 |            0.1 |\n",
      "+----------------------+----------+------------------+-----------------+----------------+\n",
      "\n",
      "\n",
      "Epoch 0:  21%|██        | 344/1660 [00:40<02:36,  8.40it/s, loss=0.525, v_num=0000]\n",
      "Epoch 0:  21%|██        | 345/1660 [00:41<02:36,  8.40it/s, loss=0.539, v_num=0000]\n",
      "Epoch 0:  21%|██        | 346/1660 [00:41<02:36,  8.41it/s, loss=0.531, v_num=0000]\n",
      "Epoch 0:  21%|██        | 347/1660 [00:41<02:36,  8.41it/s, loss=0.53, v_num=0000] \n",
      "Epoch 0:  21%|██        | 348/1660 [00:41<02:36,  8.41it/s, loss=0.51, v_num=0000]\n",
      "Epoch 0:  21%|██        | 349/1660 [00:41<02:35,  8.41it/s, loss=0.491, v_num=0000]\n",
      "Epoch 0:  21%|██        | 350/1660 [00:41<02:35,  8.41it/s, loss=0.475, v_num=0000]\n",
      "Epoch 0:  21%|██        | 351/1660 [00:41<02:35,  8.41it/s, loss=0.48, v_num=0000] \n",
      "Epoch 0:  21%|██        | 352/1660 [00:41<02:35,  8.40it/s, loss=0.499, v_num=0000]\n",
      "Epoch 0:  21%|██▏       | 353/1660 [00:42<02:35,  8.40it/s, loss=0.499, v_num=0000]\n",
      "Epoch 0:  21%|██▏       | 354/1660 [00:42<02:35,  8.40it/s, loss=0.501, v_num=0000]\n",
      "Epoch 0:  21%|██▏       | 355/1660 [00:42<02:35,  8.40it/s, loss=0.49, v_num=0000] \n",
      "Epoch 0:  21%|██▏       | 356/1660 [00:42<02:35,  8.41it/s, loss=0.486, v_num=0000]\n",
      "Epoch 0:  22%|██▏       | 357/1660 [00:42<02:35,  8.41it/s, loss=0.47, v_num=0000] \n",
      "Epoch 0:  22%|██▏       | 358/1660 [00:42<02:34,  8.41it/s, loss=0.457, v_num=0000]\n",
      "Epoch 0:  22%|██▏       | 359/1660 [00:42<02:34,  8.41it/s, loss=0.448, v_num=0000]\n",
      "Epoch 0:  22%|██▏       | 360/1660 [00:42<02:34,  8.41it/s, loss=0.456, v_num=0000]\n",
      "Epoch 0:  22%|██▏       | 361/1660 [00:42<02:34,  8.41it/s, loss=0.437, v_num=0000]\n",
      "Epoch 0:  22%|██▏       | 362/1660 [00:43<02:34,  8.41it/s, loss=0.445, v_num=0000]\n",
      "Epoch 0:  22%|██▏       | 363/1660 [00:43<02:34,  8.41it/s, loss=0.434, v_num=0000]\n",
      "Epoch 0:  22%|██▏       | 364/1660 [00:43<02:34,  8.41it/s, loss=0.439, v_num=0000]\n",
      "Epoch 0:  22%|██▏       | 365/1660 [00:43<02:33,  8.41it/s, loss=0.417, v_num=0000]\n",
      "Epoch 0:  22%|██▏       | 366/1660 [00:43<02:33,  8.41it/s, loss=0.425, v_num=0000]\n",
      "Epoch 0:  22%|██▏       | 367/1660 [00:43<02:33,  8.41it/s, loss=0.428, v_num=0000]\n",
      "Epoch 0:  22%|██▏       | 368/1660 [00:43<02:33,  8.41it/s, loss=0.422, v_num=0000]\n",
      "Epoch 0:  22%|██▏       | 369/1660 [00:43<02:33,  8.41it/s, loss=0.436, v_num=0000]\n",
      "Epoch 0:  22%|██▏       | 370/1660 [00:44<02:33,  8.41it/s, loss=0.444, v_num=0000]\n",
      "Epoch 0:  22%|██▏       | 371/1660 [00:44<02:33,  8.41it/s, loss=0.431, v_num=0000]\n",
      "Epoch 0:  22%|██▏       | 372/1660 [00:44<02:33,  8.41it/s, loss=0.425, v_num=0000]\n",
      "Epoch 0:  22%|██▏       | 373/1660 [00:44<02:33,  8.41it/s, loss=0.424, v_num=0000]\n",
      "Epoch 0:  23%|██▎       | 374/1660 [00:44<02:32,  8.41it/s, loss=0.419, v_num=0000]\n",
      "Epoch 0:  23%|██▎       | 375/1660 [00:44<02:32,  8.41it/s, loss=0.418, v_num=0000]\n",
      "Epoch 0:  23%|██▎       | 376/1660 [00:44<02:32,  8.41it/s, loss=0.417, v_num=0000]\n",
      "Epoch 0:  23%|██▎       | 377/1660 [00:44<02:32,  8.41it/s, loss=0.416, v_num=0000]\n",
      "Epoch 0:  23%|██▎       | 378/1660 [00:44<02:32,  8.41it/s, loss=0.417, v_num=0000]\n",
      "Epoch 0:  23%|██▎       | 379/1660 [00:45<02:32,  8.41it/s, loss=0.427, v_num=0000]\n",
      "Epoch 0:  23%|██▎       | 380/1660 [00:45<02:32,  8.41it/s, loss=0.414, v_num=0000]\n",
      "Epoch 0:  23%|██▎       | 381/1660 [00:45<02:31,  8.42it/s, loss=0.418, v_num=0000]\n",
      "Epoch 0:  23%|██▎       | 382/1660 [00:45<02:31,  8.42it/s, loss=0.408, v_num=0000]\n",
      "Epoch 0:  23%|██▎       | 383/1660 [00:45<02:31,  8.42it/s, loss=0.403, v_num=0000]\n",
      "Epoch 0:  23%|██▎       | 384/1660 [00:45<02:31,  8.41it/s, loss=0.397, v_num=0000]\n",
      "Epoch 0:  23%|██▎       | 385/1660 [00:45<02:31,  8.41it/s, loss=0.402, v_num=0000]\n",
      "== Status ==\n",
      "Current time: 2022-06-20 12:01:07 (running for 00:01:00.36)\n",
      "Memory usage on this node: 29.5/376.5 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1.0/8 CPUs, 1.0/1 GPUs, 0.0/34.4 GiB heap, 0.0/17.2 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/dobby/ray_results/tune_rp\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+----------------------+----------+------------------+-----------------+----------------+\n",
      "| Trial name           | status   | loc              |   learning_rate |   weight_decay |\n",
      "|----------------------+----------+------------------+-----------------+----------------|\n",
      "| train_rp_86c32_00000 | RUNNING  | 172.17.0.5:17844 |     0.000368944 |            0.1 |\n",
      "+----------------------+----------+------------------+-----------------+----------------+\n",
      "\n",
      "\n",
      "Epoch 0:  23%|██▎       | 386/1660 [00:45<02:31,  8.41it/s, loss=0.411, v_num=0000]\n",
      "Epoch 0:  23%|██▎       | 387/1660 [00:46<02:31,  8.41it/s, loss=0.417, v_num=0000]\n",
      "Epoch 0:  23%|██▎       | 388/1660 [00:46<02:31,  8.41it/s, loss=0.428, v_num=0000]\n",
      "Epoch 0:  23%|██▎       | 389/1660 [00:46<02:31,  8.41it/s, loss=0.436, v_num=0000]\n",
      "Epoch 0:  23%|██▎       | 390/1660 [00:46<02:30,  8.41it/s, loss=0.44, v_num=0000] \n",
      "Epoch 0:  24%|██▎       | 391/1660 [00:46<02:30,  8.42it/s, loss=0.439, v_num=0000]\n",
      "Epoch 0:  24%|██▎       | 392/1660 [00:46<02:30,  8.42it/s, loss=0.434, v_num=0000]\n",
      "Epoch 0:  24%|██▎       | 393/1660 [00:46<02:30,  8.42it/s, loss=0.436, v_num=0000]\n",
      "Epoch 0:  24%|██▎       | 394/1660 [00:46<02:30,  8.42it/s, loss=0.432, v_num=0000]\n",
      "Epoch 0:  24%|██▍       | 395/1660 [00:46<02:30,  8.42it/s, loss=0.451, v_num=0000]\n",
      "Epoch 0:  24%|██▍       | 396/1660 [00:47<02:30,  8.42it/s, loss=0.455, v_num=0000]\n",
      "Epoch 0:  24%|██▍       | 397/1660 [00:47<02:30,  8.42it/s, loss=0.454, v_num=0000]\n",
      "Epoch 0:  24%|██▍       | 398/1660 [00:47<02:29,  8.42it/s, loss=0.477, v_num=0000]\n",
      "Epoch 0:  24%|██▍       | 399/1660 [00:47<02:29,  8.42it/s, loss=0.475, v_num=0000]\n",
      "Epoch 0:  24%|██▍       | 400/1660 [00:47<02:29,  8.41it/s, loss=0.477, v_num=0000]\n",
      "Epoch 0:  24%|██▍       | 401/1660 [00:47<02:29,  8.41it/s, loss=0.479, v_num=0000]\n",
      "Epoch 0:  24%|██▍       | 402/1660 [00:47<02:29,  8.41it/s, loss=0.487, v_num=0000]\n",
      "Epoch 0:  24%|██▍       | 403/1660 [00:47<02:29,  8.41it/s, loss=0.49, v_num=0000] \n",
      "Epoch 0:  24%|██▍       | 404/1660 [00:48<02:29,  8.41it/s, loss=0.502, v_num=0000]\n",
      "Epoch 0:  24%|██▍       | 405/1660 [00:48<02:29,  8.42it/s, loss=0.495, v_num=0000]\n",
      "Epoch 0:  24%|██▍       | 406/1660 [00:48<02:29,  8.42it/s, loss=0.489, v_num=0000]\n",
      "Epoch 0:  25%|██▍       | 407/1660 [00:48<02:28,  8.42it/s, loss=0.473, v_num=0000]\n",
      "Epoch 0:  25%|██▍       | 408/1660 [00:48<02:28,  8.42it/s, loss=0.469, v_num=0000]\n",
      "Epoch 0:  25%|██▍       | 409/1660 [00:48<02:28,  8.42it/s, loss=0.466, v_num=0000]\n",
      "Epoch 0:  25%|██▍       | 410/1660 [00:48<02:28,  8.42it/s, loss=0.453, v_num=0000]\n",
      "Epoch 0:  25%|██▍       | 411/1660 [00:48<02:28,  8.42it/s, loss=0.457, v_num=0000]\n",
      "Epoch 0:  25%|██▍       | 412/1660 [00:48<02:28,  8.42it/s, loss=0.447, v_num=0000]\n",
      "Epoch 0:  25%|██▍       | 413/1660 [00:49<02:28,  8.42it/s, loss=0.449, v_num=0000]\n",
      "Epoch 0:  25%|██▍       | 414/1660 [00:49<02:27,  8.42it/s, loss=0.455, v_num=0000]\n",
      "Epoch 0:  25%|██▌       | 415/1660 [00:49<02:27,  8.42it/s, loss=0.439, v_num=0000]\n",
      "Epoch 0:  25%|██▌       | 416/1660 [00:49<02:27,  8.41it/s, loss=0.442, v_num=0000]\n",
      "Epoch 0:  25%|██▌       | 417/1660 [00:49<02:27,  8.41it/s, loss=0.448, v_num=0000]\n",
      "Epoch 0:  25%|██▌       | 418/1660 [00:49<02:27,  8.42it/s, loss=0.428, v_num=0000]\n",
      "Epoch 0:  25%|██▌       | 419/1660 [00:49<02:27,  8.42it/s, loss=0.421, v_num=0000]\n",
      "Epoch 0:  25%|██▌       | 420/1660 [00:49<02:27,  8.42it/s, loss=0.432, v_num=0000]\n",
      "Epoch 0:  25%|██▌       | 421/1660 [00:50<02:27,  8.42it/s, loss=0.425, v_num=0000]\n",
      "Epoch 0:  25%|██▌       | 422/1660 [00:50<02:27,  8.42it/s, loss=0.42, v_num=0000] \n",
      "Epoch 0:  25%|██▌       | 423/1660 [00:50<02:26,  8.42it/s, loss=0.421, v_num=0000]\n",
      "Epoch 0:  26%|██▌       | 424/1660 [00:50<02:26,  8.42it/s, loss=0.413, v_num=0000]\n",
      "Epoch 0:  26%|██▌       | 425/1660 [00:50<02:26,  8.42it/s, loss=0.411, v_num=0000]\n",
      "Epoch 0:  26%|██▌       | 426/1660 [00:50<02:26,  8.42it/s, loss=0.406, v_num=0000]\n",
      "Epoch 0:  26%|██▌       | 427/1660 [00:50<02:26,  8.42it/s, loss=0.406, v_num=0000]\n",
      "Epoch 0:  26%|██▌       | 428/1660 [00:50<02:26,  8.42it/s, loss=0.392, v_num=0000]\n",
      "== Status ==\n",
      "Current time: 2022-06-20 12:01:12 (running for 00:01:05.36)\n",
      "Memory usage on this node: 29.5/376.5 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1.0/8 CPUs, 1.0/1 GPUs, 0.0/34.4 GiB heap, 0.0/17.2 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/dobby/ray_results/tune_rp\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+----------------------+----------+------------------+-----------------+----------------+\n",
      "| Trial name           | status   | loc              |   learning_rate |   weight_decay |\n",
      "|----------------------+----------+------------------+-----------------+----------------|\n",
      "| train_rp_86c32_00000 | RUNNING  | 172.17.0.5:17844 |     0.000368944 |            0.1 |\n",
      "+----------------------+----------+------------------+-----------------+----------------+\n",
      "\n",
      "\n",
      "Epoch 0:  26%|██▌       | 429/1660 [00:50<02:26,  8.42it/s, loss=0.382, v_num=0000]\n",
      "Epoch 0:  26%|██▌       | 430/1660 [00:51<02:26,  8.42it/s, loss=0.386, v_num=0000]\n",
      "Epoch 0:  26%|██▌       | 431/1660 [00:51<02:25,  8.42it/s, loss=0.386, v_num=0000]\n",
      "Epoch 0:  26%|██▌       | 432/1660 [00:51<02:25,  8.42it/s, loss=0.385, v_num=0000]\n",
      "Epoch 0:  26%|██▌       | 433/1660 [00:51<02:25,  8.42it/s, loss=0.377, v_num=0000]\n",
      "Epoch 0:  26%|██▌       | 434/1660 [00:51<02:25,  8.42it/s, loss=0.39, v_num=0000] \n",
      "Epoch 0:  26%|██▌       | 435/1660 [00:51<02:25,  8.42it/s, loss=0.391, v_num=0000]\n",
      "Epoch 0:  26%|██▋       | 436/1660 [00:51<02:25,  8.42it/s, loss=0.377, v_num=0000]\n",
      "Epoch 0:  26%|██▋       | 437/1660 [00:51<02:25,  8.42it/s, loss=0.374, v_num=0000]\n",
      "Epoch 0:  26%|██▋       | 438/1660 [00:52<02:25,  8.42it/s, loss=0.375, v_num=0000]\n",
      "Epoch 0:  26%|██▋       | 439/1660 [00:52<02:25,  8.42it/s, loss=0.385, v_num=0000]\n",
      "Epoch 0:  27%|██▋       | 440/1660 [00:52<02:24,  8.42it/s, loss=0.378, v_num=0000]\n",
      "Epoch 0:  27%|██▋       | 441/1660 [00:52<02:24,  8.42it/s, loss=0.382, v_num=0000]\n",
      "Epoch 0:  27%|██▋       | 442/1660 [00:52<02:24,  8.42it/s, loss=0.382, v_num=0000]\n",
      "Epoch 0:  27%|██▋       | 442/1660 [00:52<02:24,  8.42it/s, loss=0.376, v_num=0000]\n",
      "Epoch 0:  27%|██▋       | 443/1660 [00:52<02:24,  8.42it/s, loss=0.372, v_num=0000]\n",
      "Epoch 0:  27%|██▋       | 444/1660 [00:52<02:24,  8.42it/s, loss=0.379, v_num=0000]\n",
      "Epoch 0:  27%|██▋       | 445/1660 [00:52<02:24,  8.42it/s, loss=0.386, v_num=0000]\n",
      "Epoch 0:  27%|██▋       | 446/1660 [00:52<02:24,  8.42it/s, loss=0.385, v_num=0000]\n",
      "Epoch 0:  27%|██▋       | 447/1660 [00:53<02:24,  8.42it/s, loss=0.394, v_num=0000]\n",
      "Epoch 0:  27%|██▋       | 448/1660 [00:53<02:23,  8.42it/s, loss=0.407, v_num=0000]\n",
      "Epoch 0:  27%|██▋       | 449/1660 [00:53<02:23,  8.42it/s, loss=0.409, v_num=0000]\n",
      "Epoch 0:  27%|██▋       | 450/1660 [00:53<02:23,  8.42it/s, loss=0.414, v_num=0000]\n",
      "Epoch 0:  27%|██▋       | 451/1660 [00:53<02:23,  8.42it/s, loss=0.405, v_num=0000]\n",
      "Epoch 0:  27%|██▋       | 452/1660 [00:53<02:23,  8.42it/s, loss=0.41, v_num=0000] \n",
      "Epoch 0:  27%|██▋       | 453/1660 [00:53<02:23,  8.42it/s, loss=0.413, v_num=0000]\n",
      "Epoch 0:  27%|██▋       | 454/1660 [00:53<02:23,  8.42it/s, loss=0.4, v_num=0000]  \n",
      "Epoch 0:  27%|██▋       | 455/1660 [00:54<02:23,  8.42it/s, loss=0.403, v_num=0000]\n",
      "Epoch 0:  27%|██▋       | 456/1660 [00:54<02:22,  8.42it/s, loss=0.404, v_num=0000]\n",
      "Epoch 0:  28%|██▊       | 457/1660 [00:54<02:22,  8.42it/s, loss=0.401, v_num=0000]\n",
      "Epoch 0:  28%|██▊       | 458/1660 [00:54<02:22,  8.42it/s, loss=0.399, v_num=0000]\n",
      "Epoch 0:  28%|██▊       | 459/1660 [00:54<02:22,  8.42it/s, loss=0.394, v_num=0000]\n",
      "Epoch 0:  28%|██▊       | 460/1660 [00:54<02:22,  8.42it/s, loss=0.394, v_num=0000]\n",
      "Epoch 0:  28%|██▊       | 461/1660 [00:54<02:22,  8.42it/s, loss=0.386, v_num=0000]\n",
      "Epoch 0:  28%|██▊       | 462/1660 [00:54<02:22,  8.42it/s, loss=0.391, v_num=0000]\n",
      "Epoch 0:  28%|██▊       | 463/1660 [00:54<02:22,  8.42it/s, loss=0.385, v_num=0000]\n",
      "Epoch 0:  28%|██▊       | 464/1660 [00:55<02:22,  8.42it/s, loss=0.381, v_num=0000]\n",
      "Epoch 0:  28%|██▊       | 465/1660 [00:55<02:21,  8.42it/s, loss=0.371, v_num=0000]\n",
      "Epoch 0:  28%|██▊       | 466/1660 [00:55<02:21,  8.42it/s, loss=0.367, v_num=0000]\n",
      "Epoch 0:  28%|██▊       | 467/1660 [00:55<02:21,  8.42it/s, loss=0.367, v_num=0000]\n",
      "Epoch 0:  28%|██▊       | 468/1660 [00:55<02:21,  8.42it/s, loss=0.369, v_num=0000]\n",
      "Epoch 0:  28%|██▊       | 469/1660 [00:55<02:21,  8.42it/s, loss=0.378, v_num=0000]\n",
      "Epoch 0:  28%|██▊       | 470/1660 [00:55<02:21,  8.42it/s, loss=0.38, v_num=0000] \n",
      "== Status ==\n",
      "Current time: 2022-06-20 12:01:17 (running for 00:01:10.36)\n",
      "Memory usage on this node: 29.5/376.5 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1.0/8 CPUs, 1.0/1 GPUs, 0.0/34.4 GiB heap, 0.0/17.2 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/dobby/ray_results/tune_rp\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+----------------------+----------+------------------+-----------------+----------------+\n",
      "| Trial name           | status   | loc              |   learning_rate |   weight_decay |\n",
      "|----------------------+----------+------------------+-----------------+----------------|\n",
      "| train_rp_86c32_00000 | RUNNING  | 172.17.0.5:17844 |     0.000368944 |            0.1 |\n",
      "+----------------------+----------+------------------+-----------------+----------------+\n",
      "\n",
      "\n",
      "Epoch 0:  28%|██▊       | 471/1660 [00:55<02:21,  8.42it/s, loss=0.387, v_num=0000]\n",
      "Epoch 0:  28%|██▊       | 472/1660 [00:56<02:21,  8.42it/s, loss=0.394, v_num=0000]\n",
      "Epoch 0:  28%|██▊       | 473/1660 [00:56<02:20,  8.42it/s, loss=0.394, v_num=0000]\n",
      "Epoch 0:  28%|██▊       | 473/1660 [00:56<02:20,  8.42it/s, loss=0.398, v_num=0000]\n",
      "Epoch 0:  29%|██▊       | 474/1660 [00:56<02:20,  8.42it/s, loss=0.402, v_num=0000]\n",
      "Epoch 0:  29%|██▊       | 475/1660 [00:56<02:20,  8.42it/s, loss=0.404, v_num=0000]\n",
      "Epoch 0:  29%|██▊       | 476/1660 [00:56<02:20,  8.42it/s, loss=0.42, v_num=0000] \n",
      "Epoch 0:  29%|██▊       | 477/1660 [00:56<02:20,  8.42it/s, loss=0.423, v_num=0000]\n",
      "Epoch 0:  29%|██▉       | 478/1660 [00:56<02:20,  8.42it/s, loss=0.436, v_num=0000]\n",
      "Epoch 0:  29%|██▉       | 479/1660 [00:56<02:20,  8.42it/s, loss=0.433, v_num=0000]\n",
      "Epoch 0:  29%|██▉       | 480/1660 [00:57<02:20,  8.42it/s, loss=0.428, v_num=0000]\n",
      "Epoch 0:  29%|██▉       | 481/1660 [00:57<02:20,  8.42it/s, loss=0.426, v_num=0000]\n",
      "Epoch 0:  29%|██▉       | 482/1660 [00:57<02:19,  8.42it/s, loss=0.427, v_num=0000]\n",
      "Epoch 0:  29%|██▉       | 483/1660 [00:57<02:19,  8.42it/s, loss=0.436, v_num=0000]\n",
      "Epoch 0:  29%|██▉       | 484/1660 [00:57<02:19,  8.42it/s, loss=0.434, v_num=0000]\n",
      "Epoch 0:  29%|██▉       | 485/1660 [00:57<02:19,  8.42it/s, loss=0.435, v_num=0000]\n",
      "Epoch 0:  29%|██▉       | 486/1660 [00:57<02:19,  8.42it/s, loss=0.451, v_num=0000]\n",
      "Epoch 0:  29%|██▉       | 487/1660 [00:57<02:19,  8.42it/s, loss=0.446, v_num=0000]\n",
      "Epoch 0:  29%|██▉       | 488/1660 [00:57<02:19,  8.42it/s, loss=0.438, v_num=0000]\n",
      "Epoch 0:  29%|██▉       | 489/1660 [00:58<02:19,  8.42it/s, loss=0.437, v_num=0000]\n",
      "Epoch 0:  30%|██▉       | 490/1660 [00:58<02:18,  8.42it/s, loss=0.432, v_num=0000]\n",
      "Epoch 0:  30%|██▉       | 491/1660 [00:58<02:18,  8.42it/s, loss=0.433, v_num=0000]\n",
      "Epoch 0:  30%|██▉       | 492/1660 [00:58<02:18,  8.42it/s, loss=0.432, v_num=0000]\n",
      "Epoch 0:  30%|██▉       | 493/1660 [00:58<02:18,  8.42it/s, loss=0.427, v_num=0000]\n",
      "Epoch 0:  30%|██▉       | 494/1660 [00:58<02:18,  8.42it/s, loss=0.422, v_num=0000]\n",
      "Epoch 0:  30%|██▉       | 495/1660 [00:58<02:18,  8.42it/s, loss=0.41, v_num=0000] \n",
      "Epoch 0:  30%|██▉       | 496/1660 [00:58<02:18,  8.42it/s, loss=0.396, v_num=0000]\n",
      "Epoch 0:  30%|██▉       | 497/1660 [00:59<02:18,  8.42it/s, loss=0.397, v_num=0000]\n",
      "Epoch 0:  30%|███       | 498/1660 [00:59<02:18,  8.42it/s, loss=0.386, v_num=0000]\n",
      "Epoch 0:  30%|███       | 499/1660 [00:59<02:17,  8.42it/s, loss=0.392, v_num=0000]\n",
      "Epoch 0:  30%|███       | 500/1660 [00:59<02:17,  8.42it/s, loss=0.4, v_num=0000]  \n",
      "Epoch 0:  30%|███       | 501/1660 [00:59<02:17,  8.42it/s, loss=0.409, v_num=0000]\n",
      "Epoch 0:  30%|███       | 502/1660 [00:59<02:17,  8.42it/s, loss=0.409, v_num=0000]\n",
      "Epoch 0:  30%|███       | 503/1660 [00:59<02:17,  8.42it/s, loss=0.408, v_num=0000]\n",
      "Epoch 0:  30%|███       | 504/1660 [00:59<02:17,  8.42it/s, loss=0.401, v_num=0000]\n",
      "Epoch 0:  30%|███       | 505/1660 [00:59<02:17,  8.42it/s, loss=0.4, v_num=0000]  \n",
      "Epoch 0:  30%|███       | 506/1660 [01:00<02:17,  8.42it/s, loss=0.396, v_num=0000]\n",
      "Epoch 0:  31%|███       | 507/1660 [01:00<02:16,  8.42it/s, loss=0.398, v_num=0000]\n",
      "Epoch 0:  31%|███       | 508/1660 [01:00<02:16,  8.42it/s, loss=0.396, v_num=0000]\n",
      "Epoch 0:  31%|███       | 509/1660 [01:00<02:16,  8.42it/s, loss=0.388, v_num=0000]\n",
      "Epoch 0:  31%|███       | 510/1660 [01:00<02:16,  8.42it/s, loss=0.385, v_num=0000]\n",
      "Epoch 0:  31%|███       | 511/1660 [01:00<02:16,  8.42it/s, loss=0.386, v_num=0000]\n",
      "== Status ==\n",
      "Current time: 2022-06-20 12:01:22 (running for 00:01:15.37)\n",
      "Memory usage on this node: 29.5/376.5 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1.0/8 CPUs, 1.0/1 GPUs, 0.0/34.4 GiB heap, 0.0/17.2 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/dobby/ray_results/tune_rp\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+----------------------+----------+------------------+-----------------+----------------+\n",
      "| Trial name           | status   | loc              |   learning_rate |   weight_decay |\n",
      "|----------------------+----------+------------------+-----------------+----------------|\n",
      "| train_rp_86c32_00000 | RUNNING  | 172.17.0.5:17844 |     0.000368944 |            0.1 |\n",
      "+----------------------+----------+------------------+-----------------+----------------+\n",
      "\n",
      "\n",
      "Epoch 0:  31%|███       | 512/1660 [01:00<02:16,  8.42it/s, loss=0.38, v_num=0000] \n",
      "Epoch 0:  31%|███       | 513/1660 [01:00<02:16,  8.42it/s, loss=0.39, v_num=0000]\n",
      "Epoch 0:  31%|███       | 514/1660 [01:01<02:16,  8.42it/s, loss=0.391, v_num=0000]\n",
      "Epoch 0:  31%|███       | 515/1660 [01:01<02:16,  8.42it/s, loss=0.396, v_num=0000]\n",
      "Epoch 0:  31%|███       | 516/1660 [01:01<02:15,  8.42it/s, loss=0.404, v_num=0000]\n",
      "Epoch 0:  31%|███       | 517/1660 [01:01<02:15,  8.42it/s, loss=0.394, v_num=0000]\n",
      "Epoch 0:  31%|███       | 518/1660 [01:01<02:15,  8.42it/s, loss=0.393, v_num=0000]\n",
      "Epoch 0:  31%|███▏      | 519/1660 [01:01<02:15,  8.42it/s, loss=0.386, v_num=0000]\n",
      "Epoch 0:  31%|███▏      | 520/1660 [01:01<02:15,  8.42it/s, loss=0.38, v_num=0000] \n",
      "Epoch 0:  31%|███▏      | 521/1660 [01:01<02:15,  8.42it/s, loss=0.375, v_num=0000]\n",
      "Epoch 0:  31%|███▏      | 522/1660 [01:02<02:15,  8.42it/s, loss=0.381, v_num=0000]\n",
      "Epoch 0:  32%|███▏      | 523/1660 [01:02<02:15,  8.42it/s, loss=0.38, v_num=0000] \n",
      "Epoch 0:  32%|███▏      | 524/1660 [01:02<02:14,  8.42it/s, loss=0.386, v_num=0000]\n",
      "Epoch 0:  32%|███▏      | 525/1660 [01:02<02:14,  8.42it/s, loss=0.385, v_num=0000]\n",
      "Epoch 0:  32%|███▏      | 526/1660 [01:02<02:14,  8.42it/s, loss=0.384, v_num=0000]\n",
      "Epoch 0:  32%|███▏      | 527/1660 [01:02<02:14,  8.42it/s, loss=0.387, v_num=0000]\n",
      "Epoch 0:  32%|███▏      | 528/1660 [01:02<02:14,  8.42it/s, loss=0.394, v_num=0000]\n",
      "Epoch 0:  32%|███▏      | 529/1660 [01:02<02:14,  8.42it/s, loss=0.407, v_num=0000]\n",
      "Epoch 0:  32%|███▏      | 530/1660 [01:02<02:14,  8.42it/s, loss=0.409, v_num=0000]\n",
      "Epoch 0:  32%|███▏      | 531/1660 [01:03<02:14,  8.42it/s, loss=0.396, v_num=0000]\n",
      "Epoch 0:  32%|███▏      | 532/1660 [01:03<02:14,  8.42it/s, loss=0.4, v_num=0000]  \n",
      "Epoch 0:  32%|███▏      | 533/1660 [01:03<02:13,  8.42it/s, loss=0.395, v_num=0000]\n",
      "Epoch 0:  32%|███▏      | 534/1660 [01:03<02:13,  8.42it/s, loss=0.399, v_num=0000]\n",
      "Epoch 0:  32%|███▏      | 535/1660 [01:03<02:13,  8.42it/s, loss=0.401, v_num=0000]\n",
      "Epoch 0:  32%|███▏      | 536/1660 [01:03<02:13,  8.42it/s, loss=0.391, v_num=0000]\n",
      "Epoch 0:  32%|███▏      | 537/1660 [01:03<02:13,  8.42it/s, loss=0.397, v_num=0000]\n",
      "Epoch 0:  32%|███▏      | 538/1660 [01:03<02:13,  8.42it/s, loss=0.405, v_num=0000]\n",
      "Epoch 0:  32%|███▏      | 539/1660 [01:04<02:13,  8.42it/s, loss=0.409, v_num=0000]\n",
      "Epoch 0:  33%|███▎      | 540/1660 [01:04<02:13,  8.42it/s, loss=0.408, v_num=0000]\n",
      "Epoch 0:  33%|███▎      | 541/1660 [01:04<02:12,  8.42it/s, loss=0.408, v_num=0000]\n",
      "Epoch 0:  33%|███▎      | 542/1660 [01:04<02:12,  8.42it/s, loss=0.395, v_num=0000]\n",
      "Epoch 0:  33%|███▎      | 543/1660 [01:04<02:12,  8.42it/s, loss=0.398, v_num=0000]\n",
      "Epoch 0:  33%|███▎      | 544/1660 [01:04<02:12,  8.42it/s, loss=0.399, v_num=0000]\n",
      "Epoch 0:  33%|███▎      | 545/1660 [01:04<02:12,  8.42it/s, loss=0.409, v_num=0000]\n",
      "Epoch 0:  33%|███▎      | 546/1660 [01:04<02:12,  8.42it/s, loss=0.409, v_num=0000]\n",
      "Epoch 0:  33%|███▎      | 547/1660 [01:04<02:12,  8.42it/s, loss=0.411, v_num=0000]\n",
      "Epoch 0:  33%|███▎      | 548/1660 [01:05<02:12,  8.42it/s, loss=0.4, v_num=0000]  \n",
      "Epoch 0:  33%|███▎      | 549/1660 [01:05<02:12,  8.42it/s, loss=0.388, v_num=0000]\n",
      "Epoch 0:  33%|███▎      | 550/1660 [01:05<02:11,  8.42it/s, loss=0.386, v_num=0000]\n",
      "Epoch 0:  33%|███▎      | 551/1660 [01:05<02:11,  8.41it/s, loss=0.396, v_num=0000]\n",
      "Epoch 0:  33%|███▎      | 552/1660 [01:05<02:11,  8.41it/s, loss=0.396, v_num=0000]\n",
      "Epoch 0:  33%|███▎      | 553/1660 [01:05<02:11,  8.41it/s, loss=0.389, v_num=0000]\n",
      "== Status ==\n",
      "Current time: 2022-06-20 12:01:27 (running for 00:01:20.37)\n",
      "Memory usage on this node: 29.7/376.5 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1.0/8 CPUs, 1.0/1 GPUs, 0.0/34.4 GiB heap, 0.0/17.2 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/dobby/ray_results/tune_rp\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+----------------------+----------+------------------+-----------------+----------------+\n",
      "| Trial name           | status   | loc              |   learning_rate |   weight_decay |\n",
      "|----------------------+----------+------------------+-----------------+----------------|\n",
      "| train_rp_86c32_00000 | RUNNING  | 172.17.0.5:17844 |     0.000368944 |            0.1 |\n",
      "+----------------------+----------+------------------+-----------------+----------------+\n",
      "\n",
      "\n",
      "Epoch 0:  33%|███▎      | 554/1660 [01:05<02:11,  8.41it/s, loss=0.387, v_num=0000]\n",
      "Epoch 0:  33%|███▎      | 555/1660 [01:05<02:11,  8.41it/s, loss=0.385, v_num=0000]\n",
      "Epoch 0:  33%|███▎      | 556/1660 [01:06<02:11,  8.41it/s, loss=0.39, v_num=0000] \n",
      "Epoch 0:  34%|███▎      | 557/1660 [01:06<02:11,  8.41it/s, loss=0.388, v_num=0000]\n",
      "Epoch 0:  34%|███▎      | 558/1660 [01:06<02:10,  8.41it/s, loss=0.388, v_num=0000]\n",
      "Epoch 0:  34%|███▎      | 559/1660 [01:06<02:10,  8.41it/s, loss=0.389, v_num=0000]\n",
      "Epoch 0:  34%|███▎      | 560/1660 [01:06<02:10,  8.41it/s, loss=0.39, v_num=0000] \n",
      "Epoch 0:  34%|███▍      | 561/1660 [01:06<02:10,  8.41it/s, loss=0.394, v_num=0000]\n",
      "Epoch 0:  34%|███▍      | 562/1660 [01:06<02:10,  8.41it/s, loss=0.398, v_num=0000]\n",
      "Epoch 0:  34%|███▍      | 563/1660 [01:06<02:10,  8.41it/s, loss=0.403, v_num=0000]\n",
      "Epoch 0:  34%|███▍      | 564/1660 [01:07<02:10,  8.41it/s, loss=0.404, v_num=0000]\n",
      "Epoch 0:  34%|███▍      | 565/1660 [01:07<02:10,  8.41it/s, loss=0.401, v_num=0000]\n",
      "Epoch 0:  34%|███▍      | 566/1660 [01:07<02:10,  8.41it/s, loss=0.396, v_num=0000]\n",
      "Epoch 0:  34%|███▍      | 567/1660 [01:07<02:09,  8.41it/s, loss=0.397, v_num=0000]\n",
      "Epoch 0:  34%|███▍      | 568/1660 [01:07<02:09,  8.41it/s, loss=0.402, v_num=0000]\n",
      "Epoch 0:  34%|███▍      | 569/1660 [01:07<02:09,  8.41it/s, loss=0.402, v_num=0000]\n",
      "Epoch 0:  34%|███▍      | 570/1660 [01:07<02:09,  8.41it/s, loss=0.404, v_num=0000]\n",
      "Epoch 0:  34%|███▍      | 571/1660 [01:07<02:09,  8.41it/s, loss=0.397, v_num=0000]\n",
      "Epoch 0:  34%|███▍      | 572/1660 [01:07<02:09,  8.41it/s, loss=0.407, v_num=0000]\n",
      "Epoch 0:  35%|███▍      | 573/1660 [01:08<02:09,  8.41it/s, loss=0.408, v_num=0000]\n",
      "Epoch 0:  35%|███▍      | 574/1660 [01:08<02:09,  8.41it/s, loss=0.404, v_num=0000]\n",
      "Epoch 0:  35%|███▍      | 575/1660 [01:08<02:08,  8.41it/s, loss=0.401, v_num=0000]\n",
      "Epoch 0:  35%|███▍      | 576/1660 [01:08<02:08,  8.41it/s, loss=0.395, v_num=0000]\n",
      "Epoch 0:  35%|███▍      | 577/1660 [01:08<02:08,  8.41it/s, loss=0.394, v_num=0000]\n",
      "Epoch 0:  35%|███▍      | 578/1660 [01:08<02:08,  8.41it/s, loss=0.384, v_num=0000]\n",
      "Epoch 0:  35%|███▍      | 579/1660 [01:08<02:08,  8.41it/s, loss=0.38, v_num=0000] \n",
      "Epoch 0:  35%|███▍      | 580/1660 [01:08<02:08,  8.41it/s, loss=0.38, v_num=0000]\n",
      "Epoch 0:  35%|███▌      | 581/1660 [01:09<02:08,  8.41it/s, loss=0.381, v_num=0000]\n",
      "Epoch 0:  35%|███▌      | 582/1660 [01:09<02:08,  8.41it/s, loss=0.382, v_num=0000]\n",
      "Epoch 0:  35%|███▌      | 583/1660 [01:09<02:08,  8.41it/s, loss=0.369, v_num=0000]\n",
      "Epoch 0:  35%|███▌      | 584/1660 [01:09<02:07,  8.41it/s, loss=0.366, v_num=0000]\n",
      "Epoch 0:  35%|███▌      | 585/1660 [01:09<02:07,  8.41it/s, loss=0.367, v_num=0000]\n",
      "Epoch 0:  35%|███▌      | 586/1660 [01:09<02:07,  8.41it/s, loss=0.378, v_num=0000]\n",
      "Epoch 0:  35%|███▌      | 587/1660 [01:09<02:07,  8.41it/s, loss=0.374, v_num=0000]\n",
      "Epoch 0:  35%|███▌      | 588/1660 [01:09<02:07,  8.41it/s, loss=0.374, v_num=0000]\n",
      "Epoch 0:  35%|███▌      | 589/1660 [01:10<02:07,  8.41it/s, loss=0.378, v_num=0000]\n",
      "Epoch 0:  36%|███▌      | 590/1660 [01:10<02:07,  8.41it/s, loss=0.39, v_num=0000] \n",
      "Epoch 0:  36%|███▌      | 591/1660 [01:10<02:07,  8.41it/s, loss=0.393, v_num=0000]\n",
      "Epoch 0:  36%|███▌      | 592/1660 [01:10<02:07,  8.41it/s, loss=0.381, v_num=0000]\n",
      "Epoch 0:  36%|███▌      | 593/1660 [01:10<02:06,  8.41it/s, loss=0.381, v_num=0000]\n",
      "Epoch 0:  36%|███▌      | 594/1660 [01:10<02:06,  8.41it/s, loss=0.385, v_num=0000]\n",
      "Epoch 0:  36%|███▌      | 595/1660 [01:10<02:06,  8.41it/s, loss=0.39, v_num=0000] \n",
      "== Status ==\n",
      "Current time: 2022-06-20 12:01:32 (running for 00:01:25.37)\n",
      "Memory usage on this node: 29.8/376.5 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1.0/8 CPUs, 1.0/1 GPUs, 0.0/34.4 GiB heap, 0.0/17.2 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/dobby/ray_results/tune_rp\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+----------------------+----------+------------------+-----------------+----------------+\n",
      "| Trial name           | status   | loc              |   learning_rate |   weight_decay |\n",
      "|----------------------+----------+------------------+-----------------+----------------|\n",
      "| train_rp_86c32_00000 | RUNNING  | 172.17.0.5:17844 |     0.000368944 |            0.1 |\n",
      "+----------------------+----------+------------------+-----------------+----------------+\n",
      "\n",
      "\n",
      "Epoch 0:  36%|███▌      | 596/1660 [01:10<02:06,  8.41it/s, loss=0.403, v_num=0000]\n",
      "Epoch 0:  36%|███▌      | 597/1660 [01:10<02:06,  8.41it/s, loss=0.407, v_num=0000]\n",
      "Epoch 0:  36%|███▌      | 598/1660 [01:11<02:06,  8.41it/s, loss=0.407, v_num=0000]\n",
      "Epoch 0:  36%|███▌      | 599/1660 [01:11<02:06,  8.41it/s, loss=0.404, v_num=0000]\n",
      "Epoch 0:  36%|███▌      | 600/1660 [01:11<02:06,  8.41it/s, loss=0.404, v_num=0000]\n",
      "Epoch 0:  36%|███▌      | 601/1660 [01:11<02:05,  8.41it/s, loss=0.399, v_num=0000]\n",
      "Epoch 0:  36%|███▋      | 602/1660 [01:11<02:05,  8.41it/s, loss=0.398, v_num=0000]\n",
      "Epoch 0:  36%|███▋      | 603/1660 [01:11<02:05,  8.41it/s, loss=0.395, v_num=0000]\n",
      "Epoch 0:  36%|███▋      | 604/1660 [01:11<02:05,  8.41it/s, loss=0.398, v_num=0000]\n",
      "Epoch 0:  36%|███▋      | 605/1660 [01:11<02:05,  8.41it/s, loss=0.385, v_num=0000]\n",
      "Epoch 0:  37%|███▋      | 606/1660 [01:12<02:05,  8.41it/s, loss=0.396, v_num=0000]\n",
      "Epoch 0:  37%|███▋      | 607/1660 [01:12<02:05,  8.41it/s, loss=0.395, v_num=0000]\n",
      "Epoch 0:  37%|███▋      | 608/1660 [01:12<02:05,  8.41it/s, loss=0.388, v_num=0000]\n",
      "Epoch 0:  37%|███▋      | 609/1660 [01:12<02:04,  8.41it/s, loss=0.386, v_num=0000]\n",
      "Epoch 0:  37%|███▋      | 610/1660 [01:12<02:04,  8.41it/s, loss=0.373, v_num=0000]\n",
      "Epoch 0:  37%|███▋      | 611/1660 [01:12<02:04,  8.41it/s, loss=0.376, v_num=0000]\n",
      "Epoch 0:  37%|███▋      | 612/1660 [01:12<02:04,  8.41it/s, loss=0.372, v_num=0000]\n",
      "Epoch 0:  37%|███▋      | 613/1660 [01:12<02:04,  8.41it/s, loss=0.37, v_num=0000] \n",
      "Epoch 0:  37%|███▋      | 614/1660 [01:13<02:04,  8.41it/s, loss=0.368, v_num=0000]\n",
      "Epoch 0:  37%|███▋      | 615/1660 [01:13<02:04,  8.41it/s, loss=0.364, v_num=0000]\n",
      "Epoch 0:  37%|███▋      | 616/1660 [01:13<02:04,  8.41it/s, loss=0.355, v_num=0000]\n",
      "Epoch 0:  37%|███▋      | 617/1660 [01:13<02:04,  8.41it/s, loss=0.355, v_num=0000]\n",
      "Epoch 0:  37%|███▋      | 618/1660 [01:13<02:03,  8.41it/s, loss=0.362, v_num=0000]\n",
      "Epoch 0:  37%|███▋      | 619/1660 [01:13<02:03,  8.41it/s, loss=0.365, v_num=0000]\n",
      "Epoch 0:  37%|███▋      | 620/1660 [01:13<02:03,  8.41it/s, loss=0.368, v_num=0000]\n",
      "Epoch 0:  37%|███▋      | 621/1660 [01:13<02:03,  8.41it/s, loss=0.369, v_num=0000]\n",
      "Epoch 0:  37%|███▋      | 622/1660 [01:13<02:03,  8.41it/s, loss=0.368, v_num=0000]\n",
      "Epoch 0:  38%|███▊      | 623/1660 [01:14<02:03,  8.41it/s, loss=0.38, v_num=0000] \n",
      "Epoch 0:  38%|███▊      | 624/1660 [01:14<02:03,  8.41it/s, loss=0.38, v_num=0000]\n",
      "Epoch 0:  38%|███▊      | 625/1660 [01:14<02:03,  8.41it/s, loss=0.392, v_num=0000]\n",
      "Epoch 0:  38%|███▊      | 626/1660 [01:14<02:02,  8.41it/s, loss=0.378, v_num=0000]\n",
      "Epoch 0:  38%|███▊      | 627/1660 [01:14<02:02,  8.41it/s, loss=0.385, v_num=0000]\n",
      "Epoch 0:  38%|███▊      | 628/1660 [01:14<02:02,  8.41it/s, loss=0.383, v_num=0000]\n",
      "Epoch 0:  38%|███▊      | 629/1660 [01:14<02:02,  8.41it/s, loss=0.388, v_num=0000]\n",
      "Epoch 0:  38%|███▊      | 630/1660 [01:14<02:02,  8.41it/s, loss=0.387, v_num=0000]\n",
      "Epoch 0:  38%|███▊      | 631/1660 [01:15<02:02,  8.41it/s, loss=0.384, v_num=0000]\n",
      "Epoch 0:  38%|███▊      | 632/1660 [01:15<02:02,  8.41it/s, loss=0.385, v_num=0000]\n",
      "Epoch 0:  38%|███▊      | 633/1660 [01:15<02:02,  8.41it/s, loss=0.387, v_num=0000]\n",
      "Epoch 0:  38%|███▊      | 634/1660 [01:15<02:01,  8.41it/s, loss=0.39, v_num=0000] \n",
      "Epoch 0:  38%|███▊      | 635/1660 [01:15<02:01,  8.41it/s, loss=0.396, v_num=0000]\n",
      "Epoch 0:  38%|███▊      | 636/1660 [01:15<02:01,  8.41it/s, loss=0.398, v_num=0000]\n",
      "Epoch 0:  38%|███▊      | 637/1660 [01:15<02:01,  8.41it/s, loss=0.4, v_num=0000]  \n",
      "== Status ==\n",
      "Current time: 2022-06-20 12:01:37 (running for 00:01:30.38)\n",
      "Memory usage on this node: 29.8/376.5 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1.0/8 CPUs, 1.0/1 GPUs, 0.0/34.4 GiB heap, 0.0/17.2 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/dobby/ray_results/tune_rp\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+----------------------+----------+------------------+-----------------+----------------+\n",
      "| Trial name           | status   | loc              |   learning_rate |   weight_decay |\n",
      "|----------------------+----------+------------------+-----------------+----------------|\n",
      "| train_rp_86c32_00000 | RUNNING  | 172.17.0.5:17844 |     0.000368944 |            0.1 |\n",
      "+----------------------+----------+------------------+-----------------+----------------+\n",
      "\n",
      "\n",
      "Epoch 0:  38%|███▊      | 638/1660 [01:15<02:01,  8.41it/s, loss=0.395, v_num=0000]\n",
      "Epoch 0:  38%|███▊      | 639/1660 [01:15<02:01,  8.41it/s, loss=0.399, v_num=0000]\n",
      "Epoch 0:  39%|███▊      | 640/1660 [01:16<02:01,  8.41it/s, loss=0.395, v_num=0000]\n",
      "Epoch 0:  39%|███▊      | 641/1660 [01:16<02:01,  8.41it/s, loss=0.398, v_num=0000]\n",
      "Epoch 0:  39%|███▊      | 642/1660 [01:16<02:01,  8.41it/s, loss=0.395, v_num=0000]\n",
      "Epoch 0:  39%|███▊      | 643/1660 [01:16<02:00,  8.41it/s, loss=0.391, v_num=0000]\n",
      "Epoch 0:  39%|███▉      | 644/1660 [01:16<02:00,  8.41it/s, loss=0.384, v_num=0000]\n",
      "Epoch 0:  39%|███▉      | 645/1660 [01:16<02:00,  8.41it/s, loss=0.379, v_num=0000]\n",
      "Epoch 0:  39%|███▉      | 646/1660 [01:16<02:00,  8.41it/s, loss=0.375, v_num=0000]\n",
      "Epoch 0:  39%|███▉      | 647/1660 [01:16<02:00,  8.41it/s, loss=0.369, v_num=0000]\n",
      "Epoch 0:  39%|███▉      | 648/1660 [01:17<02:00,  8.41it/s, loss=0.374, v_num=0000]\n",
      "Epoch 0:  39%|███▉      | 649/1660 [01:17<02:00,  8.41it/s, loss=0.372, v_num=0000]\n",
      "Epoch 0:  39%|███▉      | 650/1660 [01:17<02:00,  8.41it/s, loss=0.376, v_num=0000]\n",
      "Epoch 0:  39%|███▉      | 651/1660 [01:17<01:59,  8.41it/s, loss=0.374, v_num=0000]\n",
      "Epoch 0:  39%|███▉      | 652/1660 [01:17<01:59,  8.41it/s, loss=0.374, v_num=0000]\n",
      "Epoch 0:  39%|███▉      | 653/1660 [01:17<01:59,  8.41it/s, loss=0.37, v_num=0000] \n",
      "Epoch 0:  39%|███▉      | 654/1660 [01:17<01:59,  8.41it/s, loss=0.369, v_num=0000]\n",
      "Epoch 0:  39%|███▉      | 655/1660 [01:17<01:59,  8.41it/s, loss=0.36, v_num=0000] \n",
      "Epoch 0:  40%|███▉      | 656/1660 [01:18<01:59,  8.41it/s, loss=0.361, v_num=0000]\n",
      "Epoch 0:  40%|███▉      | 657/1660 [01:18<01:59,  8.41it/s, loss=0.358, v_num=0000]\n",
      "Epoch 0:  40%|███▉      | 658/1660 [01:18<01:59,  8.41it/s, loss=0.36, v_num=0000] \n",
      "Epoch 0:  40%|███▉      | 659/1660 [01:18<01:59,  8.41it/s, loss=0.359, v_num=0000]\n",
      "Epoch 0:  40%|███▉      | 660/1660 [01:18<01:58,  8.41it/s, loss=0.36, v_num=0000] \n",
      "Epoch 0:  40%|███▉      | 661/1660 [01:18<01:58,  8.41it/s, loss=0.357, v_num=0000]\n",
      "Epoch 0:  40%|███▉      | 662/1660 [01:18<01:58,  8.41it/s, loss=0.361, v_num=0000]\n",
      "Epoch 0:  40%|███▉      | 663/1660 [01:18<01:58,  8.41it/s, loss=0.357, v_num=0000]\n",
      "Epoch 0:  40%|████      | 664/1660 [01:18<01:58,  8.41it/s, loss=0.354, v_num=0000]\n",
      "Epoch 0:  40%|████      | 665/1660 [01:19<01:58,  8.41it/s, loss=0.357, v_num=0000]\n",
      "Epoch 0:  40%|████      | 666/1660 [01:19<01:58,  8.41it/s, loss=0.352, v_num=0000]\n",
      "Epoch 0:  40%|████      | 667/1660 [01:19<01:58,  8.41it/s, loss=0.355, v_num=0000]\n",
      "Epoch 0:  40%|████      | 668/1660 [01:19<01:57,  8.41it/s, loss=0.357, v_num=0000]\n",
      "Epoch 0:  40%|████      | 669/1660 [01:19<01:57,  8.41it/s, loss=0.348, v_num=0000]\n",
      "Epoch 0:  40%|████      | 670/1660 [01:19<01:57,  8.41it/s, loss=0.344, v_num=0000]\n",
      "Epoch 0:  40%|████      | 671/1660 [01:19<01:57,  8.41it/s, loss=0.347, v_num=0000]\n",
      "Epoch 0:  40%|████      | 672/1660 [01:19<01:57,  8.41it/s, loss=0.356, v_num=0000]\n",
      "Epoch 0:  41%|████      | 673/1660 [01:20<01:57,  8.41it/s, loss=0.365, v_num=0000]\n",
      "Epoch 0:  41%|████      | 674/1660 [01:20<01:57,  8.41it/s, loss=0.361, v_num=0000]\n",
      "Epoch 0:  41%|████      | 675/1660 [01:20<01:57,  8.41it/s, loss=0.364, v_num=0000]\n",
      "Epoch 0:  41%|████      | 676/1660 [01:20<01:57,  8.41it/s, loss=0.366, v_num=0000]\n",
      "Epoch 0:  41%|████      | 677/1660 [01:20<01:56,  8.41it/s, loss=0.364, v_num=0000]\n",
      "Epoch 0:  41%|████      | 678/1660 [01:20<01:56,  8.41it/s, loss=0.367, v_num=0000]\n",
      "Epoch 0:  41%|████      | 679/1660 [01:20<01:56,  8.41it/s, loss=0.361, v_num=0000]\n",
      "== Status ==\n",
      "Current time: 2022-06-20 12:01:42 (running for 00:01:35.38)\n",
      "Memory usage on this node: 29.8/376.5 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1.0/8 CPUs, 1.0/1 GPUs, 0.0/34.4 GiB heap, 0.0/17.2 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/dobby/ray_results/tune_rp\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+----------------------+----------+------------------+-----------------+----------------+\n",
      "| Trial name           | status   | loc              |   learning_rate |   weight_decay |\n",
      "|----------------------+----------+------------------+-----------------+----------------|\n",
      "| train_rp_86c32_00000 | RUNNING  | 172.17.0.5:17844 |     0.000368944 |            0.1 |\n",
      "+----------------------+----------+------------------+-----------------+----------------+\n",
      "\n",
      "\n",
      "Epoch 0:  41%|████      | 680/1660 [01:20<01:56,  8.41it/s, loss=0.363, v_num=0000]\n",
      "Epoch 0:  41%|████      | 681/1660 [01:20<01:56,  8.41it/s, loss=0.36, v_num=0000] \n",
      "Epoch 0:  41%|████      | 682/1660 [01:21<01:56,  8.41it/s, loss=0.358, v_num=0000]\n",
      "Epoch 0:  41%|████      | 683/1660 [01:21<01:56,  8.41it/s, loss=0.366, v_num=0000]\n",
      "Epoch 0:  41%|████      | 684/1660 [01:21<01:56,  8.41it/s, loss=0.373, v_num=0000]\n",
      "Epoch 0:  41%|████▏     | 685/1660 [01:21<01:55,  8.41it/s, loss=0.369, v_num=0000]\n",
      "Epoch 0:  41%|████▏     | 686/1660 [01:21<01:55,  8.41it/s, loss=0.377, v_num=0000]\n",
      "Epoch 0:  41%|████▏     | 687/1660 [01:21<01:55,  8.41it/s, loss=0.37, v_num=0000] \n",
      "Epoch 0:  41%|████▏     | 688/1660 [01:21<01:55,  8.41it/s, loss=0.373, v_num=0000]\n",
      "Epoch 0:  42%|████▏     | 689/1660 [01:21<01:55,  8.41it/s, loss=0.38, v_num=0000] \n",
      "Epoch 0:  42%|████▏     | 690/1660 [01:22<01:55,  8.41it/s, loss=0.386, v_num=0000]\n",
      "Epoch 0:  42%|████▏     | 691/1660 [01:22<01:55,  8.41it/s, loss=0.385, v_num=0000]\n",
      "Epoch 0:  42%|████▏     | 692/1660 [01:22<01:55,  8.41it/s, loss=0.379, v_num=0000]\n",
      "Epoch 0:  42%|████▏     | 693/1660 [01:22<01:54,  8.41it/s, loss=0.376, v_num=0000]\n",
      "Epoch 0:  42%|████▏     | 694/1660 [01:22<01:54,  8.41it/s, loss=0.385, v_num=0000]\n",
      "Epoch 0:  42%|████▏     | 695/1660 [01:22<01:54,  8.41it/s, loss=0.387, v_num=0000]\n",
      "Epoch 0:  42%|████▏     | 696/1660 [01:22<01:54,  8.41it/s, loss=0.378, v_num=0000]\n",
      "Epoch 0:  42%|████▏     | 697/1660 [01:22<01:54,  8.41it/s, loss=0.375, v_num=0000]\n",
      "Epoch 0:  42%|████▏     | 698/1660 [01:22<01:54,  8.41it/s, loss=0.365, v_num=0000]\n",
      "Epoch 0:  42%|████▏     | 699/1660 [01:23<01:54,  8.41it/s, loss=0.365, v_num=0000]\n",
      "Epoch 0:  42%|████▏     | 700/1660 [01:23<01:54,  8.41it/s, loss=0.366, v_num=0000]\n",
      "Epoch 0:  42%|████▏     | 701/1660 [01:23<01:54,  8.41it/s, loss=0.367, v_num=0000]\n",
      "Epoch 0:  42%|████▏     | 702/1660 [01:23<01:53,  8.41it/s, loss=0.374, v_num=0000]\n",
      "Epoch 0:  42%|████▏     | 703/1660 [01:23<01:53,  8.41it/s, loss=0.364, v_num=0000]\n",
      "Epoch 0:  42%|████▏     | 704/1660 [01:23<01:53,  8.41it/s, loss=0.362, v_num=0000]\n",
      "Epoch 0:  42%|████▏     | 705/1660 [01:23<01:53,  8.41it/s, loss=0.364, v_num=0000]\n",
      "Epoch 0:  43%|████▎     | 706/1660 [01:23<01:53,  8.41it/s, loss=0.364, v_num=0000]\n",
      "Epoch 0:  43%|████▎     | 707/1660 [01:24<01:53,  8.41it/s, loss=0.369, v_num=0000]\n",
      "Epoch 0:  43%|████▎     | 708/1660 [01:24<01:53,  8.41it/s, loss=0.366, v_num=0000]\n",
      "Epoch 0:  43%|████▎     | 709/1660 [01:24<01:53,  8.41it/s, loss=0.367, v_num=0000]\n",
      "Epoch 0:  43%|████▎     | 710/1660 [01:24<01:52,  8.41it/s, loss=0.361, v_num=0000]\n",
      "Epoch 0:  43%|████▎     | 711/1660 [01:24<01:52,  8.41it/s, loss=0.358, v_num=0000]\n",
      "Epoch 0:  43%|████▎     | 712/1660 [01:24<01:52,  8.41it/s, loss=0.354, v_num=0000]\n",
      "Epoch 0:  43%|████▎     | 713/1660 [01:24<01:52,  8.41it/s, loss=0.354, v_num=0000]\n",
      "Epoch 0:  43%|████▎     | 714/1660 [01:24<01:52,  8.41it/s, loss=0.351, v_num=0000]\n",
      "Epoch 0:  43%|████▎     | 715/1660 [01:25<01:52,  8.41it/s, loss=0.349, v_num=0000]\n",
      "Epoch 0:  43%|████▎     | 716/1660 [01:25<01:52,  8.41it/s, loss=0.357, v_num=0000]\n",
      "Epoch 0:  43%|████▎     | 717/1660 [01:25<01:52,  8.41it/s, loss=0.356, v_num=0000]\n",
      "Epoch 0:  43%|████▎     | 718/1660 [01:25<01:52,  8.41it/s, loss=0.359, v_num=0000]\n",
      "Epoch 0:  43%|████▎     | 719/1660 [01:25<01:51,  8.41it/s, loss=0.358, v_num=0000]\n",
      "Epoch 0:  43%|████▎     | 720/1660 [01:25<01:51,  8.41it/s, loss=0.351, v_num=0000]\n",
      "Epoch 0:  43%|████▎     | 721/1660 [01:25<01:51,  8.41it/s, loss=0.359, v_num=0000]\n",
      "Epoch 0:  43%|████▎     | 722/1660 [01:25<01:51,  8.41it/s, loss=0.348, v_num=0000]\n",
      "Epoch 0:  44%|████▎     | 723/1660 [01:25<01:51,  8.41it/s, loss=0.356, v_num=0000]\n",
      "== Status ==\n",
      "Current time: 2022-06-20 12:01:47 (running for 00:01:40.56)\n",
      "Memory usage on this node: 29.8/376.5 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1.0/8 CPUs, 1.0/1 GPUs, 0.0/34.4 GiB heap, 0.0/17.2 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/dobby/ray_results/tune_rp\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+----------------------+----------+------------------+-----------------+----------------+\n",
      "| Trial name           | status   | loc              |   learning_rate |   weight_decay |\n",
      "|----------------------+----------+------------------+-----------------+----------------|\n",
      "| train_rp_86c32_00000 | RUNNING  | 172.17.0.5:17844 |     0.000368944 |            0.1 |\n",
      "+----------------------+----------+------------------+-----------------+----------------+\n",
      "\n",
      "\n",
      "Epoch 0:  44%|████▎     | 724/1660 [01:26<01:51,  8.41it/s, loss=0.355, v_num=0000]\n",
      "Epoch 0:  44%|████▎     | 725/1660 [01:26<01:51,  8.41it/s, loss=0.352, v_num=0000]\n",
      "Epoch 0:  44%|████▎     | 726/1660 [01:26<01:51,  8.41it/s, loss=0.356, v_num=0000]\n",
      "Epoch 0:  44%|████▍     | 727/1660 [01:26<01:50,  8.41it/s, loss=0.354, v_num=0000]\n",
      "Epoch 0:  44%|████▍     | 728/1660 [01:26<01:50,  8.41it/s, loss=0.352, v_num=0000]\n",
      "Epoch 0:  44%|████▍     | 729/1660 [01:26<01:50,  8.41it/s, loss=0.348, v_num=0000]\n",
      "Epoch 0:  44%|████▍     | 730/1660 [01:26<01:50,  8.41it/s, loss=0.351, v_num=0000]\n",
      "Epoch 0:  44%|████▍     | 731/1660 [01:26<01:50,  8.41it/s, loss=0.356, v_num=0000]\n",
      "Epoch 0:  44%|████▍     | 732/1660 [01:27<01:50,  8.41it/s, loss=0.36, v_num=0000] \n",
      "Epoch 0:  44%|████▍     | 733/1660 [01:27<01:50,  8.41it/s, loss=0.359, v_num=0000]\n",
      "Epoch 0:  44%|████▍     | 734/1660 [01:27<01:50,  8.41it/s, loss=0.36, v_num=0000] \n",
      "Epoch 0:  44%|████▍     | 735/1660 [01:27<01:49,  8.41it/s, loss=0.36, v_num=0000]\n",
      "Epoch 0:  44%|████▍     | 736/1660 [01:27<01:49,  8.41it/s, loss=0.362, v_num=0000]\n",
      "Epoch 0:  44%|████▍     | 737/1660 [01:27<01:49,  8.41it/s, loss=0.368, v_num=0000]\n",
      "Epoch 0:  44%|████▍     | 738/1660 [01:27<01:49,  8.41it/s, loss=0.368, v_num=0000]\n",
      "Epoch 0:  45%|████▍     | 739/1660 [01:27<01:49,  8.41it/s, loss=0.371, v_num=0000]\n",
      "Epoch 0:  45%|████▍     | 740/1660 [01:28<01:49,  8.41it/s, loss=0.374, v_num=0000]\n",
      "Epoch 0:  45%|████▍     | 741/1660 [01:28<01:49,  8.41it/s, loss=0.368, v_num=0000]\n",
      "Epoch 0:  45%|████▍     | 742/1660 [01:28<01:49,  8.41it/s, loss=0.38, v_num=0000] \n",
      "Epoch 0:  45%|████▍     | 743/1660 [01:28<01:49,  8.41it/s, loss=0.389, v_num=0000]\n",
      "Epoch 0:  45%|████▍     | 744/1660 [01:28<01:48,  8.41it/s, loss=0.394, v_num=0000]\n",
      "Epoch 0:  45%|████▍     | 745/1660 [01:28<01:48,  8.41it/s, loss=0.392, v_num=0000]\n",
      "Epoch 0:  45%|████▍     | 746/1660 [01:28<01:48,  8.41it/s, loss=0.386, v_num=0000]\n",
      "Epoch 0:  45%|████▌     | 747/1660 [01:28<01:48,  8.41it/s, loss=0.395, v_num=0000]\n",
      "Epoch 0:  45%|████▌     | 748/1660 [01:28<01:48,  8.41it/s, loss=0.4, v_num=0000]  \n",
      "Epoch 0:  45%|████▌     | 749/1660 [01:29<01:48,  8.41it/s, loss=0.406, v_num=0000]\n",
      "Epoch 0:  45%|████▌     | 750/1660 [01:29<01:48,  8.41it/s, loss=0.399, v_num=0000]\n",
      "Epoch 0:  45%|████▌     | 751/1660 [01:29<01:48,  8.41it/s, loss=0.407, v_num=0000]\n",
      "Epoch 0:  45%|████▌     | 752/1660 [01:29<01:47,  8.41it/s, loss=0.408, v_num=0000]\n",
      "Epoch 0:  45%|████▌     | 753/1660 [01:29<01:47,  8.41it/s, loss=0.408, v_num=0000]\n",
      "Epoch 0:  45%|████▌     | 754/1660 [01:29<01:47,  8.41it/s, loss=0.404, v_num=0000]\n",
      "Epoch 0:  45%|████▌     | 755/1660 [01:29<01:47,  8.41it/s, loss=0.404, v_num=0000]\n",
      "Epoch 0:  46%|████▌     | 756/1660 [01:29<01:47,  8.41it/s, loss=0.398, v_num=0000]\n",
      "Epoch 0:  46%|████▌     | 757/1660 [01:30<01:47,  8.41it/s, loss=0.389, v_num=0000]\n",
      "Epoch 0:  46%|████▌     | 758/1660 [01:30<01:47,  8.41it/s, loss=0.391, v_num=0000]\n",
      "Epoch 0:  46%|████▌     | 759/1660 [01:30<01:47,  8.41it/s, loss=0.395, v_num=0000]\n",
      "Epoch 0:  46%|████▌     | 760/1660 [01:30<01:47,  8.41it/s, loss=0.391, v_num=0000]\n",
      "Epoch 0:  46%|████▌     | 761/1660 [01:30<01:46,  8.41it/s, loss=0.397, v_num=0000]\n",
      "Epoch 0:  46%|████▌     | 762/1660 [01:30<01:46,  8.41it/s, loss=0.395, v_num=0000]\n",
      "Epoch 0:  46%|████▌     | 763/1660 [01:30<01:46,  8.41it/s, loss=0.38, v_num=0000] \n",
      "Epoch 0:  46%|████▌     | 764/1660 [01:30<01:46,  8.41it/s, loss=0.371, v_num=0000]\n",
      "Epoch 0:  46%|████▌     | 765/1660 [01:30<01:46,  8.41it/s, loss=0.379, v_num=0000]\n",
      "== Status ==\n",
      "Current time: 2022-06-20 12:01:52 (running for 00:01:45.57)\n",
      "Memory usage on this node: 29.8/376.5 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1.0/8 CPUs, 1.0/1 GPUs, 0.0/34.4 GiB heap, 0.0/17.2 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/dobby/ray_results/tune_rp\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+----------------------+----------+------------------+-----------------+----------------+\n",
      "| Trial name           | status   | loc              |   learning_rate |   weight_decay |\n",
      "|----------------------+----------+------------------+-----------------+----------------|\n",
      "| train_rp_86c32_00000 | RUNNING  | 172.17.0.5:17844 |     0.000368944 |            0.1 |\n",
      "+----------------------+----------+------------------+-----------------+----------------+\n",
      "\n",
      "\n",
      "Epoch 0:  46%|████▌     | 766/1660 [01:31<01:46,  8.41it/s, loss=0.38, v_num=0000] \n",
      "Epoch 0:  46%|████▌     | 767/1660 [01:31<01:46,  8.41it/s, loss=0.368, v_num=0000]\n",
      "Epoch 0:  46%|████▋     | 768/1660 [01:31<01:46,  8.41it/s, loss=0.369, v_num=0000]\n",
      "Epoch 0:  46%|████▋     | 769/1660 [01:31<01:45,  8.41it/s, loss=0.365, v_num=0000]\n",
      "Epoch 0:  46%|████▋     | 770/1660 [01:31<01:45,  8.41it/s, loss=0.372, v_num=0000]\n",
      "Epoch 0:  46%|████▋     | 771/1660 [01:31<01:45,  8.41it/s, loss=0.363, v_num=0000]\n",
      "Epoch 0:  47%|████▋     | 772/1660 [01:31<01:45,  8.41it/s, loss=0.367, v_num=0000]\n",
      "Epoch 0:  47%|████▋     | 773/1660 [01:31<01:45,  8.41it/s, loss=0.366, v_num=0000]\n",
      "Epoch 0:  47%|████▋     | 774/1660 [01:32<01:45,  8.41it/s, loss=0.37, v_num=0000] \n",
      "Epoch 0:  47%|████▋     | 775/1660 [01:32<01:45,  8.41it/s, loss=0.373, v_num=0000]\n",
      "Epoch 0:  47%|████▋     | 776/1660 [01:32<01:45,  8.41it/s, loss=0.373, v_num=0000]\n",
      "Epoch 0:  47%|████▋     | 777/1660 [01:32<01:45,  8.41it/s, loss=0.372, v_num=0000]\n",
      "Epoch 0:  47%|████▋     | 778/1660 [01:32<01:44,  8.41it/s, loss=0.369, v_num=0000]\n",
      "Epoch 0:  47%|████▋     | 779/1660 [01:32<01:44,  8.41it/s, loss=0.363, v_num=0000]\n",
      "Epoch 0:  47%|████▋     | 780/1660 [01:32<01:44,  8.41it/s, loss=0.367, v_num=0000]\n",
      "Epoch 0:  47%|████▋     | 781/1660 [01:32<01:44,  8.41it/s, loss=0.36, v_num=0000] \n",
      "Epoch 0:  47%|████▋     | 782/1660 [01:33<01:44,  8.41it/s, loss=0.358, v_num=0000]\n",
      "Epoch 0:  47%|████▋     | 783/1660 [01:33<01:44,  8.41it/s, loss=0.363, v_num=0000]\n",
      "Epoch 0:  47%|████▋     | 784/1660 [01:33<01:44,  8.40it/s, loss=0.376, v_num=0000]\n",
      "Epoch 0:  47%|████▋     | 785/1660 [01:33<01:44,  8.40it/s, loss=0.368, v_num=0000]\n",
      "Epoch 0:  47%|████▋     | 786/1660 [01:33<01:43,  8.40it/s, loss=0.366, v_num=0000]\n",
      "Epoch 0:  47%|████▋     | 787/1660 [01:33<01:43,  8.40it/s, loss=0.371, v_num=0000]\n",
      "Epoch 0:  47%|████▋     | 788/1660 [01:33<01:43,  8.40it/s, loss=0.36, v_num=0000] \n",
      "Epoch 0:  48%|████▊     | 789/1660 [01:33<01:43,  8.40it/s, loss=0.362, v_num=0000]\n",
      "Epoch 0:  48%|████▊     | 790/1660 [01:33<01:43,  8.40it/s, loss=0.364, v_num=0000]\n",
      "Epoch 0:  48%|████▊     | 791/1660 [01:34<01:43,  8.40it/s, loss=0.364, v_num=0000]\n",
      "Epoch 0:  48%|████▊     | 792/1660 [01:34<01:43,  8.41it/s, loss=0.357, v_num=0000]\n",
      "Epoch 0:  48%|████▊     | 793/1660 [01:34<01:43,  8.41it/s, loss=0.369, v_num=0000]\n",
      "Epoch 0:  48%|████▊     | 794/1660 [01:34<01:43,  8.41it/s, loss=0.362, v_num=0000]\n",
      "Epoch 0:  48%|████▊     | 795/1660 [01:34<01:42,  8.41it/s, loss=0.356, v_num=0000]\n",
      "Epoch 0:  48%|████▊     | 796/1660 [01:34<01:42,  8.41it/s, loss=0.351, v_num=0000]\n",
      "Epoch 0:  48%|████▊     | 797/1660 [01:34<01:42,  8.41it/s, loss=0.35, v_num=0000] \n",
      "Epoch 0:  48%|████▊     | 798/1660 [01:34<01:42,  8.41it/s, loss=0.351, v_num=0000]\n",
      "Epoch 0:  48%|████▊     | 799/1660 [01:35<01:42,  8.41it/s, loss=0.349, v_num=0000]\n",
      "Epoch 0:  48%|████▊     | 800/1660 [01:35<01:42,  8.40it/s, loss=0.345, v_num=0000]\n",
      "Epoch 0:  48%|████▊     | 801/1660 [01:35<01:42,  8.40it/s, loss=0.346, v_num=0000]\n",
      "Epoch 0:  48%|████▊     | 802/1660 [01:35<01:42,  8.40it/s, loss=0.341, v_num=0000]\n",
      "Epoch 0:  48%|████▊     | 803/1660 [01:35<01:41,  8.40it/s, loss=0.338, v_num=0000]\n",
      "Epoch 0:  48%|████▊     | 804/1660 [01:35<01:41,  8.40it/s, loss=0.326, v_num=0000]\n",
      "Epoch 0:  48%|████▊     | 805/1660 [01:35<01:41,  8.40it/s, loss=0.327, v_num=0000]\n",
      "Epoch 0:  49%|████▊     | 806/1660 [01:35<01:41,  8.40it/s, loss=0.331, v_num=0000]\n",
      "Epoch 0:  49%|████▊     | 807/1660 [01:36<01:41,  8.40it/s, loss=0.333, v_num=0000]\n",
      "== Status ==\n",
      "Current time: 2022-06-20 12:01:57 (running for 00:01:50.57)\n",
      "Memory usage on this node: 29.9/376.5 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1.0/8 CPUs, 1.0/1 GPUs, 0.0/34.4 GiB heap, 0.0/17.2 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/dobby/ray_results/tune_rp\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+----------------------+----------+------------------+-----------------+----------------+\n",
      "| Trial name           | status   | loc              |   learning_rate |   weight_decay |\n",
      "|----------------------+----------+------------------+-----------------+----------------|\n",
      "| train_rp_86c32_00000 | RUNNING  | 172.17.0.5:17844 |     0.000368944 |            0.1 |\n",
      "+----------------------+----------+------------------+-----------------+----------------+\n",
      "\n",
      "\n",
      "Epoch 0:  49%|████▊     | 808/1660 [01:36<01:41,  8.40it/s, loss=0.338, v_num=0000]\n",
      "Epoch 0:  49%|████▊     | 809/1660 [01:36<01:41,  8.40it/s, loss=0.33, v_num=0000] \n",
      "Epoch 0:  49%|████▉     | 810/1660 [01:36<01:41,  8.40it/s, loss=0.325, v_num=0000]\n",
      "Epoch 0:  49%|████▉     | 811/1660 [01:36<01:41,  8.40it/s, loss=0.324, v_num=0000]\n",
      "Epoch 0:  49%|████▉     | 812/1660 [01:36<01:40,  8.41it/s, loss=0.326, v_num=0000]\n",
      "Epoch 0:  49%|████▉     | 813/1660 [01:36<01:40,  8.41it/s, loss=0.316, v_num=0000]\n",
      "Epoch 0:  49%|████▉     | 814/1660 [01:36<01:40,  8.41it/s, loss=0.316, v_num=0000]\n",
      "Epoch 0:  49%|████▉     | 815/1660 [01:36<01:40,  8.41it/s, loss=0.32, v_num=0000] \n",
      "Epoch 0:  49%|████▉     | 816/1660 [01:37<01:40,  8.40it/s, loss=0.329, v_num=0000]\n",
      "Epoch 0:  49%|████▉     | 817/1660 [01:37<01:40,  8.40it/s, loss=0.34, v_num=0000] \n",
      "Epoch 0:  49%|████▉     | 818/1660 [01:37<01:40,  8.40it/s, loss=0.351, v_num=0000]\n",
      "Epoch 0:  49%|████▉     | 819/1660 [01:37<01:40,  8.40it/s, loss=0.367, v_num=0000]\n",
      "Epoch 0:  49%|████▉     | 820/1660 [01:37<01:39,  8.40it/s, loss=0.364, v_num=0000]\n",
      "Epoch 0:  49%|████▉     | 821/1660 [01:37<01:39,  8.40it/s, loss=0.364, v_num=0000]\n",
      "Epoch 0:  50%|████▉     | 822/1660 [01:37<01:39,  8.40it/s, loss=0.363, v_num=0000]\n",
      "Epoch 0:  50%|████▉     | 823/1660 [01:37<01:39,  8.40it/s, loss=0.36, v_num=0000] \n",
      "Epoch 0:  50%|████▉     | 824/1660 [01:38<01:39,  8.40it/s, loss=0.363, v_num=0000]\n",
      "Epoch 0:  50%|████▉     | 825/1660 [01:38<01:39,  8.40it/s, loss=0.368, v_num=0000]\n",
      "Epoch 0:  50%|████▉     | 826/1660 [01:38<01:39,  8.40it/s, loss=0.365, v_num=0000]\n",
      "Epoch 0:  50%|████▉     | 827/1660 [01:38<01:39,  8.40it/s, loss=0.364, v_num=0000]\n",
      "Epoch 0:  50%|████▉     | 828/1660 [01:38<01:39,  8.40it/s, loss=0.366, v_num=0000]\n",
      "Epoch 0:  50%|████▉     | 829/1660 [01:38<01:38,  8.40it/s, loss=0.38, v_num=0000] \n",
      "Epoch 0:  50%|█████     | 830/1660 [01:38<01:38,  8.40it/s, loss=0.376, v_num=0000]\n",
      "Epoch 0:  50%|█████     | 831/1660 [01:38<01:38,  8.40it/s, loss=0.381, v_num=0000]\n",
      "Epoch 0:  50%|█████     | 832/1660 [01:39<01:38,  8.40it/s, loss=0.39, v_num=0000] \n",
      "Epoch 0:  50%|█████     | 833/1660 [01:39<01:38,  8.40it/s, loss=0.388, v_num=0000]\n",
      "Epoch 0:  50%|█████     | 834/1660 [01:39<01:38,  8.40it/s, loss=0.391, v_num=0000]\n",
      "Epoch 0:  50%|█████     | 835/1660 [01:39<01:38,  8.40it/s, loss=0.394, v_num=0000]\n",
      "Epoch 0:  50%|█████     | 836/1660 [01:39<01:38,  8.40it/s, loss=0.387, v_num=0000]\n",
      "Epoch 0:  50%|█████     | 837/1660 [01:39<01:37,  8.40it/s, loss=0.382, v_num=0000]\n",
      "Epoch 0:  50%|█████     | 838/1660 [01:39<01:37,  8.40it/s, loss=0.366, v_num=0000]\n",
      "Epoch 0:  51%|█████     | 839/1660 [01:39<01:37,  8.40it/s, loss=0.353, v_num=0000]\n",
      "Epoch 0:  51%|█████     | 840/1660 [01:40<01:37,  8.40it/s, loss=0.357, v_num=0000]\n",
      "Epoch 0:  51%|█████     | 841/1660 [01:40<01:37,  8.40it/s, loss=0.355, v_num=0000]\n",
      "Epoch 0:  51%|█████     | 842/1660 [01:40<01:37,  8.40it/s, loss=0.363, v_num=0000]\n",
      "Epoch 0:  51%|█████     | 843/1660 [01:40<01:37,  8.40it/s, loss=0.364, v_num=0000]\n",
      "Epoch 0:  51%|█████     | 844/1660 [01:40<01:37,  8.40it/s, loss=0.373, v_num=0000]\n",
      "Epoch 0:  51%|█████     | 845/1660 [01:40<01:37,  8.40it/s, loss=0.372, v_num=0000]\n",
      "Epoch 0:  51%|█████     | 846/1660 [01:40<01:36,  8.40it/s, loss=0.374, v_num=0000]\n",
      "Epoch 0:  51%|█████     | 847/1660 [01:40<01:36,  8.40it/s, loss=0.379, v_num=0000]\n",
      "Epoch 0:  51%|█████     | 848/1660 [01:41<01:36,  8.39it/s, loss=0.38, v_num=0000] \n",
      "== Status ==\n",
      "Current time: 2022-06-20 12:02:02 (running for 00:01:55.57)\n",
      "Memory usage on this node: 29.9/376.5 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1.0/8 CPUs, 1.0/1 GPUs, 0.0/34.4 GiB heap, 0.0/17.2 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/dobby/ray_results/tune_rp\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+----------------------+----------+------------------+-----------------+----------------+\n",
      "| Trial name           | status   | loc              |   learning_rate |   weight_decay |\n",
      "|----------------------+----------+------------------+-----------------+----------------|\n",
      "| train_rp_86c32_00000 | RUNNING  | 172.17.0.5:17844 |     0.000368944 |            0.1 |\n",
      "+----------------------+----------+------------------+-----------------+----------------+\n",
      "\n",
      "\n",
      "Epoch 0:  51%|█████     | 849/1660 [01:41<01:36,  8.39it/s, loss=0.381, v_num=0000]\n",
      "Epoch 0:  51%|█████     | 850/1660 [01:41<01:36,  8.39it/s, loss=0.383, v_num=0000]\n",
      "Epoch 0:  51%|█████▏    | 851/1660 [01:41<01:36,  8.39it/s, loss=0.399, v_num=0000]\n",
      "Epoch 0:  51%|█████▏    | 852/1660 [01:41<01:36,  8.39it/s, loss=0.388, v_num=0000]\n",
      "Epoch 0:  51%|█████▏    | 853/1660 [01:41<01:36,  8.39it/s, loss=0.39, v_num=0000] \n",
      "Epoch 0:  51%|█████▏    | 854/1660 [01:41<01:36,  8.39it/s, loss=0.387, v_num=0000]\n",
      "Epoch 0:  52%|█████▏    | 855/1660 [01:41<01:35,  8.39it/s, loss=0.39, v_num=0000] \n",
      "Epoch 0:  52%|█████▏    | 856/1660 [01:41<01:35,  8.39it/s, loss=0.4, v_num=0000] \n",
      "Epoch 0:  52%|█████▏    | 857/1660 [01:42<01:35,  8.39it/s, loss=0.407, v_num=0000]\n",
      "Epoch 0:  52%|█████▏    | 858/1660 [01:42<01:35,  8.39it/s, loss=0.416, v_num=0000]\n",
      "Epoch 0:  52%|█████▏    | 859/1660 [01:42<01:35,  8.39it/s, loss=0.422, v_num=0000]\n",
      "Epoch 0:  52%|█████▏    | 860/1660 [01:42<01:35,  8.39it/s, loss=0.432, v_num=0000]\n",
      "Epoch 0:  52%|█████▏    | 861/1660 [01:42<01:35,  8.39it/s, loss=0.438, v_num=0000]\n",
      "Epoch 0:  52%|█████▏    | 862/1660 [01:42<01:35,  8.39it/s, loss=0.447, v_num=0000]\n",
      "Epoch 0:  52%|█████▏    | 863/1660 [01:42<01:34,  8.39it/s, loss=0.451, v_num=0000]\n",
      "Epoch 0:  52%|█████▏    | 864/1660 [01:42<01:34,  8.39it/s, loss=0.455, v_num=0000]\n",
      "Epoch 0:  52%|█████▏    | 865/1660 [01:43<01:34,  8.39it/s, loss=0.463, v_num=0000]\n",
      "Epoch 0:  52%|█████▏    | 866/1660 [01:43<01:34,  8.39it/s, loss=0.461, v_num=0000]\n",
      "Epoch 0:  52%|█████▏    | 867/1660 [01:43<01:34,  8.39it/s, loss=0.446, v_num=0000]\n",
      "Epoch 0:  52%|█████▏    | 868/1660 [01:43<01:34,  8.39it/s, loss=0.45, v_num=0000] \n",
      "Epoch 0:  52%|█████▏    | 869/1660 [01:43<01:34,  8.39it/s, loss=0.439, v_num=0000]\n",
      "Epoch 0:  52%|█████▏    | 870/1660 [01:43<01:34,  8.39it/s, loss=0.442, v_num=0000]\n",
      "Epoch 0:  52%|█████▏    | 871/1660 [01:43<01:34,  8.39it/s, loss=0.426, v_num=0000]\n",
      "Epoch 0:  53%|█████▎    | 872/1660 [01:43<01:33,  8.39it/s, loss=0.437, v_num=0000]\n",
      "Epoch 0:  53%|█████▎    | 873/1660 [01:44<01:33,  8.39it/s, loss=0.434, v_num=0000]\n",
      "Epoch 0:  53%|█████▎    | 874/1660 [01:44<01:33,  8.39it/s, loss=0.434, v_num=0000]\n",
      "Epoch 0:  53%|█████▎    | 875/1660 [01:44<01:33,  8.39it/s, loss=0.433, v_num=0000]\n",
      "Epoch 0:  53%|█████▎    | 876/1660 [01:44<01:33,  8.39it/s, loss=0.424, v_num=0000]\n",
      "Epoch 0:  53%|█████▎    | 877/1660 [01:44<01:33,  8.39it/s, loss=0.415, v_num=0000]\n",
      "Epoch 0:  53%|█████▎    | 878/1660 [01:44<01:33,  8.39it/s, loss=0.42, v_num=0000] \n",
      "Epoch 0:  53%|█████▎    | 879/1660 [01:44<01:33,  8.39it/s, loss=0.419, v_num=0000]\n",
      "Epoch 0:  53%|█████▎    | 880/1660 [01:44<01:33,  8.38it/s, loss=0.422, v_num=0000]\n",
      "Epoch 0:  53%|█████▎    | 881/1660 [01:45<01:32,  8.38it/s, loss=0.422, v_num=0000]\n",
      "Epoch 0:  53%|█████▎    | 882/1660 [01:45<01:32,  8.38it/s, loss=0.412, v_num=0000]\n",
      "Epoch 0:  53%|█████▎    | 883/1660 [01:45<01:32,  8.38it/s, loss=0.415, v_num=0000]\n",
      "Epoch 0:  53%|█████▎    | 884/1660 [01:45<01:32,  8.38it/s, loss=0.401, v_num=0000]\n",
      "Epoch 0:  53%|█████▎    | 885/1660 [01:45<01:32,  8.38it/s, loss=0.388, v_num=0000]\n",
      "Epoch 0:  53%|█████▎    | 886/1660 [01:45<01:32,  8.38it/s, loss=0.39, v_num=0000] \n",
      "Epoch 0:  53%|█████▎    | 887/1660 [01:45<01:32,  8.38it/s, loss=0.402, v_num=0000]\n",
      "Epoch 0:  53%|█████▎    | 888/1660 [01:45<01:32,  8.38it/s, loss=0.39, v_num=0000] \n",
      "Epoch 0:  54%|█████▎    | 889/1660 [01:46<01:31,  8.38it/s, loss=0.386, v_num=0000]\n",
      "== Status ==\n",
      "Current time: 2022-06-20 12:02:07 (running for 00:02:00.58)\n",
      "Memory usage on this node: 29.9/376.5 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1.0/8 CPUs, 1.0/1 GPUs, 0.0/34.4 GiB heap, 0.0/17.2 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/dobby/ray_results/tune_rp\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+----------------------+----------+------------------+-----------------+----------------+\n",
      "| Trial name           | status   | loc              |   learning_rate |   weight_decay |\n",
      "|----------------------+----------+------------------+-----------------+----------------|\n",
      "| train_rp_86c32_00000 | RUNNING  | 172.17.0.5:17844 |     0.000368944 |            0.1 |\n",
      "+----------------------+----------+------------------+-----------------+----------------+\n",
      "\n",
      "\n",
      "Epoch 0:  54%|█████▎    | 890/1660 [01:46<01:31,  8.38it/s, loss=0.392, v_num=0000]\n",
      "Epoch 0:  54%|█████▎    | 891/1660 [01:46<01:31,  8.38it/s, loss=0.386, v_num=0000]\n",
      "Epoch 0:  54%|█████▎    | 892/1660 [01:46<01:31,  8.38it/s, loss=0.368, v_num=0000]\n",
      "Epoch 0:  54%|█████▍    | 893/1660 [01:46<01:31,  8.38it/s, loss=0.371, v_num=0000]\n",
      "Epoch 0:  54%|█████▍    | 894/1660 [01:46<01:31,  8.38it/s, loss=0.378, v_num=0000]\n",
      "Epoch 0:  54%|█████▍    | 895/1660 [01:46<01:31,  8.38it/s, loss=0.374, v_num=0000]\n",
      "Epoch 0:  54%|█████▍    | 896/1660 [01:46<01:31,  8.38it/s, loss=0.377, v_num=0000]\n",
      "Epoch 0:  54%|█████▍    | 897/1660 [01:47<01:31,  8.38it/s, loss=0.379, v_num=0000]\n",
      "Epoch 0:  54%|█████▍    | 898/1660 [01:47<01:30,  8.38it/s, loss=0.371, v_num=0000]\n",
      "Epoch 0:  54%|█████▍    | 899/1660 [01:47<01:30,  8.38it/s, loss=0.369, v_num=0000]\n",
      "Epoch 0:  54%|█████▍    | 900/1660 [01:47<01:30,  8.38it/s, loss=0.356, v_num=0000]\n",
      "Epoch 0:  54%|█████▍    | 901/1660 [01:47<01:30,  8.38it/s, loss=0.359, v_num=0000]\n",
      "Epoch 0:  54%|█████▍    | 902/1660 [01:47<01:30,  8.38it/s, loss=0.361, v_num=0000]\n",
      "Epoch 0:  54%|█████▍    | 903/1660 [01:47<01:30,  8.38it/s, loss=0.356, v_num=0000]\n",
      "Epoch 0:  54%|█████▍    | 904/1660 [01:47<01:30,  8.38it/s, loss=0.364, v_num=0000]\n",
      "Epoch 0:  55%|█████▍    | 905/1660 [01:47<01:30,  8.38it/s, loss=0.375, v_num=0000]\n",
      "Epoch 0:  55%|█████▍    | 906/1660 [01:48<01:29,  8.38it/s, loss=0.368, v_num=0000]\n",
      "Epoch 0:  55%|█████▍    | 907/1660 [01:48<01:29,  8.38it/s, loss=0.36, v_num=0000] \n",
      "Epoch 0:  55%|█████▍    | 908/1660 [01:48<01:29,  8.38it/s, loss=0.363, v_num=0000]\n",
      "Epoch 0:  55%|█████▍    | 909/1660 [01:48<01:29,  8.38it/s, loss=0.37, v_num=0000] \n",
      "Epoch 0:  55%|█████▍    | 910/1660 [01:48<01:29,  8.38it/s, loss=0.363, v_num=0000]\n",
      "Epoch 0:  55%|█████▍    | 911/1660 [01:48<01:29,  8.38it/s, loss=0.369, v_num=0000]\n",
      "Epoch 0:  55%|█████▍    | 912/1660 [01:48<01:29,  8.38it/s, loss=0.377, v_num=0000]\n",
      "Epoch 0:  55%|█████▌    | 913/1660 [01:49<01:29,  8.38it/s, loss=0.372, v_num=0000]\n",
      "Epoch 0:  55%|█████▌    | 914/1660 [01:49<01:29,  8.38it/s, loss=0.366, v_num=0000]\n",
      "Epoch 0:  55%|█████▌    | 915/1660 [01:49<01:28,  8.37it/s, loss=0.363, v_num=0000]\n",
      "Epoch 0:  55%|█████▌    | 916/1660 [01:49<01:28,  8.37it/s, loss=0.36, v_num=0000] \n",
      "Epoch 0:  55%|█████▌    | 917/1660 [01:49<01:28,  8.37it/s, loss=0.364, v_num=0000]\n",
      "Epoch 0:  55%|█████▌    | 918/1660 [01:49<01:28,  8.37it/s, loss=0.36, v_num=0000] \n",
      "Epoch 0:  55%|█████▌    | 919/1660 [01:49<01:28,  8.37it/s, loss=0.364, v_num=0000]\n",
      "Epoch 0:  55%|█████▌    | 920/1660 [01:49<01:28,  8.37it/s, loss=0.366, v_num=0000]\n",
      "Epoch 0:  55%|█████▌    | 921/1660 [01:49<01:28,  8.37it/s, loss=0.362, v_num=0000]\n",
      "Epoch 0:  56%|█████▌    | 922/1660 [01:50<01:28,  8.37it/s, loss=0.361, v_num=0000]\n",
      "Epoch 0:  56%|█████▌    | 923/1660 [01:50<01:28,  8.37it/s, loss=0.364, v_num=0000]\n",
      "Epoch 0:  56%|█████▌    | 924/1660 [01:50<01:27,  8.37it/s, loss=0.354, v_num=0000]\n",
      "Epoch 0:  56%|█████▌    | 925/1660 [01:50<01:27,  8.37it/s, loss=0.347, v_num=0000]\n",
      "Epoch 0:  56%|█████▌    | 926/1660 [01:50<01:27,  8.37it/s, loss=0.352, v_num=0000]\n",
      "Epoch 0:  56%|█████▌    | 927/1660 [01:50<01:27,  8.37it/s, loss=0.352, v_num=0000]\n",
      "Epoch 0:  56%|█████▌    | 928/1660 [01:50<01:27,  8.37it/s, loss=0.354, v_num=0000]\n",
      "Epoch 0:  56%|█████▌    | 929/1660 [01:50<01:27,  8.37it/s, loss=0.349, v_num=0000]\n",
      "== Status ==\n",
      "Current time: 2022-06-20 12:02:12 (running for 00:02:05.58)\n",
      "Memory usage on this node: 29.9/376.5 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1.0/8 CPUs, 1.0/1 GPUs, 0.0/34.4 GiB heap, 0.0/17.2 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/dobby/ray_results/tune_rp\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+----------------------+----------+------------------+-----------------+----------------+\n",
      "| Trial name           | status   | loc              |   learning_rate |   weight_decay |\n",
      "|----------------------+----------+------------------+-----------------+----------------|\n",
      "| train_rp_86c32_00000 | RUNNING  | 172.17.0.5:17844 |     0.000368944 |            0.1 |\n",
      "+----------------------+----------+------------------+-----------------+----------------+\n",
      "\n",
      "\n",
      "Epoch 0:  56%|█████▌    | 930/1660 [01:51<01:27,  8.37it/s, loss=0.352, v_num=0000]\n",
      "Epoch 0:  56%|█████▌    | 931/1660 [01:51<01:27,  8.37it/s, loss=0.349, v_num=0000]\n",
      "Epoch 0:  56%|█████▌    | 932/1660 [01:51<01:26,  8.37it/s, loss=0.346, v_num=0000]\n",
      "Epoch 0:  56%|█████▌    | 933/1660 [01:51<01:26,  8.37it/s, loss=0.35, v_num=0000] \n",
      "Epoch 0:  56%|█████▋    | 934/1660 [01:51<01:26,  8.37it/s, loss=0.351, v_num=0000]\n",
      "Epoch 0:  56%|█████▋    | 935/1660 [01:51<01:26,  8.37it/s, loss=0.354, v_num=0000]\n",
      "Epoch 0:  56%|█████▋    | 936/1660 [01:51<01:26,  8.37it/s, loss=0.357, v_num=0000]\n",
      "Epoch 0:  56%|█████▋    | 937/1660 [01:51<01:26,  8.37it/s, loss=0.349, v_num=0000]\n",
      "Epoch 0:  57%|█████▋    | 938/1660 [01:52<01:26,  8.37it/s, loss=0.355, v_num=0000]\n",
      "Epoch 0:  57%|█████▋    | 939/1660 [01:52<01:26,  8.37it/s, loss=0.357, v_num=0000]\n",
      "Epoch 0:  57%|█████▋    | 940/1660 [01:52<01:25,  8.37it/s, loss=0.356, v_num=0000]\n",
      "Epoch 0:  57%|█████▋    | 941/1660 [01:52<01:25,  8.37it/s, loss=0.361, v_num=0000]\n",
      "Epoch 0:  57%|█████▋    | 942/1660 [01:52<01:25,  8.37it/s, loss=0.352, v_num=0000]\n",
      "Epoch 0:  57%|█████▋    | 943/1660 [01:52<01:25,  8.37it/s, loss=0.35, v_num=0000] \n",
      "Epoch 0:  57%|█████▋    | 944/1660 [01:52<01:25,  8.37it/s, loss=0.351, v_num=0000]\n",
      "Epoch 0:  57%|█████▋    | 945/1660 [01:52<01:25,  8.37it/s, loss=0.35, v_num=0000] \n",
      "Epoch 0:  57%|█████▋    | 946/1660 [01:53<01:25,  8.37it/s, loss=0.352, v_num=0000]\n",
      "Epoch 0:  57%|█████▋    | 947/1660 [01:53<01:25,  8.37it/s, loss=0.36, v_num=0000] \n",
      "Epoch 0:  57%|█████▋    | 948/1660 [01:53<01:25,  8.37it/s, loss=0.366, v_num=0000]\n",
      "Epoch 0:  57%|█████▋    | 949/1660 [01:53<01:24,  8.37it/s, loss=0.37, v_num=0000] \n",
      "Epoch 0:  57%|█████▋    | 950/1660 [01:53<01:24,  8.37it/s, loss=0.376, v_num=0000]\n",
      "Epoch 0:  57%|█████▋    | 951/1660 [01:53<01:24,  8.37it/s, loss=0.378, v_num=0000]\n",
      "Epoch 0:  57%|█████▋    | 952/1660 [01:53<01:24,  8.37it/s, loss=0.374, v_num=0000]\n",
      "Epoch 0:  57%|█████▋    | 953/1660 [01:53<01:24,  8.37it/s, loss=0.373, v_num=0000]\n",
      "Epoch 0:  57%|█████▋    | 954/1660 [01:53<01:24,  8.37it/s, loss=0.373, v_num=0000]\n",
      "Epoch 0:  58%|█████▊    | 955/1660 [01:54<01:24,  8.37it/s, loss=0.365, v_num=0000]\n",
      "Epoch 0:  58%|█████▊    | 956/1660 [01:54<01:24,  8.37it/s, loss=0.36, v_num=0000] \n",
      "Epoch 0:  58%|█████▊    | 957/1660 [01:54<01:23,  8.37it/s, loss=0.365, v_num=0000]\n",
      "Epoch 0:  58%|█████▊    | 958/1660 [01:54<01:23,  8.37it/s, loss=0.363, v_num=0000]\n",
      "Epoch 0:  58%|█████▊    | 959/1660 [01:54<01:23,  8.37it/s, loss=0.359, v_num=0000]\n",
      "Epoch 0:  58%|█████▊    | 960/1660 [01:54<01:23,  8.37it/s, loss=0.361, v_num=0000]\n",
      "Epoch 0:  58%|█████▊    | 961/1660 [01:54<01:23,  8.37it/s, loss=0.355, v_num=0000]\n",
      "Epoch 0:  58%|█████▊    | 962/1660 [01:54<01:23,  8.37it/s, loss=0.362, v_num=0000]\n",
      "Epoch 0:  58%|█████▊    | 963/1660 [01:55<01:23,  8.37it/s, loss=0.362, v_num=0000]\n",
      "Epoch 0:  58%|█████▊    | 964/1660 [01:55<01:23,  8.37it/s, loss=0.359, v_num=0000]\n",
      "Epoch 0:  58%|█████▊    | 965/1660 [01:55<01:23,  8.37it/s, loss=0.36, v_num=0000] \n",
      "Epoch 0:  58%|█████▊    | 966/1660 [01:55<01:22,  8.37it/s, loss=0.357, v_num=0000]\n",
      "Epoch 0:  58%|█████▊    | 967/1660 [01:55<01:22,  8.37it/s, loss=0.352, v_num=0000]\n",
      "Epoch 0:  58%|█████▊    | 968/1660 [01:55<01:22,  8.37it/s, loss=0.347, v_num=0000]\n",
      "Epoch 0:  58%|█████▊    | 969/1660 [01:55<01:22,  8.37it/s, loss=0.34, v_num=0000] \n",
      "Epoch 0:  58%|█████▊    | 970/1660 [01:55<01:22,  8.37it/s, loss=0.324, v_num=0000]\n",
      "Epoch 0:  58%|█████▊    | 971/1660 [01:56<01:22,  8.37it/s, loss=0.319, v_num=0000]\n",
      "== Status ==\n",
      "Current time: 2022-06-20 12:02:17 (running for 00:02:10.58)\n",
      "Memory usage on this node: 29.9/376.5 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1.0/8 CPUs, 1.0/1 GPUs, 0.0/34.4 GiB heap, 0.0/17.2 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/dobby/ray_results/tune_rp\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+----------------------+----------+------------------+-----------------+----------------+\n",
      "| Trial name           | status   | loc              |   learning_rate |   weight_decay |\n",
      "|----------------------+----------+------------------+-----------------+----------------|\n",
      "| train_rp_86c32_00000 | RUNNING  | 172.17.0.5:17844 |     0.000368944 |            0.1 |\n",
      "+----------------------+----------+------------------+-----------------+----------------+\n",
      "\n",
      "\n",
      "Epoch 0:  59%|█████▊    | 972/1660 [01:56<01:22,  8.37it/s, loss=0.317, v_num=0000]\n",
      "Epoch 0:  59%|█████▊    | 973/1660 [01:56<01:22,  8.37it/s, loss=0.316, v_num=0000]\n",
      "Epoch 0:  59%|█████▊    | 974/1660 [01:56<01:21,  8.37it/s, loss=0.315, v_num=0000]\n",
      "Epoch 0:  59%|█████▊    | 975/1660 [01:56<01:21,  8.37it/s, loss=0.32, v_num=0000] \n",
      "Epoch 0:  59%|█████▉    | 976/1660 [01:56<01:21,  8.36it/s, loss=0.32, v_num=0000]\n",
      "Epoch 0:  59%|█████▉    | 977/1660 [01:56<01:21,  8.36it/s, loss=0.324, v_num=0000]\n",
      "Epoch 0:  59%|█████▉    | 978/1660 [01:56<01:21,  8.36it/s, loss=0.324, v_num=0000]\n",
      "Epoch 0:  59%|█████▉    | 979/1660 [01:57<01:21,  8.36it/s, loss=0.319, v_num=0000]\n",
      "Epoch 0:  59%|█████▉    | 980/1660 [01:57<01:21,  8.36it/s, loss=0.313, v_num=0000]\n",
      "Epoch 0:  59%|█████▉    | 981/1660 [01:57<01:21,  8.36it/s, loss=0.313, v_num=0000]\n",
      "Epoch 0:  59%|█████▉    | 982/1660 [01:57<01:21,  8.36it/s, loss=0.32, v_num=0000] \n",
      "Epoch 0:  59%|█████▉    | 983/1660 [01:57<01:20,  8.36it/s, loss=0.325, v_num=0000]\n",
      "Epoch 0:  59%|█████▉    | 984/1660 [01:57<01:20,  8.36it/s, loss=0.323, v_num=0000]\n",
      "Epoch 0:  59%|█████▉    | 985/1660 [01:57<01:20,  8.36it/s, loss=0.34, v_num=0000] \n",
      "Epoch 0:  59%|█████▉    | 986/1660 [01:57<01:20,  8.36it/s, loss=0.336, v_num=0000]\n",
      "Epoch 0:  59%|█████▉    | 987/1660 [01:58<01:20,  8.36it/s, loss=0.332, v_num=0000]\n",
      "Epoch 0:  60%|█████▉    | 988/1660 [01:58<01:20,  8.36it/s, loss=0.329, v_num=0000]\n",
      "Epoch 0:  60%|█████▉    | 989/1660 [01:58<01:20,  8.36it/s, loss=0.329, v_num=0000]\n",
      "Epoch 0:  60%|█████▉    | 990/1660 [01:58<01:20,  8.36it/s, loss=0.348, v_num=0000]\n",
      "Epoch 0:  60%|█████▉    | 991/1660 [01:58<01:20,  8.36it/s, loss=0.35, v_num=0000] \n",
      "Epoch 0:  60%|█████▉    | 992/1660 [01:58<01:19,  8.36it/s, loss=0.355, v_num=0000]\n",
      "Epoch 0:  60%|█████▉    | 993/1660 [01:58<01:19,  8.36it/s, loss=0.356, v_num=0000]\n",
      "Epoch 0:  60%|█████▉    | 994/1660 [01:58<01:19,  8.36it/s, loss=0.357, v_num=0000]\n",
      "Epoch 0:  60%|█████▉    | 995/1660 [01:59<01:19,  8.36it/s, loss=0.352, v_num=0000]\n",
      "Epoch 0:  60%|██████    | 996/1660 [01:59<01:19,  8.36it/s, loss=0.356, v_num=0000]\n",
      "Epoch 0:  60%|██████    | 997/1660 [01:59<01:19,  8.36it/s, loss=0.346, v_num=0000]\n",
      "Epoch 0:  60%|██████    | 998/1660 [01:59<01:19,  8.36it/s, loss=0.344, v_num=0000]\n",
      "Epoch 0:  60%|██████    | 999/1660 [01:59<01:19,  8.36it/s, loss=0.348, v_num=0000]\n",
      "Epoch 0:  60%|██████    | 1000/1660 [01:59<01:18,  8.36it/s, loss=0.348, v_num=0000]\n",
      "Epoch 0:  60%|██████    | 1001/1660 [01:59<01:18,  8.36it/s, loss=0.346, v_num=0000]\n",
      "Epoch 0:  60%|██████    | 1002/1660 [01:59<01:18,  8.36it/s, loss=0.333, v_num=0000]\n",
      "Epoch 0:  60%|██████    | 1003/1660 [02:00<01:18,  8.36it/s, loss=0.326, v_num=0000]\n",
      "Epoch 0:  60%|██████    | 1004/1660 [02:00<01:18,  8.36it/s, loss=0.34, v_num=0000] \n",
      "Epoch 0:  61%|██████    | 1005/1660 [02:00<01:18,  8.36it/s, loss=0.321, v_num=0000]\n",
      "Epoch 0:  61%|██████    | 1006/1660 [02:00<01:18,  8.36it/s, loss=0.318, v_num=0000]\n",
      "Epoch 0:  61%|██████    | 1007/1660 [02:00<01:18,  8.36it/s, loss=0.317, v_num=0000]\n",
      "Epoch 0:  61%|██████    | 1008/1660 [02:00<01:18,  8.35it/s, loss=0.32, v_num=0000] \n",
      "Epoch 0:  61%|██████    | 1009/1660 [02:00<01:17,  8.35it/s, loss=0.322, v_num=0000]\n",
      "Epoch 0:  61%|██████    | 1010/1660 [02:00<01:17,  8.35it/s, loss=0.316, v_num=0000]\n",
      "Epoch 0:  61%|██████    | 1011/1660 [02:01<01:17,  8.35it/s, loss=0.315, v_num=0000]\n",
      "== Status ==\n",
      "Current time: 2022-06-20 12:02:22 (running for 00:02:15.58)\n",
      "Memory usage on this node: 30.0/376.5 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1.0/8 CPUs, 1.0/1 GPUs, 0.0/34.4 GiB heap, 0.0/17.2 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/dobby/ray_results/tune_rp\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+----------------------+----------+------------------+-----------------+----------------+\n",
      "| Trial name           | status   | loc              |   learning_rate |   weight_decay |\n",
      "|----------------------+----------+------------------+-----------------+----------------|\n",
      "| train_rp_86c32_00000 | RUNNING  | 172.17.0.5:17844 |     0.000368944 |            0.1 |\n",
      "+----------------------+----------+------------------+-----------------+----------------+\n",
      "\n",
      "\n",
      "Epoch 0:  61%|██████    | 1012/1660 [02:01<01:17,  8.35it/s, loss=0.313, v_num=0000]\n",
      "Epoch 0:  61%|██████    | 1013/1660 [02:01<01:17,  8.35it/s, loss=0.313, v_num=0000]\n",
      "Epoch 0:  61%|██████    | 1014/1660 [02:01<01:17,  8.35it/s, loss=0.315, v_num=0000]\n",
      "Epoch 0:  61%|██████    | 1015/1660 [02:01<01:17,  8.35it/s, loss=0.324, v_num=0000]\n",
      "Epoch 0:  61%|██████    | 1016/1660 [02:01<01:17,  8.35it/s, loss=0.325, v_num=0000]\n",
      "Epoch 0:  61%|██████▏   | 1017/1660 [02:01<01:16,  8.35it/s, loss=0.329, v_num=0000]\n",
      "Epoch 0:  61%|██████▏   | 1018/1660 [02:01<01:16,  8.35it/s, loss=0.333, v_num=0000]\n",
      "Epoch 0:  61%|██████▏   | 1019/1660 [02:01<01:16,  8.35it/s, loss=0.327, v_num=0000]\n",
      "Epoch 0:  61%|██████▏   | 1020/1660 [02:02<01:16,  8.35it/s, loss=0.33, v_num=0000] \n",
      "Epoch 0:  62%|██████▏   | 1021/1660 [02:02<01:16,  8.35it/s, loss=0.331, v_num=0000]\n",
      "Epoch 0:  62%|██████▏   | 1022/1660 [02:02<01:16,  8.35it/s, loss=0.334, v_num=0000]\n",
      "Epoch 0:  62%|██████▏   | 1023/1660 [02:02<01:16,  8.35it/s, loss=0.34, v_num=0000] \n",
      "Epoch 0:  62%|██████▏   | 1024/1660 [02:02<01:16,  8.35it/s, loss=0.331, v_num=0000]\n",
      "Epoch 0:  62%|██████▏   | 1025/1660 [02:02<01:16,  8.35it/s, loss=0.331, v_num=0000]\n",
      "Epoch 0:  62%|██████▏   | 1026/1660 [02:02<01:15,  8.35it/s, loss=0.336, v_num=0000]\n",
      "Epoch 0:  62%|██████▏   | 1027/1660 [02:02<01:15,  8.35it/s, loss=0.343, v_num=0000]\n",
      "Epoch 0:  62%|██████▏   | 1028/1660 [02:03<01:15,  8.35it/s, loss=0.336, v_num=0000]\n",
      "Epoch 0:  62%|██████▏   | 1029/1660 [02:03<01:15,  8.35it/s, loss=0.338, v_num=0000]\n",
      "Epoch 0:  62%|██████▏   | 1030/1660 [02:03<01:15,  8.35it/s, loss=0.33, v_num=0000] \n",
      "Epoch 0:  62%|██████▏   | 1031/1660 [02:03<01:15,  8.35it/s, loss=0.345, v_num=0000]\n",
      "Epoch 0:  62%|██████▏   | 1032/1660 [02:03<01:15,  8.35it/s, loss=0.351, v_num=0000]\n",
      "Epoch 0:  62%|██████▏   | 1033/1660 [02:03<01:15,  8.35it/s, loss=0.347, v_num=0000]\n",
      "Epoch 0:  62%|██████▏   | 1034/1660 [02:03<01:14,  8.35it/s, loss=0.349, v_num=0000]\n",
      "Epoch 0:  62%|██████▏   | 1035/1660 [02:03<01:14,  8.35it/s, loss=0.343, v_num=0000]\n",
      "Epoch 0:  62%|██████▏   | 1036/1660 [02:04<01:14,  8.35it/s, loss=0.345, v_num=0000]\n",
      "Epoch 0:  62%|██████▏   | 1037/1660 [02:04<01:14,  8.35it/s, loss=0.341, v_num=0000]\n",
      "Epoch 0:  63%|██████▎   | 1038/1660 [02:04<01:14,  8.35it/s, loss=0.339, v_num=0000]\n",
      "Epoch 0:  63%|██████▎   | 1039/1660 [02:04<01:14,  8.35it/s, loss=0.343, v_num=0000]\n",
      "Epoch 0:  63%|██████▎   | 1040/1660 [02:04<01:14,  8.35it/s, loss=0.344, v_num=0000]\n",
      "Epoch 0:  63%|██████▎   | 1041/1660 [02:04<01:14,  8.35it/s, loss=0.34, v_num=0000] \n",
      "Epoch 0:  63%|██████▎   | 1042/1660 [02:04<01:14,  8.35it/s, loss=0.339, v_num=0000]\n",
      "Epoch 0:  63%|██████▎   | 1043/1660 [02:04<01:13,  8.35it/s, loss=0.342, v_num=0000]\n",
      "Epoch 0:  63%|██████▎   | 1044/1660 [02:05<01:13,  8.35it/s, loss=0.341, v_num=0000]\n",
      "Epoch 0:  63%|██████▎   | 1045/1660 [02:05<01:13,  8.35it/s, loss=0.347, v_num=0000]\n",
      "Epoch 0:  63%|██████▎   | 1046/1660 [02:05<01:13,  8.35it/s, loss=0.339, v_num=0000]\n",
      "Epoch 0:  63%|██████▎   | 1047/1660 [02:05<01:13,  8.35it/s, loss=0.333, v_num=0000]\n",
      "Epoch 0:  63%|██████▎   | 1048/1660 [02:05<01:13,  8.35it/s, loss=0.34, v_num=0000] \n",
      "Epoch 0:  63%|██████▎   | 1049/1660 [02:05<01:13,  8.35it/s, loss=0.336, v_num=0000]\n",
      "Epoch 0:  63%|██████▎   | 1050/1660 [02:05<01:13,  8.35it/s, loss=0.335, v_num=0000]\n",
      "Epoch 0:  63%|██████▎   | 1051/1660 [02:05<01:12,  8.35it/s, loss=0.318, v_num=0000]\n",
      "Epoch 0:  63%|██████▎   | 1052/1660 [02:05<01:12,  8.35it/s, loss=0.321, v_num=0000]== Status ==\n",
      "Current time: 2022-06-20 12:02:27 (running for 00:02:20.59)\n",
      "Memory usage on this node: 30.0/376.5 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1.0/8 CPUs, 1.0/1 GPUs, 0.0/34.4 GiB heap, 0.0/17.2 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/dobby/ray_results/tune_rp\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+----------------------+----------+------------------+-----------------+----------------+\n",
      "| Trial name           | status   | loc              |   learning_rate |   weight_decay |\n",
      "|----------------------+----------+------------------+-----------------+----------------|\n",
      "| train_rp_86c32_00000 | RUNNING  | 172.17.0.5:17844 |     0.000368944 |            0.1 |\n",
      "+----------------------+----------+------------------+-----------------+----------------+\n",
      "\n",
      "\n",
      "\n",
      "Epoch 0:  63%|██████▎   | 1053/1660 [02:06<01:12,  8.35it/s, loss=0.322, v_num=0000]\n",
      "Epoch 0:  63%|██████▎   | 1054/1660 [02:06<01:12,  8.35it/s, loss=0.316, v_num=0000]\n",
      "Epoch 0:  64%|██████▎   | 1055/1660 [02:06<01:12,  8.35it/s, loss=0.322, v_num=0000]\n",
      "Epoch 0:  64%|██████▎   | 1056/1660 [02:06<01:12,  8.35it/s, loss=0.324, v_num=0000]\n",
      "Epoch 0:  64%|██████▎   | 1057/1660 [02:06<01:12,  8.35it/s, loss=0.325, v_num=0000]\n",
      "Epoch 0:  64%|██████▎   | 1058/1660 [02:06<01:12,  8.35it/s, loss=0.322, v_num=0000]\n",
      "Epoch 0:  64%|██████▍   | 1059/1660 [02:06<01:11,  8.35it/s, loss=0.331, v_num=0000]\n",
      "Epoch 0:  64%|██████▍   | 1060/1660 [02:06<01:11,  8.35it/s, loss=0.324, v_num=0000]\n",
      "Epoch 0:  64%|██████▍   | 1061/1660 [02:07<01:11,  8.35it/s, loss=0.328, v_num=0000]\n",
      "Epoch 0:  64%|██████▍   | 1062/1660 [02:07<01:11,  8.35it/s, loss=0.33, v_num=0000] \n",
      "Epoch 0:  64%|██████▍   | 1063/1660 [02:07<01:11,  8.35it/s, loss=0.322, v_num=0000]\n",
      "Epoch 0:  64%|██████▍   | 1064/1660 [02:07<01:11,  8.35it/s, loss=0.332, v_num=0000]\n",
      "Epoch 0:  64%|██████▍   | 1065/1660 [02:07<01:11,  8.35it/s, loss=0.326, v_num=0000]\n",
      "Epoch 0:  64%|██████▍   | 1066/1660 [02:07<01:11,  8.35it/s, loss=0.33, v_num=0000] \n",
      "Epoch 0:  64%|██████▍   | 1067/1660 [02:07<01:11,  8.35it/s, loss=0.337, v_num=0000]\n",
      "Epoch 0:  64%|██████▍   | 1068/1660 [02:07<01:10,  8.35it/s, loss=0.336, v_num=0000]\n",
      "Epoch 0:  64%|██████▍   | 1069/1660 [02:08<01:10,  8.35it/s, loss=0.341, v_num=0000]\n",
      "Epoch 0:  64%|██████▍   | 1070/1660 [02:08<01:10,  8.35it/s, loss=0.35, v_num=0000] \n",
      "Epoch 0:  65%|██████▍   | 1071/1660 [02:08<01:10,  8.35it/s, loss=0.352, v_num=0000]\n",
      "Epoch 0:  65%|██████▍   | 1072/1660 [02:08<01:10,  8.34it/s, loss=0.348, v_num=0000]\n",
      "Epoch 0:  65%|██████▍   | 1073/1660 [02:08<01:10,  8.34it/s, loss=0.352, v_num=0000]\n",
      "Epoch 0:  65%|██████▍   | 1074/1660 [02:08<01:10,  8.34it/s, loss=0.356, v_num=0000]\n",
      "Epoch 0:  65%|██████▍   | 1075/1660 [02:08<01:10,  8.34it/s, loss=0.352, v_num=0000]\n",
      "Epoch 0:  65%|██████▍   | 1076/1660 [02:09<01:10,  8.34it/s, loss=0.34, v_num=0000] \n",
      "Epoch 0:  65%|██████▍   | 1077/1660 [02:09<01:09,  8.34it/s, loss=0.344, v_num=0000]\n",
      "Epoch 0:  65%|██████▍   | 1078/1660 [02:09<01:09,  8.34it/s, loss=0.341, v_num=0000]\n",
      "Epoch 0:  65%|██████▌   | 1079/1660 [02:09<01:09,  8.34it/s, loss=0.342, v_num=0000]\n",
      "Epoch 0:  65%|██████▌   | 1080/1660 [02:09<01:09,  8.34it/s, loss=0.342, v_num=0000]\n",
      "Epoch 0:  65%|██████▌   | 1081/1660 [02:09<01:09,  8.34it/s, loss=0.34, v_num=0000] \n",
      "Epoch 0:  65%|██████▌   | 1082/1660 [02:09<01:09,  8.34it/s, loss=0.341, v_num=0000]\n",
      "Epoch 0:  65%|██████▌   | 1083/1660 [02:09<01:09,  8.34it/s, loss=0.346, v_num=0000]\n",
      "Epoch 0:  65%|██████▌   | 1084/1660 [02:10<01:09,  8.34it/s, loss=0.34, v_num=0000] \n",
      "Epoch 0:  65%|██████▌   | 1085/1660 [02:10<01:08,  8.34it/s, loss=0.342, v_num=0000]\n",
      "Epoch 0:  65%|██████▌   | 1086/1660 [02:10<01:08,  8.34it/s, loss=0.346, v_num=0000]\n",
      "Epoch 0:  65%|██████▌   | 1087/1660 [02:10<01:08,  8.34it/s, loss=0.343, v_num=0000]\n",
      "Epoch 0:  66%|██████▌   | 1088/1660 [02:10<01:08,  8.34it/s, loss=0.345, v_num=0000]\n",
      "Epoch 0:  66%|██████▌   | 1089/1660 [02:10<01:08,  8.33it/s, loss=0.341, v_num=0000]\n",
      "Epoch 0:  66%|██████▌   | 1090/1660 [02:10<01:08,  8.34it/s, loss=0.334, v_num=0000]\n",
      "Epoch 0:  66%|██████▌   | 1091/1660 [02:10<01:08,  8.33it/s, loss=0.341, v_num=0000]\n",
      "== Status ==\n",
      "Current time: 2022-06-20 12:02:32 (running for 00:02:25.59)\n",
      "Memory usage on this node: 30.1/376.5 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1.0/8 CPUs, 1.0/1 GPUs, 0.0/34.4 GiB heap, 0.0/17.2 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/dobby/ray_results/tune_rp\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+----------------------+----------+------------------+-----------------+----------------+\n",
      "| Trial name           | status   | loc              |   learning_rate |   weight_decay |\n",
      "|----------------------+----------+------------------+-----------------+----------------|\n",
      "| train_rp_86c32_00000 | RUNNING  | 172.17.0.5:17844 |     0.000368944 |            0.1 |\n",
      "+----------------------+----------+------------------+-----------------+----------------+\n",
      "\n",
      "\n",
      "Epoch 0:  66%|██████▌   | 1092/1660 [02:11<01:08,  8.33it/s, loss=0.336, v_num=0000]\n",
      "Epoch 0:  66%|██████▌   | 1093/1660 [02:11<01:08,  8.33it/s, loss=0.34, v_num=0000] \n",
      "Epoch 0:  66%|██████▌   | 1094/1660 [02:11<01:07,  8.34it/s, loss=0.342, v_num=0000]\n",
      "Epoch 0:  66%|██████▌   | 1095/1660 [02:11<01:07,  8.33it/s, loss=0.342, v_num=0000]\n",
      "Epoch 0:  66%|██████▌   | 1096/1660 [02:11<01:07,  8.33it/s, loss=0.35, v_num=0000] \n",
      "Epoch 0:  66%|██████▌   | 1097/1660 [02:11<01:07,  8.33it/s, loss=0.353, v_num=0000]\n",
      "Epoch 0:  66%|██████▌   | 1098/1660 [02:11<01:07,  8.33it/s, loss=0.366, v_num=0000]\n",
      "Epoch 0:  66%|██████▌   | 1099/1660 [02:11<01:07,  8.34it/s, loss=0.357, v_num=0000]\n",
      "Epoch 0:  66%|██████▋   | 1100/1660 [02:11<01:07,  8.34it/s, loss=0.37, v_num=0000] \n",
      "Epoch 0:  66%|██████▋   | 1101/1660 [02:12<01:07,  8.34it/s, loss=0.372, v_num=0000]\n",
      "Epoch 0:  66%|██████▋   | 1102/1660 [02:12<01:06,  8.34it/s, loss=0.374, v_num=0000]\n",
      "Epoch 0:  66%|██████▋   | 1103/1660 [02:12<01:06,  8.34it/s, loss=0.366, v_num=0000]\n",
      "Epoch 0:  67%|██████▋   | 1104/1660 [02:12<01:06,  8.33it/s, loss=0.359, v_num=0000]\n",
      "Epoch 0:  67%|██████▋   | 1105/1660 [02:12<01:06,  8.33it/s, loss=0.351, v_num=0000]\n",
      "Epoch 0:  67%|██████▋   | 1106/1660 [02:12<01:06,  8.33it/s, loss=0.353, v_num=0000]\n",
      "Epoch 0:  67%|██████▋   | 1107/1660 [02:12<01:06,  8.33it/s, loss=0.352, v_num=0000]\n",
      "Epoch 0:  67%|██████▋   | 1108/1660 [02:12<01:06,  8.33it/s, loss=0.346, v_num=0000]\n",
      "Epoch 0:  67%|██████▋   | 1109/1660 [02:13<01:06,  8.33it/s, loss=0.352, v_num=0000]\n",
      "Epoch 0:  67%|██████▋   | 1110/1660 [02:13<01:06,  8.33it/s, loss=0.358, v_num=0000]\n",
      "Epoch 0:  67%|██████▋   | 1111/1660 [02:13<01:05,  8.33it/s, loss=0.359, v_num=0000]\n",
      "Epoch 0:  67%|██████▋   | 1112/1660 [02:13<01:05,  8.33it/s, loss=0.351, v_num=0000]\n",
      "Epoch 0:  67%|██████▋   | 1113/1660 [02:13<01:05,  8.33it/s, loss=0.348, v_num=0000]\n",
      "Epoch 0:  67%|██████▋   | 1114/1660 [02:13<01:05,  8.33it/s, loss=0.34, v_num=0000] \n",
      "Epoch 0:  67%|██████▋   | 1115/1660 [02:13<01:05,  8.33it/s, loss=0.34, v_num=0000]\n",
      "Epoch 0:  67%|██████▋   | 1116/1660 [02:13<01:05,  8.33it/s, loss=0.341, v_num=0000]\n",
      "Epoch 0:  67%|██████▋   | 1117/1660 [02:14<01:05,  8.33it/s, loss=0.341, v_num=0000]\n",
      "Epoch 0:  67%|██████▋   | 1118/1660 [02:14<01:05,  8.33it/s, loss=0.33, v_num=0000] \n",
      "Epoch 0:  67%|██████▋   | 1119/1660 [02:14<01:04,  8.33it/s, loss=0.334, v_num=0000]\n",
      "Epoch 0:  67%|██████▋   | 1120/1660 [02:14<01:04,  8.33it/s, loss=0.324, v_num=0000]\n",
      "Epoch 0:  68%|██████▊   | 1121/1660 [02:14<01:04,  8.33it/s, loss=0.322, v_num=0000]\n",
      "Epoch 0:  68%|██████▊   | 1122/1660 [02:14<01:04,  8.33it/s, loss=0.321, v_num=0000]\n",
      "Epoch 0:  68%|██████▊   | 1123/1660 [02:14<01:04,  8.33it/s, loss=0.322, v_num=0000]\n",
      "Epoch 0:  68%|██████▊   | 1124/1660 [02:14<01:04,  8.33it/s, loss=0.323, v_num=0000]\n",
      "Epoch 0:  68%|██████▊   | 1125/1660 [02:15<01:04,  8.33it/s, loss=0.333, v_num=0000]\n",
      "Epoch 0:  68%|██████▊   | 1126/1660 [02:15<01:04,  8.33it/s, loss=0.321, v_num=0000]\n",
      "Epoch 0:  68%|██████▊   | 1127/1660 [02:15<01:03,  8.33it/s, loss=0.329, v_num=0000]\n",
      "Epoch 0:  68%|██████▊   | 1128/1660 [02:15<01:03,  8.33it/s, loss=0.336, v_num=0000]\n",
      "Epoch 0:  68%|██████▊   | 1129/1660 [02:15<01:03,  8.33it/s, loss=0.33, v_num=0000] \n",
      "Epoch 0:  68%|██████▊   | 1130/1660 [02:15<01:03,  8.33it/s, loss=0.325, v_num=0000]\n",
      "Epoch 0:  68%|██████▊   | 1131/1660 [02:15<01:03,  8.33it/s, loss=0.327, v_num=0000]\n",
      "Epoch 0:  68%|██████▊   | 1132/1660 [02:15<01:03,  8.33it/s, loss=0.339, v_num=0000]\n",
      "Epoch 0:  68%|██████▊   | 1133/1660 [02:15<01:03,  8.33it/s, loss=0.334, v_num=0000]\n",
      "== Status ==\n",
      "Current time: 2022-06-20 12:02:37 (running for 00:02:30.59)\n",
      "Memory usage on this node: 30.1/376.5 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1.0/8 CPUs, 1.0/1 GPUs, 0.0/34.4 GiB heap, 0.0/17.2 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/dobby/ray_results/tune_rp\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+----------------------+----------+------------------+-----------------+----------------+\n",
      "| Trial name           | status   | loc              |   learning_rate |   weight_decay |\n",
      "|----------------------+----------+------------------+-----------------+----------------|\n",
      "| train_rp_86c32_00000 | RUNNING  | 172.17.0.5:17844 |     0.000368944 |            0.1 |\n",
      "+----------------------+----------+------------------+-----------------+----------------+\n",
      "\n",
      "\n",
      "Epoch 0:  68%|██████▊   | 1134/1660 [02:16<01:03,  8.33it/s, loss=0.338, v_num=0000]\n",
      "Epoch 0:  68%|██████▊   | 1135/1660 [02:16<01:02,  8.33it/s, loss=0.343, v_num=0000]\n",
      "Epoch 0:  68%|██████▊   | 1136/1660 [02:16<01:02,  8.33it/s, loss=0.343, v_num=0000]\n",
      "Epoch 0:  68%|██████▊   | 1137/1660 [02:16<01:02,  8.33it/s, loss=0.343, v_num=0000]\n",
      "Epoch 0:  69%|██████▊   | 1138/1660 [02:16<01:02,  8.33it/s, loss=0.343, v_num=0000]\n",
      "Epoch 0:  69%|██████▊   | 1139/1660 [02:16<01:02,  8.33it/s, loss=0.336, v_num=0000]\n",
      "Epoch 0:  69%|██████▊   | 1140/1660 [02:16<01:02,  8.33it/s, loss=0.331, v_num=0000]\n",
      "Epoch 0:  69%|██████▊   | 1141/1660 [02:16<01:02,  8.33it/s, loss=0.329, v_num=0000]\n",
      "Epoch 0:  69%|██████▉   | 1142/1660 [02:17<01:02,  8.33it/s, loss=0.322, v_num=0000]\n",
      "Epoch 0:  69%|██████▉   | 1143/1660 [02:17<01:02,  8.33it/s, loss=0.328, v_num=0000]\n",
      "Epoch 0:  69%|██████▉   | 1144/1660 [02:17<01:01,  8.33it/s, loss=0.327, v_num=0000]\n",
      "Epoch 0:  69%|██████▉   | 1145/1660 [02:17<01:01,  8.33it/s, loss=0.326, v_num=0000]\n",
      "Epoch 0:  69%|██████▉   | 1146/1660 [02:17<01:01,  8.33it/s, loss=0.333, v_num=0000]\n",
      "Epoch 0:  69%|██████▉   | 1147/1660 [02:17<01:01,  8.33it/s, loss=0.325, v_num=0000]\n",
      "Epoch 0:  69%|██████▉   | 1148/1660 [02:17<01:01,  8.33it/s, loss=0.324, v_num=0000]\n",
      "Epoch 0:  69%|██████▉   | 1149/1660 [02:17<01:01,  8.33it/s, loss=0.33, v_num=0000] \n",
      "Epoch 0:  69%|██████▉   | 1150/1660 [02:17<01:01,  8.33it/s, loss=0.328, v_num=0000]\n",
      "Epoch 0:  69%|██████▉   | 1151/1660 [02:18<01:01,  8.33it/s, loss=0.325, v_num=0000]\n",
      "Epoch 0:  69%|██████▉   | 1152/1660 [02:18<01:00,  8.33it/s, loss=0.326, v_num=0000]\n",
      "Epoch 0:  69%|██████▉   | 1153/1660 [02:18<01:00,  8.33it/s, loss=0.334, v_num=0000]\n",
      "Epoch 0:  70%|██████▉   | 1154/1660 [02:18<01:00,  8.33it/s, loss=0.337, v_num=0000]\n",
      "Epoch 0:  70%|██████▉   | 1155/1660 [02:18<01:00,  8.33it/s, loss=0.33, v_num=0000] \n",
      "Epoch 0:  70%|██████▉   | 1156/1660 [02:18<01:00,  8.33it/s, loss=0.326, v_num=0000]\n",
      "Epoch 0:  70%|██████▉   | 1157/1660 [02:18<01:00,  8.33it/s, loss=0.327, v_num=0000]\n",
      "Epoch 0:  70%|██████▉   | 1158/1660 [02:18<01:00,  8.33it/s, loss=0.333, v_num=0000]\n",
      "Epoch 0:  70%|██████▉   | 1159/1660 [02:19<01:00,  8.33it/s, loss=0.335, v_num=0000]\n",
      "Epoch 0:  70%|██████▉   | 1160/1660 [02:19<01:00,  8.33it/s, loss=0.344, v_num=0000]\n",
      "Epoch 0:  70%|██████▉   | 1161/1660 [02:19<00:59,  8.33it/s, loss=0.35, v_num=0000] \n",
      "Epoch 0:  70%|███████   | 1162/1660 [02:19<00:59,  8.33it/s, loss=0.356, v_num=0000]\n",
      "Epoch 0:  70%|███████   | 1163/1660 [02:19<00:59,  8.33it/s, loss=0.355, v_num=0000]\n",
      "Epoch 0:  70%|███████   | 1164/1660 [02:19<00:59,  8.33it/s, loss=0.361, v_num=0000]\n",
      "Epoch 0:  70%|███████   | 1165/1660 [02:19<00:59,  8.33it/s, loss=0.367, v_num=0000]\n",
      "Epoch 0:  70%|███████   | 1166/1660 [02:19<00:59,  8.33it/s, loss=0.368, v_num=0000]\n",
      "Epoch 0:  70%|███████   | 1167/1660 [02:20<00:59,  8.33it/s, loss=0.377, v_num=0000]\n",
      "Epoch 0:  70%|███████   | 1168/1660 [02:20<00:59,  8.33it/s, loss=0.378, v_num=0000]\n",
      "Epoch 0:  70%|███████   | 1169/1660 [02:20<00:58,  8.33it/s, loss=0.367, v_num=0000]\n",
      "Epoch 0:  70%|███████   | 1170/1660 [02:20<00:58,  8.33it/s, loss=0.37, v_num=0000] \n",
      "Epoch 0:  71%|███████   | 1171/1660 [02:20<00:58,  8.33it/s, loss=0.366, v_num=0000]\n",
      "Epoch 0:  71%|███████   | 1172/1660 [02:20<00:58,  8.33it/s, loss=0.363, v_num=0000]\n",
      "Epoch 0:  71%|███████   | 1173/1660 [02:20<00:58,  8.33it/s, loss=0.356, v_num=0000]\n",
      "Epoch 0:  71%|███████   | 1174/1660 [02:20<00:58,  8.33it/s, loss=0.361, v_num=0000]\n",
      "Epoch 0:  71%|███████   | 1175/1660 [02:21<00:58,  8.33it/s, loss=0.367, v_num=0000]\n",
      "== Status ==\n",
      "Current time: 2022-06-20 12:02:42 (running for 00:02:35.60)\n",
      "Memory usage on this node: 30.1/376.5 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1.0/8 CPUs, 1.0/1 GPUs, 0.0/34.4 GiB heap, 0.0/17.2 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/dobby/ray_results/tune_rp\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+----------------------+----------+------------------+-----------------+----------------+\n",
      "| Trial name           | status   | loc              |   learning_rate |   weight_decay |\n",
      "|----------------------+----------+------------------+-----------------+----------------|\n",
      "| train_rp_86c32_00000 | RUNNING  | 172.17.0.5:17844 |     0.000368944 |            0.1 |\n",
      "+----------------------+----------+------------------+-----------------+----------------+\n",
      "\n",
      "\n",
      "Epoch 0:  71%|███████   | 1176/1660 [02:21<00:58,  8.33it/s, loss=0.367, v_num=0000]\n",
      "Epoch 0:  71%|███████   | 1177/1660 [02:21<00:57,  8.33it/s, loss=0.37, v_num=0000] \n",
      "Epoch 0:  71%|███████   | 1178/1660 [02:21<00:57,  8.33it/s, loss=0.367, v_num=0000]\n",
      "Epoch 0:  71%|███████   | 1179/1660 [02:21<00:57,  8.33it/s, loss=0.366, v_num=0000]\n",
      "Epoch 0:  71%|███████   | 1180/1660 [02:21<00:57,  8.33it/s, loss=0.367, v_num=0000]\n",
      "Epoch 0:  71%|███████   | 1181/1660 [02:21<00:57,  8.33it/s, loss=0.377, v_num=0000]\n",
      "Epoch 0:  71%|███████   | 1182/1660 [02:21<00:57,  8.33it/s, loss=0.376, v_num=0000]\n",
      "Epoch 0:  71%|███████▏  | 1183/1660 [02:21<00:57,  8.33it/s, loss=0.382, v_num=0000]\n",
      "Epoch 0:  71%|███████▏  | 1184/1660 [02:22<00:57,  8.33it/s, loss=0.376, v_num=0000]\n",
      "Epoch 0:  71%|███████▏  | 1185/1660 [02:22<00:57,  8.33it/s, loss=0.372, v_num=0000]\n",
      "Epoch 0:  71%|███████▏  | 1186/1660 [02:22<00:56,  8.33it/s, loss=0.376, v_num=0000]\n",
      "Epoch 0:  72%|███████▏  | 1187/1660 [02:22<00:56,  8.33it/s, loss=0.365, v_num=0000]\n",
      "Epoch 0:  72%|███████▏  | 1188/1660 [02:22<00:56,  8.33it/s, loss=0.36, v_num=0000] \n",
      "Epoch 0:  72%|███████▏  | 1189/1660 [02:22<00:56,  8.33it/s, loss=0.367, v_num=0000]\n",
      "Epoch 0:  72%|███████▏  | 1190/1660 [02:22<00:56,  8.33it/s, loss=0.371, v_num=0000]\n",
      "Epoch 0:  72%|███████▏  | 1191/1660 [02:22<00:56,  8.33it/s, loss=0.375, v_num=0000]\n",
      "Epoch 0:  72%|███████▏  | 1192/1660 [02:23<00:56,  8.33it/s, loss=0.373, v_num=0000]\n",
      "Epoch 0:  72%|███████▏  | 1193/1660 [02:23<00:56,  8.33it/s, loss=0.375, v_num=0000]\n",
      "Epoch 0:  72%|███████▏  | 1194/1660 [02:23<00:55,  8.33it/s, loss=0.365, v_num=0000]\n",
      "Epoch 0:  72%|███████▏  | 1195/1660 [02:23<00:55,  8.33it/s, loss=0.368, v_num=0000]\n",
      "Epoch 0:  72%|███████▏  | 1196/1660 [02:23<00:55,  8.33it/s, loss=0.372, v_num=0000]\n",
      "Epoch 0:  72%|███████▏  | 1197/1660 [02:23<00:55,  8.33it/s, loss=0.365, v_num=0000]\n",
      "Epoch 0:  72%|███████▏  | 1198/1660 [02:23<00:55,  8.33it/s, loss=0.362, v_num=0000]\n",
      "Epoch 0:  72%|███████▏  | 1199/1660 [02:23<00:55,  8.33it/s, loss=0.368, v_num=0000]\n",
      "Epoch 0:  72%|███████▏  | 1200/1660 [02:24<00:55,  8.33it/s, loss=0.364, v_num=0000]\n",
      "Epoch 0:  72%|███████▏  | 1201/1660 [02:24<00:55,  8.33it/s, loss=0.35, v_num=0000] \n",
      "Epoch 0:  72%|███████▏  | 1202/1660 [02:24<00:54,  8.33it/s, loss=0.354, v_num=0000]\n",
      "Epoch 0:  72%|███████▏  | 1203/1660 [02:24<00:54,  8.33it/s, loss=0.339, v_num=0000]\n",
      "Epoch 0:  73%|███████▎  | 1204/1660 [02:24<00:54,  8.33it/s, loss=0.352, v_num=0000]\n",
      "Epoch 0:  73%|███████▎  | 1205/1660 [02:24<00:54,  8.33it/s, loss=0.352, v_num=0000]\n",
      "Epoch 0:  73%|███████▎  | 1205/1660 [02:24<00:54,  8.33it/s, loss=0.349, v_num=0000]\n",
      "Epoch 0:  73%|███████▎  | 1206/1660 [02:24<00:54,  8.33it/s, loss=0.351, v_num=0000]\n",
      "Epoch 0:  73%|███████▎  | 1207/1660 [02:24<00:54,  8.33it/s, loss=0.353, v_num=0000]\n",
      "Epoch 0:  73%|███████▎  | 1208/1660 [02:24<00:54,  8.33it/s, loss=0.357, v_num=0000]\n",
      "Epoch 0:  73%|███████▎  | 1209/1660 [02:25<00:54,  8.33it/s, loss=0.355, v_num=0000]\n",
      "Epoch 0:  73%|███████▎  | 1210/1660 [02:25<00:53,  8.33it/s, loss=0.357, v_num=0000]\n",
      "Epoch 0:  73%|███████▎  | 1211/1660 [02:25<00:53,  8.33it/s, loss=0.352, v_num=0000]\n",
      "Epoch 0:  73%|███████▎  | 1212/1660 [02:25<00:53,  8.33it/s, loss=0.352, v_num=0000]\n",
      "Epoch 0:  73%|███████▎  | 1212/1660 [02:25<00:53,  8.33it/s, loss=0.347, v_num=0000]\n",
      "Epoch 0:  73%|███████▎  | 1213/1660 [02:25<00:53,  8.34it/s, loss=0.351, v_num=0000]\n",
      "Epoch 0:  73%|███████▎  | 1214/1660 [02:25<00:53,  8.34it/s, loss=0.349, v_num=0000]\n",
      "Epoch 0:  73%|███████▎  | 1215/1660 [02:25<00:53,  8.34it/s, loss=0.34, v_num=0000] \n",
      "Epoch 0:  73%|███████▎  | 1216/1660 [02:25<00:53,  8.33it/s, loss=0.334, v_num=0000]\n",
      "== Status ==\n",
      "Current time: 2022-06-20 12:02:47 (running for 00:02:40.60)\n",
      "Memory usage on this node: 30.1/376.5 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1.0/8 CPUs, 1.0/1 GPUs, 0.0/34.4 GiB heap, 0.0/17.2 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/dobby/ray_results/tune_rp\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+----------------------+----------+------------------+-----------------+----------------+\n",
      "| Trial name           | status   | loc              |   learning_rate |   weight_decay |\n",
      "|----------------------+----------+------------------+-----------------+----------------|\n",
      "| train_rp_86c32_00000 | RUNNING  | 172.17.0.5:17844 |     0.000368944 |            0.1 |\n",
      "+----------------------+----------+------------------+-----------------+----------------+\n",
      "\n",
      "\n",
      "Epoch 0:  73%|███████▎  | 1217/1660 [02:26<00:53,  8.33it/s, loss=0.335, v_num=0000]\n",
      "Epoch 0:  73%|███████▎  | 1218/1660 [02:26<00:53,  8.33it/s, loss=0.34, v_num=0000] \n",
      "Epoch 0:  73%|███████▎  | 1219/1660 [02:26<00:52,  8.33it/s, loss=0.332, v_num=0000]\n",
      "Epoch 0:  73%|███████▎  | 1220/1660 [02:26<00:52,  8.33it/s, loss=0.33, v_num=0000] \n",
      "Epoch 0:  74%|███████▎  | 1221/1660 [02:26<00:52,  8.33it/s, loss=0.322, v_num=0000]\n",
      "Epoch 0:  74%|███████▎  | 1222/1660 [02:26<00:52,  8.33it/s, loss=0.315, v_num=0000]\n",
      "Epoch 0:  74%|███████▎  | 1223/1660 [02:26<00:52,  8.33it/s, loss=0.314, v_num=0000]\n",
      "Epoch 0:  74%|███████▎  | 1224/1660 [02:26<00:52,  8.33it/s, loss=0.307, v_num=0000]\n",
      "Epoch 0:  74%|███████▍  | 1225/1660 [02:26<00:52,  8.34it/s, loss=0.305, v_num=0000]\n",
      "Epoch 0:  74%|███████▍  | 1226/1660 [02:27<00:52,  8.34it/s, loss=0.3, v_num=0000]  \n",
      "Epoch 0:  74%|███████▍  | 1227/1660 [02:27<00:51,  8.34it/s, loss=0.3, v_num=0000]\n",
      "Epoch 0:  74%|███████▍  | 1228/1660 [02:27<00:51,  8.34it/s, loss=0.297, v_num=0000]\n",
      "Epoch 0:  74%|███████▍  | 1229/1660 [02:27<00:51,  8.34it/s, loss=0.307, v_num=0000]\n",
      "Epoch 0:  74%|███████▍  | 1230/1660 [02:27<00:51,  8.34it/s, loss=0.298, v_num=0000]\n",
      "Epoch 0:  74%|███████▍  | 1231/1660 [02:27<00:51,  8.34it/s, loss=0.299, v_num=0000]\n",
      "Epoch 0:  74%|███████▍  | 1232/1660 [02:27<00:51,  8.33it/s, loss=0.313, v_num=0000]\n",
      "Epoch 0:  74%|███████▍  | 1233/1660 [02:27<00:51,  8.33it/s, loss=0.306, v_num=0000]\n",
      "Epoch 0:  74%|███████▍  | 1234/1660 [02:28<00:51,  8.33it/s, loss=0.312, v_num=0000]\n",
      "Epoch 0:  74%|███████▍  | 1235/1660 [02:28<00:50,  8.33it/s, loss=0.316, v_num=0000]\n",
      "Epoch 0:  74%|███████▍  | 1236/1660 [02:28<00:50,  8.34it/s, loss=0.314, v_num=0000]\n",
      "Epoch 0:  75%|███████▍  | 1237/1660 [02:28<00:50,  8.34it/s, loss=0.316, v_num=0000]\n",
      "Epoch 0:  75%|███████▍  | 1238/1660 [02:28<00:50,  8.34it/s, loss=0.309, v_num=0000]\n",
      "Epoch 0:  75%|███████▍  | 1239/1660 [02:28<00:50,  8.34it/s, loss=0.318, v_num=0000]\n",
      "Epoch 0:  75%|███████▍  | 1240/1660 [02:28<00:50,  8.34it/s, loss=0.321, v_num=0000]\n",
      "Epoch 0:  75%|███████▍  | 1241/1660 [02:28<00:50,  8.34it/s, loss=0.328, v_num=0000]\n",
      "Epoch 0:  75%|███████▍  | 1242/1660 [02:28<00:50,  8.34it/s, loss=0.338, v_num=0000]\n",
      "Epoch 0:  75%|███████▍  | 1243/1660 [02:29<00:50,  8.34it/s, loss=0.339, v_num=0000]\n",
      "Epoch 0:  75%|███████▍  | 1244/1660 [02:29<00:49,  8.34it/s, loss=0.339, v_num=0000]\n",
      "Epoch 0:  75%|███████▌  | 1245/1660 [02:29<00:49,  8.34it/s, loss=0.349, v_num=0000]\n",
      "Epoch 0:  75%|███████▌  | 1246/1660 [02:29<00:49,  8.34it/s, loss=0.357, v_num=0000]\n",
      "Epoch 0:  75%|███████▌  | 1247/1660 [02:29<00:49,  8.34it/s, loss=0.35, v_num=0000] \n",
      "Epoch 0:  75%|███████▌  | 1248/1660 [02:29<00:49,  8.34it/s, loss=0.349, v_num=0000]\n",
      "Epoch 0:  75%|███████▌  | 1249/1660 [02:29<00:49,  8.34it/s, loss=0.345, v_num=0000]\n",
      "Epoch 0:  75%|███████▌  | 1250/1660 [02:29<00:49,  8.34it/s, loss=0.351, v_num=0000]\n",
      "Epoch 0:  75%|███████▌  | 1251/1660 [02:30<00:49,  8.34it/s, loss=0.359, v_num=0000]\n",
      "Epoch 0:  75%|███████▌  | 1252/1660 [02:30<00:48,  8.34it/s, loss=0.348, v_num=0000]\n",
      "Epoch 0:  75%|███████▌  | 1253/1660 [02:30<00:48,  8.34it/s, loss=0.349, v_num=0000]\n",
      "Epoch 0:  76%|███████▌  | 1254/1660 [02:30<00:48,  8.34it/s, loss=0.345, v_num=0000]\n",
      "Epoch 0:  76%|███████▌  | 1255/1660 [02:30<00:48,  8.34it/s, loss=0.346, v_num=0000]\n",
      "Epoch 0:  76%|███████▌  | 1256/1660 [02:30<00:48,  8.34it/s, loss=0.36, v_num=0000] \n",
      "Epoch 0:  76%|███████▌  | 1257/1660 [02:30<00:48,  8.34it/s, loss=0.355, v_num=0000]\n",
      "Epoch 0:  76%|███████▌  | 1258/1660 [02:30<00:48,  8.34it/s, loss=0.362, v_num=0000]\n",
      "Epoch 0:  76%|███████▌  | 1259/1660 [02:31<00:48,  8.34it/s, loss=0.36, v_num=0000] \n",
      "== Status ==\n",
      "Current time: 2022-06-20 12:02:52 (running for 00:02:45.60)\n",
      "Memory usage on this node: 29.7/376.5 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1.0/8 CPUs, 1.0/1 GPUs, 0.0/34.4 GiB heap, 0.0/17.2 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/dobby/ray_results/tune_rp\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+----------------------+----------+------------------+-----------------+----------------+\n",
      "| Trial name           | status   | loc              |   learning_rate |   weight_decay |\n",
      "|----------------------+----------+------------------+-----------------+----------------|\n",
      "| train_rp_86c32_00000 | RUNNING  | 172.17.0.5:17844 |     0.000368944 |            0.1 |\n",
      "+----------------------+----------+------------------+-----------------+----------------+\n",
      "\n",
      "\n",
      "Epoch 0:  76%|███████▌  | 1260/1660 [02:31<00:47,  8.34it/s, loss=0.36, v_num=0000]\n",
      "Epoch 0:  76%|███████▌  | 1261/1660 [02:31<00:47,  8.34it/s, loss=0.368, v_num=0000]\n",
      "Epoch 0:  76%|███████▌  | 1262/1660 [02:31<00:47,  8.34it/s, loss=0.361, v_num=0000]\n",
      "Epoch 0:  76%|███████▌  | 1263/1660 [02:31<00:47,  8.34it/s, loss=0.365, v_num=0000]\n",
      "Epoch 0:  76%|███████▌  | 1264/1660 [02:31<00:47,  8.34it/s, loss=0.365, v_num=0000]\n",
      "Epoch 0:  76%|███████▌  | 1265/1660 [02:31<00:47,  8.34it/s, loss=0.363, v_num=0000]\n",
      "Epoch 0:  76%|███████▋  | 1266/1660 [02:31<00:47,  8.34it/s, loss=0.356, v_num=0000]\n",
      "Epoch 0:  76%|███████▋  | 1267/1660 [02:31<00:47,  8.34it/s, loss=0.371, v_num=0000]\n",
      "Epoch 0:  76%|███████▋  | 1268/1660 [02:32<00:47,  8.34it/s, loss=0.377, v_num=0000]\n",
      "Epoch 0:  76%|███████▋  | 1269/1660 [02:32<00:46,  8.34it/s, loss=0.368, v_num=0000]\n",
      "Epoch 0:  77%|███████▋  | 1270/1660 [02:32<00:46,  8.34it/s, loss=0.362, v_num=0000]\n",
      "Epoch 0:  77%|███████▋  | 1271/1660 [02:32<00:46,  8.34it/s, loss=0.35, v_num=0000] \n",
      "Epoch 0:  77%|███████▋  | 1272/1660 [02:32<00:46,  8.34it/s, loss=0.354, v_num=0000]\n",
      "Epoch 0:  77%|███████▋  | 1273/1660 [02:32<00:46,  8.34it/s, loss=0.359, v_num=0000]\n",
      "Epoch 0:  77%|███████▋  | 1274/1660 [02:32<00:46,  8.34it/s, loss=0.357, v_num=0000]\n",
      "Epoch 0:  77%|███████▋  | 1275/1660 [02:32<00:46,  8.34it/s, loss=0.36, v_num=0000] \n",
      "Epoch 0:  77%|███████▋  | 1276/1660 [02:33<00:46,  8.34it/s, loss=0.35, v_num=0000]\n",
      "Epoch 0:  77%|███████▋  | 1277/1660 [02:33<00:45,  8.34it/s, loss=0.353, v_num=0000]\n",
      "Epoch 0:  77%|███████▋  | 1278/1660 [02:33<00:45,  8.34it/s, loss=0.355, v_num=0000]\n",
      "Epoch 0:  77%|███████▋  | 1279/1660 [02:33<00:45,  8.34it/s, loss=0.352, v_num=0000]\n",
      "Epoch 0:  77%|███████▋  | 1280/1660 [02:33<00:45,  8.34it/s, loss=0.345, v_num=0000]\n",
      "Epoch 0:  77%|███████▋  | 1281/1660 [02:33<00:45,  8.34it/s, loss=0.332, v_num=0000]\n",
      "Epoch 0:  77%|███████▋  | 1282/1660 [02:33<00:45,  8.34it/s, loss=0.342, v_num=0000]\n",
      "Epoch 0:  77%|███████▋  | 1283/1660 [02:33<00:45,  8.34it/s, loss=0.354, v_num=0000]\n",
      "Epoch 0:  77%|███████▋  | 1284/1660 [02:34<00:45,  8.34it/s, loss=0.364, v_num=0000]\n",
      "Epoch 0:  77%|███████▋  | 1285/1660 [02:34<00:44,  8.34it/s, loss=0.361, v_num=0000]\n",
      "Epoch 0:  77%|███████▋  | 1286/1660 [02:34<00:44,  8.34it/s, loss=0.367, v_num=0000]\n",
      "Epoch 0:  78%|███████▊  | 1287/1660 [02:34<00:44,  8.34it/s, loss=0.358, v_num=0000]\n",
      "Epoch 0:  78%|███████▊  | 1288/1660 [02:34<00:44,  8.34it/s, loss=0.355, v_num=0000]\n",
      "Epoch 0:  78%|███████▊  | 1289/1660 [02:34<00:44,  8.34it/s, loss=0.358, v_num=0000]\n",
      "Epoch 0:  78%|███████▊  | 1290/1660 [02:34<00:44,  8.34it/s, loss=0.366, v_num=0000]\n",
      "Epoch 0:  78%|███████▊  | 1291/1660 [02:34<00:44,  8.34it/s, loss=0.363, v_num=0000]\n",
      "Epoch 0:  78%|███████▊  | 1292/1660 [02:34<00:44,  8.34it/s, loss=0.365, v_num=0000]\n",
      "Epoch 0:  78%|███████▊  | 1293/1660 [02:35<00:44,  8.34it/s, loss=0.359, v_num=0000]\n",
      "Epoch 0:  78%|███████▊  | 1294/1660 [02:35<00:43,  8.34it/s, loss=0.354, v_num=0000]\n",
      "Epoch 0:  78%|███████▊  | 1295/1660 [02:35<00:43,  8.34it/s, loss=0.343, v_num=0000]\n",
      "Epoch 0:  78%|███████▊  | 1296/1660 [02:35<00:43,  8.34it/s, loss=0.339, v_num=0000]\n",
      "Epoch 0:  78%|███████▊  | 1297/1660 [02:35<00:43,  8.34it/s, loss=0.334, v_num=0000]\n",
      "Epoch 0:  78%|███████▊  | 1298/1660 [02:35<00:43,  8.34it/s, loss=0.324, v_num=0000]\n",
      "Epoch 0:  78%|███████▊  | 1299/1660 [02:35<00:43,  8.34it/s, loss=0.332, v_num=0000]\n",
      "Epoch 0:  78%|███████▊  | 1300/1660 [02:35<00:43,  8.34it/s, loss=0.343, v_num=0000]\n",
      "== Status ==\n",
      "Current time: 2022-06-20 12:02:57 (running for 00:02:50.60)\n",
      "Memory usage on this node: 29.7/376.5 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1.0/8 CPUs, 1.0/1 GPUs, 0.0/34.4 GiB heap, 0.0/17.2 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/dobby/ray_results/tune_rp\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+----------------------+----------+------------------+-----------------+----------------+\n",
      "| Trial name           | status   | loc              |   learning_rate |   weight_decay |\n",
      "|----------------------+----------+------------------+-----------------+----------------|\n",
      "| train_rp_86c32_00000 | RUNNING  | 172.17.0.5:17844 |     0.000368944 |            0.1 |\n",
      "+----------------------+----------+------------------+-----------------+----------------+\n",
      "\n",
      "\n",
      "Epoch 0:  78%|███████▊  | 1301/1660 [02:36<00:43,  8.34it/s, loss=0.35, v_num=0000] \n",
      "Epoch 0:  78%|███████▊  | 1302/1660 [02:36<00:42,  8.34it/s, loss=0.34, v_num=0000]\n",
      "Epoch 0:  78%|███████▊  | 1303/1660 [02:36<00:42,  8.34it/s, loss=0.331, v_num=0000]\n",
      "Epoch 0:  79%|███████▊  | 1304/1660 [02:36<00:42,  8.34it/s, loss=0.327, v_num=0000]\n",
      "Epoch 0:  79%|███████▊  | 1305/1660 [02:36<00:42,  8.34it/s, loss=0.322, v_num=0000]\n",
      "Epoch 0:  79%|███████▊  | 1306/1660 [02:36<00:42,  8.34it/s, loss=0.326, v_num=0000]\n",
      "Epoch 0:  79%|███████▊  | 1307/1660 [02:36<00:42,  8.34it/s, loss=0.33, v_num=0000] \n",
      "Epoch 0:  79%|███████▉  | 1308/1660 [02:36<00:42,  8.34it/s, loss=0.326, v_num=0000]\n",
      "Epoch 0:  79%|███████▉  | 1309/1660 [02:36<00:42,  8.34it/s, loss=0.336, v_num=0000]\n",
      "Epoch 0:  79%|███████▉  | 1310/1660 [02:37<00:41,  8.34it/s, loss=0.33, v_num=0000] \n",
      "Epoch 0:  79%|███████▉  | 1311/1660 [02:37<00:41,  8.34it/s, loss=0.334, v_num=0000]\n",
      "Epoch 0:  79%|███████▉  | 1312/1660 [02:37<00:41,  8.34it/s, loss=0.343, v_num=0000]\n",
      "Epoch 0:  79%|███████▉  | 1313/1660 [02:37<00:41,  8.34it/s, loss=0.342, v_num=0000]\n",
      "Epoch 0:  79%|███████▉  | 1314/1660 [02:37<00:41,  8.34it/s, loss=0.35, v_num=0000] \n",
      "Epoch 0:  79%|███████▉  | 1315/1660 [02:37<00:41,  8.34it/s, loss=0.36, v_num=0000]\n",
      "Epoch 0:  79%|███████▉  | 1316/1660 [02:37<00:41,  8.34it/s, loss=0.362, v_num=0000]\n",
      "Epoch 0:  79%|███████▉  | 1317/1660 [02:37<00:41,  8.34it/s, loss=0.366, v_num=0000]\n",
      "Epoch 0:  79%|███████▉  | 1318/1660 [02:38<00:41,  8.34it/s, loss=0.369, v_num=0000]\n",
      "Epoch 0:  79%|███████▉  | 1319/1660 [02:38<00:40,  8.34it/s, loss=0.36, v_num=0000] \n",
      "Epoch 0:  80%|███████▉  | 1320/1660 [02:38<00:40,  8.34it/s, loss=0.354, v_num=0000]\n",
      "Epoch 0:  80%|███████▉  | 1321/1660 [02:38<00:40,  8.34it/s, loss=0.353, v_num=0000]\n",
      "Epoch 0:  80%|███████▉  | 1322/1660 [02:38<00:40,  8.34it/s, loss=0.357, v_num=0000]\n",
      "Epoch 0:  80%|███████▉  | 1323/1660 [02:38<00:40,  8.34it/s, loss=0.352, v_num=0000]\n",
      "Epoch 0:  80%|███████▉  | 1324/1660 [02:38<00:40,  8.34it/s, loss=0.343, v_num=0000]\n",
      "Epoch 0:  80%|███████▉  | 1325/1660 [02:38<00:40,  8.34it/s, loss=0.346, v_num=0000]\n",
      "Epoch 0:  80%|███████▉  | 1326/1660 [02:39<00:40,  8.34it/s, loss=0.338, v_num=0000]\n",
      "Epoch 0:  80%|███████▉  | 1327/1660 [02:39<00:39,  8.34it/s, loss=0.34, v_num=0000] \n",
      "Epoch 0:  80%|████████  | 1328/1660 [02:39<00:39,  8.34it/s, loss=0.347, v_num=0000]\n",
      "Epoch 0:  80%|████████  | 1329/1660 [02:39<00:39,  8.34it/s, loss=0.339, v_num=0000]\n",
      "Epoch 0:  80%|████████  | 1330/1660 [02:39<00:39,  8.34it/s, loss=0.339, v_num=0000]\n",
      "Epoch 0:  80%|████████  | 1331/1660 [02:39<00:39,  8.34it/s, loss=0.337, v_num=0000]\n",
      "Epoch 0:  80%|████████  | 1332/1660 [02:39<00:39,  8.34it/s, loss=0.321, v_num=0000]\n",
      "Epoch 0:  80%|████████  | 1333/1660 [02:39<00:39,  8.34it/s, loss=0.326, v_num=0000]\n",
      "Epoch 0:  80%|████████  | 1334/1660 [02:39<00:39,  8.34it/s, loss=0.336, v_num=0000]\n",
      "Epoch 0:  80%|████████  | 1335/1660 [02:40<00:38,  8.34it/s, loss=0.334, v_num=0000]\n",
      "Epoch 0:  80%|████████  | 1336/1660 [02:40<00:38,  8.34it/s, loss=0.345, v_num=0000]\n",
      "Epoch 0:  81%|████████  | 1337/1660 [02:40<00:38,  8.34it/s, loss=0.353, v_num=0000]\n",
      "Epoch 0:  81%|████████  | 1338/1660 [02:40<00:38,  8.34it/s, loss=0.359, v_num=0000]\n",
      "Epoch 0:  81%|████████  | 1339/1660 [02:40<00:38,  8.34it/s, loss=0.367, v_num=0000]\n",
      "Epoch 0:  81%|████████  | 1340/1660 [02:40<00:38,  8.34it/s, loss=0.38, v_num=0000] \n",
      "Epoch 0:  81%|████████  | 1341/1660 [02:40<00:38,  8.34it/s, loss=0.376, v_num=0000]\n",
      "Epoch 0:  81%|████████  | 1342/1660 [02:40<00:38,  8.34it/s, loss=0.375, v_num=0000]\n",
      "Epoch 0:  81%|████████  | 1343/1660 [02:41<00:38,  8.34it/s, loss=0.382, v_num=0000]\n",
      "== Status ==\n",
      "Current time: 2022-06-20 12:03:02 (running for 00:02:55.61)\n",
      "Memory usage on this node: 29.7/376.5 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1.0/8 CPUs, 1.0/1 GPUs, 0.0/34.4 GiB heap, 0.0/17.2 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/dobby/ray_results/tune_rp\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+----------------------+----------+------------------+-----------------+----------------+\n",
      "| Trial name           | status   | loc              |   learning_rate |   weight_decay |\n",
      "|----------------------+----------+------------------+-----------------+----------------|\n",
      "| train_rp_86c32_00000 | RUNNING  | 172.17.0.5:17844 |     0.000368944 |            0.1 |\n",
      "+----------------------+----------+------------------+-----------------+----------------+\n",
      "\n",
      "\n",
      "Epoch 0:  81%|████████  | 1344/1660 [02:41<00:37,  8.34it/s, loss=0.383, v_num=0000]\n",
      "Epoch 0:  81%|████████  | 1345/1660 [02:41<00:37,  8.34it/s, loss=0.38, v_num=0000] \n",
      "Epoch 0:  81%|████████  | 1346/1660 [02:41<00:37,  8.34it/s, loss=0.38, v_num=0000]\n",
      "Epoch 0:  81%|████████  | 1347/1660 [02:41<00:37,  8.34it/s, loss=0.376, v_num=0000]\n",
      "Epoch 0:  81%|████████  | 1348/1660 [02:41<00:37,  8.34it/s, loss=0.376, v_num=0000]\n",
      "Epoch 0:  81%|████████  | 1348/1660 [02:41<00:37,  8.34it/s, loss=0.376, v_num=0000]\n",
      "Epoch 0:  81%|████████▏ | 1349/1660 [02:41<00:37,  8.34it/s, loss=0.372, v_num=0000]\n",
      "Epoch 0:  81%|████████▏ | 1350/1660 [02:41<00:37,  8.34it/s, loss=0.371, v_num=0000]\n",
      "Epoch 0:  81%|████████▏ | 1351/1660 [02:42<00:37,  8.34it/s, loss=0.384, v_num=0000]\n",
      "Epoch 0:  81%|████████▏ | 1352/1660 [02:42<00:36,  8.34it/s, loss=0.385, v_num=0000]\n",
      "Epoch 0:  82%|████████▏ | 1353/1660 [02:42<00:36,  8.34it/s, loss=0.378, v_num=0000]\n",
      "Epoch 0:  82%|████████▏ | 1354/1660 [02:42<00:36,  8.34it/s, loss=0.371, v_num=0000]\n",
      "Epoch 0:  82%|████████▏ | 1355/1660 [02:42<00:36,  8.34it/s, loss=0.371, v_num=0000]\n",
      "Epoch 0:  82%|████████▏ | 1355/1660 [02:42<00:36,  8.34it/s, loss=0.368, v_num=0000]\n",
      "Epoch 0:  82%|████████▏ | 1356/1660 [02:42<00:36,  8.34it/s, loss=0.364, v_num=0000]\n",
      "Epoch 0:  82%|████████▏ | 1357/1660 [02:42<00:36,  8.34it/s, loss=0.357, v_num=0000]\n",
      "Epoch 0:  82%|████████▏ | 1358/1660 [02:42<00:36,  8.34it/s, loss=0.355, v_num=0000]\n",
      "Epoch 0:  82%|████████▏ | 1359/1660 [02:42<00:36,  8.34it/s, loss=0.35, v_num=0000] \n",
      "Epoch 0:  82%|████████▏ | 1360/1660 [02:43<00:35,  8.34it/s, loss=0.337, v_num=0000]\n",
      "Epoch 0:  82%|████████▏ | 1361/1660 [02:43<00:35,  8.34it/s, loss=0.342, v_num=0000]\n",
      "Epoch 0:  82%|████████▏ | 1362/1660 [02:43<00:35,  8.34it/s, loss=0.339, v_num=0000]\n",
      "Epoch 0:  82%|████████▏ | 1363/1660 [02:43<00:35,  8.34it/s, loss=0.336, v_num=0000]\n",
      "Epoch 0:  82%|████████▏ | 1364/1660 [02:43<00:35,  8.34it/s, loss=0.334, v_num=0000]\n",
      "Epoch 0:  82%|████████▏ | 1365/1660 [02:43<00:35,  8.34it/s, loss=0.34, v_num=0000] \n",
      "Epoch 0:  82%|████████▏ | 1366/1660 [02:43<00:35,  8.34it/s, loss=0.34, v_num=0000]\n",
      "Epoch 0:  82%|████████▏ | 1367/1660 [02:43<00:35,  8.34it/s, loss=0.343, v_num=0000]\n",
      "Epoch 0:  82%|████████▏ | 1368/1660 [02:44<00:35,  8.34it/s, loss=0.34, v_num=0000] \n",
      "Epoch 0:  82%|████████▏ | 1369/1660 [02:44<00:34,  8.34it/s, loss=0.346, v_num=0000]\n",
      "Epoch 0:  83%|████████▎ | 1370/1660 [02:44<00:34,  8.34it/s, loss=0.349, v_num=0000]\n",
      "Epoch 0:  83%|████████▎ | 1371/1660 [02:44<00:34,  8.34it/s, loss=0.341, v_num=0000]\n",
      "Epoch 0:  83%|████████▎ | 1372/1660 [02:44<00:34,  8.34it/s, loss=0.347, v_num=0000]\n",
      "Epoch 0:  83%|████████▎ | 1373/1660 [02:44<00:34,  8.34it/s, loss=0.355, v_num=0000]\n",
      "Epoch 0:  83%|████████▎ | 1374/1660 [02:44<00:34,  8.34it/s, loss=0.351, v_num=0000]\n",
      "Epoch 0:  83%|████████▎ | 1375/1660 [02:44<00:34,  8.34it/s, loss=0.352, v_num=0000]\n",
      "Epoch 0:  83%|████████▎ | 1376/1660 [02:45<00:34,  8.34it/s, loss=0.345, v_num=0000]\n",
      "Epoch 0:  83%|████████▎ | 1377/1660 [02:45<00:33,  8.34it/s, loss=0.344, v_num=0000]\n",
      "Epoch 0:  83%|████████▎ | 1378/1660 [02:45<00:33,  8.34it/s, loss=0.338, v_num=0000]\n",
      "Epoch 0:  83%|████████▎ | 1379/1660 [02:45<00:33,  8.34it/s, loss=0.338, v_num=0000]\n",
      "Epoch 0:  83%|████████▎ | 1380/1660 [02:45<00:33,  8.34it/s, loss=0.338, v_num=0000]\n",
      "Epoch 0:  83%|████████▎ | 1381/1660 [02:45<00:33,  8.34it/s, loss=0.335, v_num=0000]\n",
      "Epoch 0:  83%|████████▎ | 1382/1660 [02:45<00:33,  8.34it/s, loss=0.335, v_num=0000]\n",
      "Epoch 0:  83%|████████▎ | 1383/1660 [02:45<00:33,  8.34it/s, loss=0.33, v_num=0000] \n",
      "Epoch 0:  83%|████████▎ | 1384/1660 [02:45<00:33,  8.34it/s, loss=0.333, v_num=0000]\n",
      "== Status ==\n",
      "Current time: 2022-06-20 12:03:07 (running for 00:03:00.61)\n",
      "Memory usage on this node: 29.7/376.5 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1.0/8 CPUs, 1.0/1 GPUs, 0.0/34.4 GiB heap, 0.0/17.2 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/dobby/ray_results/tune_rp\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+----------------------+----------+------------------+-----------------+----------------+\n",
      "| Trial name           | status   | loc              |   learning_rate |   weight_decay |\n",
      "|----------------------+----------+------------------+-----------------+----------------|\n",
      "| train_rp_86c32_00000 | RUNNING  | 172.17.0.5:17844 |     0.000368944 |            0.1 |\n",
      "+----------------------+----------+------------------+-----------------+----------------+\n",
      "\n",
      "\n",
      "Epoch 0:  83%|████████▎ | 1385/1660 [02:46<00:32,  8.34it/s, loss=0.327, v_num=0000]\n",
      "Epoch 0:  83%|████████▎ | 1386/1660 [02:46<00:32,  8.34it/s, loss=0.33, v_num=0000] \n",
      "Epoch 0:  84%|████████▎ | 1387/1660 [02:46<00:32,  8.34it/s, loss=0.333, v_num=0000]\n",
      "Epoch 0:  84%|████████▎ | 1388/1660 [02:46<00:32,  8.34it/s, loss=0.334, v_num=0000]\n",
      "Epoch 0:  84%|████████▎ | 1389/1660 [02:46<00:32,  8.34it/s, loss=0.336, v_num=0000]\n",
      "Epoch 0:  84%|████████▎ | 1390/1660 [02:46<00:32,  8.34it/s, loss=0.334, v_num=0000]\n",
      "Epoch 0:  84%|████████▍ | 1391/1660 [02:46<00:32,  8.34it/s, loss=0.337, v_num=0000]\n",
      "Epoch 0:  84%|████████▍ | 1392/1660 [02:46<00:32,  8.34it/s, loss=0.329, v_num=0000]\n",
      "Epoch 0:  84%|████████▍ | 1393/1660 [02:47<00:32,  8.34it/s, loss=0.337, v_num=0000]\n",
      "Epoch 0:  84%|████████▍ | 1394/1660 [02:47<00:31,  8.34it/s, loss=0.34, v_num=0000] \n",
      "Epoch 0:  84%|████████▍ | 1395/1660 [02:47<00:31,  8.34it/s, loss=0.345, v_num=0000]\n",
      "Epoch 0:  84%|████████▍ | 1396/1660 [02:47<00:31,  8.34it/s, loss=0.343, v_num=0000]\n",
      "Epoch 0:  84%|████████▍ | 1397/1660 [02:47<00:31,  8.34it/s, loss=0.345, v_num=0000]\n",
      "Epoch 0:  84%|████████▍ | 1398/1660 [02:47<00:31,  8.34it/s, loss=0.35, v_num=0000] \n",
      "Epoch 0:  84%|████████▍ | 1399/1660 [02:47<00:31,  8.34it/s, loss=0.349, v_num=0000]\n",
      "Epoch 0:  84%|████████▍ | 1400/1660 [02:47<00:31,  8.34it/s, loss=0.345, v_num=0000]\n",
      "Epoch 0:  84%|████████▍ | 1401/1660 [02:47<00:31,  8.34it/s, loss=0.355, v_num=0000]\n",
      "Epoch 0:  84%|████████▍ | 1402/1660 [02:48<00:30,  8.34it/s, loss=0.359, v_num=0000]\n",
      "Epoch 0:  85%|████████▍ | 1403/1660 [02:48<00:30,  8.34it/s, loss=0.365, v_num=0000]\n",
      "Epoch 0:  85%|████████▍ | 1404/1660 [02:48<00:30,  8.34it/s, loss=0.363, v_num=0000]\n",
      "Epoch 0:  85%|████████▍ | 1405/1660 [02:48<00:30,  8.34it/s, loss=0.364, v_num=0000]\n",
      "Epoch 0:  85%|████████▍ | 1406/1660 [02:48<00:30,  8.34it/s, loss=0.36, v_num=0000] \n",
      "Epoch 0:  85%|████████▍ | 1407/1660 [02:48<00:30,  8.34it/s, loss=0.359, v_num=0000]\n",
      "Epoch 0:  85%|████████▍ | 1408/1660 [02:48<00:30,  8.34it/s, loss=0.355, v_num=0000]\n",
      "Epoch 0:  85%|████████▍ | 1409/1660 [02:48<00:30,  8.34it/s, loss=0.35, v_num=0000] \n",
      "Epoch 0:  85%|████████▍ | 1410/1660 [02:49<00:29,  8.34it/s, loss=0.357, v_num=0000]\n",
      "Epoch 0:  85%|████████▌ | 1411/1660 [02:49<00:29,  8.34it/s, loss=0.361, v_num=0000]\n",
      "Epoch 0:  85%|████████▌ | 1412/1660 [02:49<00:29,  8.34it/s, loss=0.37, v_num=0000] \n",
      "Epoch 0:  85%|████████▌ | 1413/1660 [02:49<00:29,  8.34it/s, loss=0.358, v_num=0000]\n",
      "Epoch 0:  85%|████████▌ | 1414/1660 [02:49<00:29,  8.34it/s, loss=0.357, v_num=0000]\n",
      "Epoch 0:  85%|████████▌ | 1415/1660 [02:49<00:29,  8.34it/s, loss=0.36, v_num=0000] \n",
      "Epoch 0:  85%|████████▌ | 1416/1660 [02:49<00:29,  8.34it/s, loss=0.364, v_num=0000]\n",
      "Epoch 0:  85%|████████▌ | 1417/1660 [02:49<00:29,  8.34it/s, loss=0.359, v_num=0000]\n",
      "Epoch 0:  85%|████████▌ | 1418/1660 [02:50<00:29,  8.34it/s, loss=0.358, v_num=0000]\n",
      "Epoch 0:  85%|████████▌ | 1419/1660 [02:50<00:28,  8.34it/s, loss=0.363, v_num=0000]\n",
      "Epoch 0:  86%|████████▌ | 1420/1660 [02:50<00:28,  8.34it/s, loss=0.377, v_num=0000]\n",
      "Epoch 0:  86%|████████▌ | 1421/1660 [02:50<00:28,  8.34it/s, loss=0.363, v_num=0000]\n",
      "Epoch 0:  86%|████████▌ | 1422/1660 [02:50<00:28,  8.34it/s, loss=0.359, v_num=0000]\n",
      "Epoch 0:  86%|████████▌ | 1423/1660 [02:50<00:28,  8.34it/s, loss=0.353, v_num=0000]\n",
      "Epoch 0:  86%|████████▌ | 1424/1660 [02:50<00:28,  8.34it/s, loss=0.355, v_num=0000]\n",
      "Epoch 0:  86%|████████▌ | 1425/1660 [02:50<00:28,  8.34it/s, loss=0.356, v_num=0000]\n",
      "Epoch 0:  86%|████████▌ | 1426/1660 [02:50<00:28,  8.34it/s, loss=0.355, v_num=0000]\n",
      "== Status ==\n",
      "Current time: 2022-06-20 12:03:12 (running for 00:03:05.61)\n",
      "Memory usage on this node: 29.8/376.5 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1.0/8 CPUs, 1.0/1 GPUs, 0.0/34.4 GiB heap, 0.0/17.2 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/dobby/ray_results/tune_rp\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+----------------------+----------+------------------+-----------------+----------------+\n",
      "| Trial name           | status   | loc              |   learning_rate |   weight_decay |\n",
      "|----------------------+----------+------------------+-----------------+----------------|\n",
      "| train_rp_86c32_00000 | RUNNING  | 172.17.0.5:17844 |     0.000368944 |            0.1 |\n",
      "+----------------------+----------+------------------+-----------------+----------------+\n",
      "\n",
      "\n",
      "Epoch 0:  86%|████████▌ | 1427/1660 [02:51<00:27,  8.34it/s, loss=0.353, v_num=0000]\n",
      "Epoch 0:  86%|████████▌ | 1428/1660 [02:51<00:27,  8.34it/s, loss=0.358, v_num=0000]\n",
      "Epoch 0:  86%|████████▌ | 1429/1660 [02:51<00:27,  8.34it/s, loss=0.362, v_num=0000]\n",
      "Epoch 0:  86%|████████▌ | 1430/1660 [02:51<00:27,  8.34it/s, loss=0.357, v_num=0000]\n",
      "Epoch 0:  86%|████████▌ | 1431/1660 [02:51<00:27,  8.34it/s, loss=0.348, v_num=0000]\n",
      "Epoch 0:  86%|████████▋ | 1432/1660 [02:51<00:27,  8.34it/s, loss=0.342, v_num=0000]\n",
      "Epoch 0:  86%|████████▋ | 1433/1660 [02:51<00:27,  8.34it/s, loss=0.342, v_num=0000]\n",
      "Epoch 0:  86%|████████▋ | 1434/1660 [02:51<00:27,  8.34it/s, loss=0.343, v_num=0000]\n",
      "Epoch 0:  86%|████████▋ | 1435/1660 [02:52<00:26,  8.34it/s, loss=0.335, v_num=0000]\n",
      "Epoch 0:  87%|████████▋ | 1436/1660 [02:52<00:26,  8.34it/s, loss=0.336, v_num=0000]\n",
      "Epoch 0:  87%|████████▋ | 1437/1660 [02:52<00:26,  8.34it/s, loss=0.342, v_num=0000]\n",
      "Epoch 0:  87%|████████▋ | 1438/1660 [02:52<00:26,  8.34it/s, loss=0.338, v_num=0000]\n",
      "Epoch 0:  87%|████████▋ | 1439/1660 [02:52<00:26,  8.34it/s, loss=0.335, v_num=0000]\n",
      "Epoch 0:  87%|████████▋ | 1440/1660 [02:52<00:26,  8.34it/s, loss=0.323, v_num=0000]\n",
      "Epoch 0:  87%|████████▋ | 1441/1660 [02:52<00:26,  8.34it/s, loss=0.328, v_num=0000]\n",
      "Epoch 0:  87%|████████▋ | 1442/1660 [02:52<00:26,  8.34it/s, loss=0.334, v_num=0000]\n",
      "Epoch 0:  87%|████████▋ | 1443/1660 [02:53<00:26,  8.34it/s, loss=0.334, v_num=0000]\n",
      "Epoch 0:  87%|████████▋ | 1444/1660 [02:53<00:25,  8.34it/s, loss=0.337, v_num=0000]\n",
      "Epoch 0:  87%|████████▋ | 1445/1660 [02:53<00:25,  8.34it/s, loss=0.338, v_num=0000]\n",
      "Epoch 0:  87%|████████▋ | 1446/1660 [02:53<00:25,  8.34it/s, loss=0.35, v_num=0000] \n",
      "Epoch 0:  87%|████████▋ | 1447/1660 [02:53<00:25,  8.34it/s, loss=0.348, v_num=0000]\n",
      "Epoch 0:  87%|████████▋ | 1448/1660 [02:53<00:25,  8.34it/s, loss=0.348, v_num=0000]\n",
      "Epoch 0:  87%|████████▋ | 1448/1660 [02:53<00:25,  8.34it/s, loss=0.352, v_num=0000]\n",
      "Epoch 0:  87%|████████▋ | 1449/1660 [02:53<00:25,  8.34it/s, loss=0.354, v_num=0000]\n",
      "Epoch 0:  87%|████████▋ | 1450/1660 [02:53<00:25,  8.34it/s, loss=0.358, v_num=0000]\n",
      "Epoch 0:  87%|████████▋ | 1451/1660 [02:53<00:25,  8.34it/s, loss=0.361, v_num=0000]\n",
      "Epoch 0:  87%|████████▋ | 1452/1660 [02:54<00:24,  8.34it/s, loss=0.364, v_num=0000]\n",
      "Epoch 0:  88%|████████▊ | 1453/1660 [02:54<00:24,  8.34it/s, loss=0.359, v_num=0000]\n",
      "Epoch 0:  88%|████████▊ | 1454/1660 [02:54<00:24,  8.34it/s, loss=0.357, v_num=0000]\n",
      "Epoch 0:  88%|████████▊ | 1455/1660 [02:54<00:24,  8.34it/s, loss=0.365, v_num=0000]\n",
      "Epoch 0:  88%|████████▊ | 1456/1660 [02:54<00:24,  8.34it/s, loss=0.367, v_num=0000]\n",
      "Epoch 0:  88%|████████▊ | 1457/1660 [02:54<00:24,  8.34it/s, loss=0.37, v_num=0000] \n",
      "Epoch 0:  88%|████████▊ | 1458/1660 [02:54<00:24,  8.34it/s, loss=0.375, v_num=0000]\n",
      "Epoch 0:  88%|████████▊ | 1459/1660 [02:54<00:24,  8.34it/s, loss=0.384, v_num=0000]\n",
      "Epoch 0:  88%|████████▊ | 1460/1660 [02:55<00:23,  8.34it/s, loss=0.387, v_num=0000]\n",
      "Epoch 0:  88%|████████▊ | 1461/1660 [02:55<00:23,  8.34it/s, loss=0.389, v_num=0000]\n",
      "Epoch 0:  88%|████████▊ | 1462/1660 [02:55<00:23,  8.34it/s, loss=0.392, v_num=0000]\n",
      "Epoch 0:  88%|████████▊ | 1463/1660 [02:55<00:23,  8.34it/s, loss=0.397, v_num=0000]\n",
      "Epoch 0:  88%|████████▊ | 1464/1660 [02:55<00:23,  8.34it/s, loss=0.395, v_num=0000]\n",
      "Epoch 0:  88%|████████▊ | 1465/1660 [02:55<00:23,  8.34it/s, loss=0.398, v_num=0000]\n",
      "Epoch 0:  88%|████████▊ | 1466/1660 [02:55<00:23,  8.34it/s, loss=0.396, v_num=0000]\n",
      "Epoch 0:  88%|████████▊ | 1467/1660 [02:55<00:23,  8.34it/s, loss=0.389, v_num=0000]\n",
      "Epoch 0:  88%|████████▊ | 1468/1660 [02:56<00:23,  8.34it/s, loss=0.381, v_num=0000]\n",
      "== Status ==\n",
      "Current time: 2022-06-20 12:03:17 (running for 00:03:10.62)\n",
      "Memory usage on this node: 29.8/376.5 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1.0/8 CPUs, 1.0/1 GPUs, 0.0/34.4 GiB heap, 0.0/17.2 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/dobby/ray_results/tune_rp\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+----------------------+----------+------------------+-----------------+----------------+\n",
      "| Trial name           | status   | loc              |   learning_rate |   weight_decay |\n",
      "|----------------------+----------+------------------+-----------------+----------------|\n",
      "| train_rp_86c32_00000 | RUNNING  | 172.17.0.5:17844 |     0.000368944 |            0.1 |\n",
      "+----------------------+----------+------------------+-----------------+----------------+\n",
      "\n",
      "\n",
      "Epoch 0:  88%|████████▊ | 1469/1660 [02:56<00:22,  8.34it/s, loss=0.384, v_num=0000]\n",
      "Epoch 0:  89%|████████▊ | 1470/1660 [02:56<00:22,  8.34it/s, loss=0.374, v_num=0000]\n",
      "Epoch 0:  89%|████████▊ | 1471/1660 [02:56<00:22,  8.34it/s, loss=0.373, v_num=0000]\n",
      "Epoch 0:  89%|████████▊ | 1472/1660 [02:56<00:22,  8.34it/s, loss=0.368, v_num=0000]\n",
      "Epoch 0:  89%|████████▊ | 1473/1660 [02:56<00:22,  8.34it/s, loss=0.374, v_num=0000]\n",
      "Epoch 0:  89%|████████▉ | 1474/1660 [02:56<00:22,  8.34it/s, loss=0.373, v_num=0000]\n",
      "Epoch 0:  89%|████████▉ | 1475/1660 [02:56<00:22,  8.34it/s, loss=0.366, v_num=0000]\n",
      "Epoch 0:  89%|████████▉ | 1476/1660 [02:56<00:22,  8.34it/s, loss=0.362, v_num=0000]\n",
      "Epoch 0:  89%|████████▉ | 1477/1660 [02:57<00:21,  8.34it/s, loss=0.362, v_num=0000]\n",
      "Epoch 0:  89%|████████▉ | 1478/1660 [02:57<00:21,  8.34it/s, loss=0.36, v_num=0000] \n",
      "Epoch 0:  89%|████████▉ | 1479/1660 [02:57<00:21,  8.34it/s, loss=0.34, v_num=0000]\n",
      "Epoch 0:  89%|████████▉ | 1480/1660 [02:57<00:21,  8.34it/s, loss=0.34, v_num=0000]\n",
      "Epoch 0:  89%|████████▉ | 1481/1660 [02:57<00:21,  8.34it/s, loss=0.337, v_num=0000]\n",
      "Epoch 0:  89%|████████▉ | 1482/1660 [02:57<00:21,  8.34it/s, loss=0.332, v_num=0000]\n",
      "Epoch 0:  89%|████████▉ | 1483/1660 [02:57<00:21,  8.34it/s, loss=0.332, v_num=0000]\n",
      "Epoch 0:  89%|████████▉ | 1484/1660 [02:57<00:21,  8.34it/s, loss=0.335, v_num=0000]\n",
      "Epoch 0:  89%|████████▉ | 1485/1660 [02:58<00:20,  8.34it/s, loss=0.331, v_num=0000]\n",
      "Epoch 0:  90%|████████▉ | 1486/1660 [02:58<00:20,  8.34it/s, loss=0.326, v_num=0000]\n",
      "Epoch 0:  90%|████████▉ | 1487/1660 [02:58<00:20,  8.34it/s, loss=0.331, v_num=0000]\n",
      "Epoch 0:  90%|████████▉ | 1488/1660 [02:58<00:20,  8.34it/s, loss=0.335, v_num=0000]\n",
      "Epoch 0:  90%|████████▉ | 1489/1660 [02:58<00:20,  8.34it/s, loss=0.336, v_num=0000]\n",
      "Epoch 0:  90%|████████▉ | 1490/1660 [02:58<00:20,  8.34it/s, loss=0.349, v_num=0000]\n",
      "Epoch 0:  90%|████████▉ | 1491/1660 [02:58<00:20,  8.34it/s, loss=0.347, v_num=0000]\n",
      "Epoch 0:  90%|████████▉ | 1492/1660 [02:58<00:20,  8.34it/s, loss=0.348, v_num=0000]\n",
      "Epoch 0:  90%|████████▉ | 1493/1660 [02:59<00:20,  8.34it/s, loss=0.346, v_num=0000]\n",
      "Epoch 0:  90%|█████████ | 1494/1660 [02:59<00:19,  8.34it/s, loss=0.349, v_num=0000]\n",
      "Epoch 0:  90%|█████████ | 1495/1660 [02:59<00:19,  8.34it/s, loss=0.347, v_num=0000]\n",
      "Epoch 0:  90%|█████████ | 1496/1660 [02:59<00:19,  8.34it/s, loss=0.352, v_num=0000]\n",
      "Epoch 0:  90%|█████████ | 1497/1660 [02:59<00:19,  8.34it/s, loss=0.349, v_num=0000]\n",
      "Epoch 0:  90%|█████████ | 1498/1660 [02:59<00:19,  8.34it/s, loss=0.35, v_num=0000] \n",
      "Epoch 0:  90%|█████████ | 1499/1660 [02:59<00:19,  8.34it/s, loss=0.364, v_num=0000]\n",
      "Epoch 0:  90%|█████████ | 1500/1660 [02:59<00:19,  8.34it/s, loss=0.373, v_num=0000]\n",
      "Epoch 0:  90%|█████████ | 1501/1660 [02:59<00:19,  8.34it/s, loss=0.373, v_num=0000]\n",
      "Epoch 0:  90%|█████████ | 1501/1660 [02:59<00:19,  8.34it/s, loss=0.367, v_num=0000]\n",
      "Epoch 0:  90%|█████████ | 1502/1660 [03:00<00:18,  8.34it/s, loss=0.366, v_num=0000]\n",
      "Epoch 0:  91%|█████████ | 1503/1660 [03:00<00:18,  8.34it/s, loss=0.367, v_num=0000]\n",
      "Epoch 0:  91%|█████████ | 1504/1660 [03:00<00:18,  8.34it/s, loss=0.363, v_num=0000]\n",
      "Epoch 0:  91%|█████████ | 1505/1660 [03:00<00:18,  8.34it/s, loss=0.365, v_num=0000]\n",
      "Epoch 0:  91%|█████████ | 1506/1660 [03:00<00:18,  8.34it/s, loss=0.359, v_num=0000]\n",
      "Epoch 0:  91%|█████████ | 1507/1660 [03:00<00:18,  8.34it/s, loss=0.358, v_num=0000]\n",
      "Epoch 0:  91%|█████████ | 1508/1660 [03:00<00:18,  8.34it/s, loss=0.349, v_num=0000]\n",
      "Epoch 0:  91%|█████████ | 1509/1660 [03:00<00:18,  8.34it/s, loss=0.344, v_num=0000]\n",
      "Epoch 0:  91%|█████████ | 1510/1660 [03:01<00:17,  8.34it/s, loss=0.344, v_num=0000]\n",
      "== Status ==\n",
      "Current time: 2022-06-20 12:03:22 (running for 00:03:15.62)\n",
      "Memory usage on this node: 29.8/376.5 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1.0/8 CPUs, 1.0/1 GPUs, 0.0/34.4 GiB heap, 0.0/17.2 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/dobby/ray_results/tune_rp\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+----------------------+----------+------------------+-----------------+----------------+\n",
      "| Trial name           | status   | loc              |   learning_rate |   weight_decay |\n",
      "|----------------------+----------+------------------+-----------------+----------------|\n",
      "| train_rp_86c32_00000 | RUNNING  | 172.17.0.5:17844 |     0.000368944 |            0.1 |\n",
      "+----------------------+----------+------------------+-----------------+----------------+\n",
      "\n",
      "\n",
      "Epoch 0:  91%|█████████ | 1511/1660 [03:01<00:17,  8.34it/s, loss=0.348, v_num=0000]\n",
      "Epoch 0:  91%|█████████ | 1512/1660 [03:01<00:17,  8.34it/s, loss=0.354, v_num=0000]\n",
      "Epoch 0:  91%|█████████ | 1513/1660 [03:01<00:17,  8.34it/s, loss=0.361, v_num=0000]\n",
      "Epoch 0:  91%|█████████ | 1514/1660 [03:01<00:17,  8.34it/s, loss=0.358, v_num=0000]\n",
      "Epoch 0:  91%|█████████▏| 1515/1660 [03:01<00:17,  8.34it/s, loss=0.361, v_num=0000]\n",
      "Epoch 0:  91%|█████████▏| 1516/1660 [03:01<00:17,  8.34it/s, loss=0.358, v_num=0000]\n",
      "Epoch 0:  91%|█████████▏| 1517/1660 [03:01<00:17,  8.34it/s, loss=0.359, v_num=0000]\n",
      "Epoch 0:  91%|█████████▏| 1518/1660 [03:01<00:17,  8.34it/s, loss=0.36, v_num=0000] \n",
      "Epoch 0:  92%|█████████▏| 1519/1660 [03:02<00:16,  8.34it/s, loss=0.359, v_num=0000]\n",
      "Epoch 0:  92%|█████████▏| 1520/1660 [03:02<00:16,  8.34it/s, loss=0.348, v_num=0000]\n",
      "Epoch 0:  92%|█████████▏| 1521/1660 [03:02<00:16,  8.34it/s, loss=0.358, v_num=0000]\n",
      "Epoch 0:  92%|█████████▏| 1522/1660 [03:02<00:16,  8.34it/s, loss=0.358, v_num=0000]\n",
      "Epoch 0:  92%|█████████▏| 1523/1660 [03:02<00:16,  8.34it/s, loss=0.357, v_num=0000]\n",
      "Epoch 0:  92%|█████████▏| 1524/1660 [03:02<00:16,  8.34it/s, loss=0.355, v_num=0000]\n",
      "Epoch 0:  92%|█████████▏| 1525/1660 [03:02<00:16,  8.34it/s, loss=0.355, v_num=0000]\n",
      "Epoch 0:  92%|█████████▏| 1526/1660 [03:02<00:16,  8.34it/s, loss=0.354, v_num=0000]\n",
      "Epoch 0:  92%|█████████▏| 1527/1660 [03:03<00:15,  8.34it/s, loss=0.361, v_num=0000]\n",
      "Epoch 0:  92%|█████████▏| 1528/1660 [03:03<00:15,  8.34it/s, loss=0.359, v_num=0000]\n",
      "Epoch 0:  92%|█████████▏| 1529/1660 [03:03<00:15,  8.34it/s, loss=0.352, v_num=0000]\n",
      "Epoch 0:  92%|█████████▏| 1530/1660 [03:03<00:15,  8.34it/s, loss=0.352, v_num=0000]\n",
      "Epoch 0:  92%|█████████▏| 1530/1660 [03:03<00:15,  8.34it/s, loss=0.349, v_num=0000]\n",
      "Epoch 0:  92%|█████████▏| 1531/1660 [03:03<00:15,  8.34it/s, loss=0.351, v_num=0000]\n",
      "Epoch 0:  92%|█████████▏| 1532/1660 [03:03<00:15,  8.34it/s, loss=0.35, v_num=0000] \n",
      "Epoch 0:  92%|█████████▏| 1533/1660 [03:03<00:15,  8.34it/s, loss=0.336, v_num=0000]\n",
      "Epoch 0:  92%|█████████▏| 1534/1660 [03:03<00:15,  8.34it/s, loss=0.347, v_num=0000]\n",
      "Epoch 0:  92%|█████████▏| 1535/1660 [03:04<00:14,  8.34it/s, loss=0.344, v_num=0000]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A \n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Validation:   0%|          | 0/125 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/125 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Validation DataLoader 0:   1%|          | 1/125 [00:00<00:15,  8.15it/s]\u001b[A\n",
      "Epoch 0:  93%|█████████▎| 1536/1660 [03:04<00:14,  8.32it/s, loss=0.344, v_num=0000]\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Epoch 0:  93%|█████████▎| 1537/1660 [03:04<00:14,  8.32it/s, loss=0.344, v_num=0000]\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Validation DataLoader 0:   2%|▏         | 3/125 [00:00<00:10, 11.81it/s]\u001b[A\n",
      "Epoch 0:  93%|█████████▎| 1538/1660 [03:04<00:14,  8.32it/s, loss=0.344, v_num=0000]\n",
      "Epoch 0:  93%|█████████▎| 1538/1660 [03:04<00:14,  8.32it/s, loss=0.344, v_num=0000]\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Epoch 0:  93%|█████████▎| 1539/1660 [03:04<00:14,  8.32it/s, loss=0.344, v_num=0000]\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Validation DataLoader 0:   4%|▍         | 5/125 [00:00<00:09, 12.71it/s]\u001b[A\n",
      "Epoch 0:  93%|█████████▎| 1540/1660 [03:04<00:14,  8.33it/s, loss=0.344, v_num=0000]\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Epoch 0:  93%|█████████▎| 1541/1660 [03:05<00:14,  8.33it/s, loss=0.344, v_num=0000]\n",
      "Validation DataLoader 0:   6%|▌         | 7/125 [00:00<00:08, 13.43it/s]\u001b[A\n",
      "Epoch 0:  93%|█████████▎| 1542/1660 [03:05<00:14,  8.33it/s, loss=0.344, v_num=0000]\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Epoch 0:  93%|█████████▎| 1543/1660 [03:05<00:14,  8.33it/s, loss=0.344, v_num=0000]\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Validation DataLoader 0:   7%|▋         | 9/125 [00:00<00:08, 13.69it/s]\u001b[A\n",
      "Epoch 0:  93%|█████████▎| 1544/1660 [03:05<00:13,  8.33it/s, loss=0.344, v_num=0000]\n",
      "Epoch 0:  93%|█████████▎| 1545/1660 [03:05<00:13,  8.34it/s, loss=0.344, v_num=0000]\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Validation DataLoader 0:   9%|▉         | 11/125 [00:00<00:08, 13.85it/s]\u001b[A\n",
      "Epoch 0:  93%|█████████▎| 1546/1660 [03:05<00:13,  8.34it/s, loss=0.344, v_num=0000]\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Epoch 0:  93%|█████████▎| 1547/1660 [03:05<00:13,  8.34it/s, loss=0.344, v_num=0000]\n",
      "Validation DataLoader 0:  10%|█         | 13/125 [00:00<00:08, 13.83it/s]\u001b[A\n",
      "Epoch 0:  93%|█████████▎| 1548/1660 [03:05<00:13,  8.34it/s, loss=0.344, v_num=0000]\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Epoch 0:  93%|█████████▎| 1549/1660 [03:05<00:13,  8.35it/s, loss=0.344, v_num=0000]\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Validation DataLoader 0:  12%|█▏        | 15/125 [00:01<00:07, 13.95it/s]\u001b[A\n",
      "Epoch 0:  93%|█████████▎| 1550/1660 [03:05<00:13,  8.35it/s, loss=0.344, v_num=0000]\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Epoch 0:  93%|█████████▎| 1551/1660 [03:05<00:13,  8.35it/s, loss=0.344, v_num=0000]\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Validation DataLoader 0:  14%|█▎        | 17/125 [00:01<00:07, 14.03it/s]\u001b[A\n",
      "Epoch 0:  93%|█████████▎| 1552/1660 [03:05<00:12,  8.35it/s, loss=0.344, v_num=0000]\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Epoch 0:  94%|█████████▎| 1553/1660 [03:05<00:12,  8.35it/s, loss=0.344, v_num=0000]\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Validation DataLoader 0:  15%|█▌        | 19/125 [00:01<00:07, 14.05it/s]\u001b[A\n",
      "Epoch 0:  94%|█████████▎| 1554/1660 [03:05<00:12,  8.36it/s, loss=0.344, v_num=0000]\n",
      "Epoch 0:  94%|█████████▎| 1555/1660 [03:06<00:12,  8.36it/s, loss=0.344, v_num=0000]\n",
      "== Status ==\n",
      "Current time: 2022-06-20 12:03:27 (running for 00:03:20.62)\n",
      "Memory usage on this node: 29.8/376.5 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1.0/8 CPUs, 1.0/1 GPUs, 0.0/34.4 GiB heap, 0.0/17.2 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/dobby/ray_results/tune_rp\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+----------------------+----------+------------------+-----------------+----------------+\n",
      "| Trial name           | status   | loc              |   learning_rate |   weight_decay |\n",
      "|----------------------+----------+------------------+-----------------+----------------|\n",
      "| train_rp_86c32_00000 | RUNNING  | 172.17.0.5:17844 |     0.000368944 |            0.1 |\n",
      "+----------------------+----------+------------------+-----------------+----------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Validation DataLoader 0:  17%|█▋        | 21/125 [00:01<00:07, 14.11it/s]\u001b[A\n",
      "Epoch 0:  94%|█████████▎| 1556/1660 [03:06<00:12,  8.36it/s, loss=0.344, v_num=0000]\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Epoch 0:  94%|█████████▍| 1557/1660 [03:06<00:12,  8.36it/s, loss=0.344, v_num=0000]\n",
      "Validation DataLoader 0:  18%|█▊        | 23/125 [00:01<00:07, 14.11it/s]\u001b[A\n",
      "Epoch 0:  94%|█████████▍| 1558/1660 [03:06<00:12,  8.37it/s, loss=0.344, v_num=0000]\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Epoch 0:  94%|█████████▍| 1559/1660 [03:06<00:12,  8.37it/s, loss=0.344, v_num=0000]\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Validation DataLoader 0:  20%|██        | 25/125 [00:01<00:07, 14.13it/s]\u001b[A\n",
      "Epoch 0:  94%|█████████▍| 1560/1660 [03:06<00:11,  8.37it/s, loss=0.344, v_num=0000]\n",
      "Epoch 0:  94%|█████████▍| 1561/1660 [03:06<00:11,  8.37it/s, loss=0.344, v_num=0000]\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Validation DataLoader 0:  22%|██▏       | 27/125 [00:01<00:07, 13.99it/s]\u001b[A\n",
      "Epoch 0:  94%|█████████▍| 1562/1660 [03:06<00:11,  8.37it/s, loss=0.344, v_num=0000]\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Epoch 0:  94%|█████████▍| 1563/1660 [03:06<00:11,  8.38it/s, loss=0.344, v_num=0000]\n",
      "Validation DataLoader 0:  23%|██▎       | 29/125 [00:02<00:06, 14.13it/s]\u001b[A\n",
      "Epoch 0:  94%|█████████▍| 1564/1660 [03:06<00:11,  8.38it/s, loss=0.344, v_num=0000]\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Epoch 0:  94%|█████████▍| 1565/1660 [03:06<00:11,  8.38it/s, loss=0.344, v_num=0000]\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Validation DataLoader 0:  25%|██▍       | 31/125 [00:02<00:06, 14.17it/s]\u001b[A\n",
      "Epoch 0:  94%|█████████▍| 1566/1660 [03:06<00:11,  8.38it/s, loss=0.344, v_num=0000]\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Epoch 0:  94%|█████████▍| 1567/1660 [03:06<00:11,  8.39it/s, loss=0.344, v_num=0000]\n",
      "Validation DataLoader 0:  26%|██▋       | 33/125 [00:02<00:06, 14.18it/s]\u001b[A\n",
      "Epoch 0:  94%|█████████▍| 1568/1660 [03:06<00:10,  8.39it/s, loss=0.344, v_num=0000]\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Epoch 0:  95%|█████████▍| 1569/1660 [03:07<00:10,  8.39it/s, loss=0.344, v_num=0000]\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Validation DataLoader 0:  28%|██▊       | 35/125 [00:02<00:06, 14.05it/s]\u001b[A\n",
      "Epoch 0:  95%|█████████▍| 1570/1660 [03:07<00:10,  8.39it/s, loss=0.344, v_num=0000]\n",
      "Epoch 0:  95%|█████████▍| 1571/1660 [03:07<00:10,  8.39it/s, loss=0.344, v_num=0000]\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Validation DataLoader 0:  30%|██▉       | 37/125 [00:02<00:06, 14.12it/s]\u001b[A\n",
      "Epoch 0:  95%|█████████▍| 1572/1660 [03:07<00:10,  8.40it/s, loss=0.344, v_num=0000]\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Epoch 0:  95%|█████████▍| 1573/1660 [03:07<00:10,  8.40it/s, loss=0.344, v_num=0000]\n",
      "Validation DataLoader 0:  31%|███       | 39/125 [00:02<00:06, 14.13it/s]\u001b[A\n",
      "Epoch 0:  95%|█████████▍| 1574/1660 [03:07<00:10,  8.40it/s, loss=0.344, v_num=0000]\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Epoch 0:  95%|█████████▍| 1575/1660 [03:07<00:10,  8.40it/s, loss=0.344, v_num=0000]\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Validation DataLoader 0:  33%|███▎      | 41/125 [00:02<00:05, 14.13it/s]\u001b[A\n",
      "Epoch 0:  95%|█████████▍| 1576/1660 [03:07<00:09,  8.40it/s, loss=0.344, v_num=0000]\n",
      "Epoch 0:  95%|█████████▌| 1577/1660 [03:07<00:09,  8.41it/s, loss=0.344, v_num=0000]\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Validation DataLoader 0:  34%|███▍      | 43/125 [00:03<00:05, 14.18it/s]\u001b[A\n",
      "Epoch 0:  95%|█████████▌| 1578/1660 [03:07<00:09,  8.41it/s, loss=0.344, v_num=0000]\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Epoch 0:  95%|█████████▌| 1579/1660 [03:07<00:09,  8.41it/s, loss=0.344, v_num=0000]\n",
      "Validation DataLoader 0:  36%|███▌      | 45/125 [00:03<00:05, 14.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|███▌      | 45/125 [00:03<00:05, 14.22it/s]\u001b[A\n",
      "Epoch 0:  95%|█████████▌| 1580/1660 [03:07<00:09,  8.41it/s, loss=0.344, v_num=0000]\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Epoch 0:  95%|█████████▌| 1581/1660 [03:07<00:09,  8.42it/s, loss=0.344, v_num=0000]\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Validation DataLoader 0:  38%|███▊      | 47/125 [00:03<00:05, 14.27it/s]\u001b[A\n",
      "Epoch 0:  95%|█████████▌| 1582/1660 [03:07<00:09,  8.42it/s, loss=0.344, v_num=0000]\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Epoch 0:  95%|█████████▌| 1583/1660 [03:08<00:09,  8.42it/s, loss=0.344, v_num=0000]\n",
      "Validation DataLoader 0:  39%|███▉      | 49/125 [00:03<00:05, 14.07it/s]\u001b[A\n",
      "Epoch 0:  95%|█████████▌| 1584/1660 [03:08<00:09,  8.42it/s, loss=0.344, v_num=0000]\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Epoch 0:  95%|█████████▌| 1585/1660 [03:08<00:08,  8.42it/s, loss=0.344, v_num=0000]\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Validation DataLoader 0:  41%|████      | 51/125 [00:03<00:05, 14.15it/s]\u001b[A\n",
      "Epoch 0:  96%|█████████▌| 1586/1660 [03:08<00:08,  8.43it/s, loss=0.344, v_num=0000]\n",
      "Epoch 0:  96%|█████████▌| 1587/1660 [03:08<00:08,  8.43it/s, loss=0.344, v_num=0000]\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Validation DataLoader 0:  42%|████▏     | 53/125 [00:03<00:05, 14.20it/s]\u001b[A\n",
      "Epoch 0:  96%|█████████▌| 1588/1660 [03:08<00:08,  8.43it/s, loss=0.344, v_num=0000]\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Epoch 0:  96%|█████████▌| 1589/1660 [03:08<00:08,  8.43it/s, loss=0.344, v_num=0000]\n",
      "Validation DataLoader 0:  44%|████▍     | 55/125 [00:03<00:04, 14.17it/s]\u001b[A\n",
      "Epoch 0:  96%|█████████▌| 1590/1660 [03:08<00:08,  8.44it/s, loss=0.344, v_num=0000]\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Epoch 0:  96%|█████████▌| 1591/1660 [03:08<00:08,  8.44it/s, loss=0.344, v_num=0000]\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Validation DataLoader 0:  46%|████▌     | 57/125 [00:04<00:04, 14.23it/s]\u001b[A\n",
      "Epoch 0:  96%|█████████▌| 1592/1660 [03:08<00:08,  8.44it/s, loss=0.344, v_num=0000]\n",
      "Epoch 0:  96%|█████████▌| 1593/1660 [03:08<00:07,  8.44it/s, loss=0.344, v_num=0000]\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Validation DataLoader 0:  47%|████▋     | 59/125 [00:04<00:04, 14.24it/s]\u001b[A\n",
      "Epoch 0:  96%|█████████▌| 1594/1660 [03:08<00:07,  8.44it/s, loss=0.344, v_num=0000]\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Epoch 0:  96%|█████████▌| 1595/1660 [03:08<00:07,  8.45it/s, loss=0.344, v_num=0000]\n",
      "Validation DataLoader 0:  49%|████▉     | 61/125 [00:04<00:04, 14.23it/s]\u001b[A\n",
      "Epoch 0:  96%|█████████▌| 1596/1660 [03:08<00:07,  8.45it/s, loss=0.344, v_num=0000]\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Epoch 0:  96%|█████████▌| 1597/1660 [03:08<00:07,  8.45it/s, loss=0.344, v_num=0000]\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Validation DataLoader 0:  50%|█████     | 63/125 [00:04<00:04, 14.10it/s]\u001b[A\n",
      "Epoch 0:  96%|█████████▋| 1598/1660 [03:09<00:07,  8.45it/s, loss=0.344, v_num=0000]\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Epoch 0:  96%|█████████▋| 1599/1660 [03:09<00:07,  8.45it/s, loss=0.344, v_num=0000]\n",
      "Validation DataLoader 0:  52%|█████▏    | 65/125 [00:04<00:04, 14.13it/s]\u001b[A\n",
      "Epoch 0:  96%|█████████▋| 1600/1660 [03:09<00:07,  8.46it/s, loss=0.344, v_num=0000]\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Epoch 0:  96%|█████████▋| 1601/1660 [03:09<00:06,  8.46it/s, loss=0.344, v_num=0000]\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Validation DataLoader 0:  54%|█████▎    | 67/125 [00:04<00:04, 14.15it/s]\u001b[A\n",
      "Epoch 0:  97%|█████████▋| 1602/1660 [03:09<00:06,  8.46it/s, loss=0.344, v_num=0000]\n",
      "Epoch 0:  97%|█████████▋| 1603/1660 [03:09<00:06,  8.46it/s, loss=0.344, v_num=0000]\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Validation DataLoader 0:  55%|█████▌    | 69/125 [00:04<00:03, 14.15it/s]\u001b[A\n",
      "Epoch 0:  97%|█████████▋| 1604/1660 [03:09<00:06,  8.46it/s, loss=0.344, v_num=0000]\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Epoch 0:  97%|█████████▋| 1605/1660 [03:09<00:06,  8.47it/s, loss=0.344, v_num=0000]\n",
      "Validation DataLoader 0:  57%|█████▋    | 71/125 [00:05<00:03, 13.99it/s]\u001b[A\n",
      "Epoch 0:  97%|█████████▋| 1606/1660 [03:09<00:06,  8.47it/s, loss=0.344, v_num=0000]\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Epoch 0:  97%|█████████▋| 1607/1660 [03:09<00:06,  8.47it/s, loss=0.344, v_num=0000]\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Validation DataLoader 0:  58%|█████▊    | 73/125 [00:05<00:03, 14.04it/s]\u001b[A\n",
      "Epoch 0:  97%|█████████▋| 1608/1660 [03:09<00:06,  8.47it/s, loss=0.344, v_num=0000]\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Epoch 0:  97%|█████████▋| 1609/1660 [03:09<00:06,  8.48it/s, loss=0.344, v_num=0000]\n",
      "Validation DataLoader 0:  60%|██████    | 75/125 [00:05<00:03, 14.06it/s]\u001b[A\n",
      "Epoch 0:  97%|█████████▋| 1610/1660 [03:09<00:05,  8.48it/s, loss=0.344, v_num=0000]\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Epoch 0:  97%|█████████▋| 1611/1660 [03:09<00:05,  8.48it/s, loss=0.344, v_num=0000]\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Validation DataLoader 0:  62%|██████▏   | 77/125 [00:05<00:03, 14.05it/s]\u001b[A\n",
      "Epoch 0:  97%|█████████▋| 1612/1660 [03:10<00:05,  8.48it/s, loss=0.344, v_num=0000]\n",
      "Epoch 0:  97%|█████████▋| 1613/1660 [03:10<00:05,  8.48it/s, loss=0.344, v_num=0000]\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Validation DataLoader 0:  63%|██████▎   | 79/125 [00:05<00:03, 14.09it/s]\u001b[A\n",
      "Epoch 0:  97%|█████████▋| 1614/1660 [03:10<00:05,  8.49it/s, loss=0.344, v_num=0000]\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Epoch 0:  97%|█████████▋| 1615/1660 [03:10<00:05,  8.49it/s, loss=0.344, v_num=0000]\n",
      "Validation DataLoader 0:  65%|██████▍   | 81/125 [00:05<00:03, 14.12it/s]\u001b[A\n",
      "Epoch 0:  97%|█████████▋| 1616/1660 [03:10<00:05,  8.49it/s, loss=0.344, v_num=0000]\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Epoch 0:  97%|█████████▋| 1617/1660 [03:10<00:05,  8.49it/s, loss=0.344, v_num=0000]\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Validation DataLoader 0:  66%|██████▋   | 83/125 [00:05<00:02, 14.14it/s]\u001b[A\n",
      "Epoch 0:  97%|█████████▋| 1618/1660 [03:10<00:04,  8.49it/s, loss=0.344, v_num=0000]\n",
      "Epoch 0:  98%|█████████▊| 1619/1660 [03:10<00:04,  8.50it/s, loss=0.344, v_num=0000]\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Validation DataLoader 0:  68%|██████▊   | 85/125 [00:06<00:02, 14.05it/s]\u001b[A\n",
      "Epoch 0:  98%|█████████▊| 1620/1660 [03:10<00:04,  8.50it/s, loss=0.344, v_num=0000]\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Epoch 0:  98%|█████████▊| 1621/1660 [03:10<00:04,  8.50it/s, loss=0.344, v_num=0000]\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Validation DataLoader 0:  70%|██████▉   | 87/125 [00:06<00:02, 14.10it/s]\u001b[A\n",
      "Epoch 0:  98%|█████████▊| 1622/1660 [03:10<00:04,  8.50it/s, loss=0.344, v_num=0000]\n",
      "Epoch 0:  98%|█████████▊| 1623/1660 [03:10<00:04,  8.50it/s, loss=0.344, v_num=0000]\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Validation DataLoader 0:  71%|███████   | 89/125 [00:06<00:02, 14.12it/s]\u001b[A\n",
      "Epoch 0:  98%|█████████▊| 1624/1660 [03:10<00:04,  8.51it/s, loss=0.344, v_num=0000]\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Epoch 0:  98%|█████████▊| 1625/1660 [03:10<00:04,  8.51it/s, loss=0.344, v_num=0000]\n",
      "Validation DataLoader 0:  73%|███████▎  | 91/125 [00:06<00:02, 14.01it/s]\u001b[A\n",
      "Epoch 0:  98%|█████████▊| 1626/1660 [03:11<00:03,  8.51it/s, loss=0.344, v_num=0000]\n",
      "== Status ==\n",
      "Current time: 2022-06-20 12:03:32 (running for 00:03:25.62)\n",
      "Memory usage on this node: 29.8/376.5 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1.0/8 CPUs, 1.0/1 GPUs, 0.0/34.4 GiB heap, 0.0/17.2 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/dobby/ray_results/tune_rp\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+----------------------+----------+------------------+-----------------+----------------+\n",
      "| Trial name           | status   | loc              |   learning_rate |   weight_decay |\n",
      "|----------------------+----------+------------------+-----------------+----------------|\n",
      "| train_rp_86c32_00000 | RUNNING  | 172.17.0.5:17844 |     0.000368944 |            0.1 |\n",
      "+----------------------+----------+------------------+-----------------+----------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Epoch 0:  98%|█████████▊| 1627/1660 [03:11<00:03,  8.51it/s, loss=0.344, v_num=0000]\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Validation DataLoader 0:  74%|███████▍  | 93/125 [00:06<00:02, 14.04it/s]\u001b[A\n",
      "Epoch 0:  98%|█████████▊| 1628/1660 [03:11<00:03,  8.51it/s, loss=0.344, v_num=0000]\n",
      "Epoch 0:  98%|█████████▊| 1629/1660 [03:11<00:03,  8.52it/s, loss=0.344, v_num=0000]\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Validation DataLoader 0:  76%|███████▌  | 95/125 [00:06<00:02, 14.10it/s]\u001b[A\n",
      "Epoch 0:  98%|█████████▊| 1630/1660 [03:11<00:03,  8.52it/s, loss=0.344, v_num=0000]\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Epoch 0:  98%|█████████▊| 1631/1660 [03:11<00:03,  8.52it/s, loss=0.344, v_num=0000]\n",
      "Validation DataLoader 0:  78%|███████▊  | 97/125 [00:06<00:01, 14.13it/s]\u001b[A\n",
      "Epoch 0:  98%|█████████▊| 1632/1660 [03:11<00:03,  8.52it/s, loss=0.344, v_num=0000]\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Epoch 0:  98%|█████████▊| 1633/1660 [03:11<00:03,  8.53it/s, loss=0.344, v_num=0000]\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Validation DataLoader 0:  79%|███████▉  | 99/125 [00:07<00:01, 14.01it/s]\u001b[A\n",
      "Epoch 0:  98%|█████████▊| 1634/1660 [03:11<00:03,  8.53it/s, loss=0.344, v_num=0000]\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Epoch 0:  98%|█████████▊| 1635/1660 [03:11<00:02,  8.53it/s, loss=0.344, v_num=0000]\n",
      "Validation DataLoader 0:  81%|████████  | 101/125 [00:07<00:01, 14.07it/s]\u001b[A\n",
      "Epoch 0:  99%|█████████▊| 1636/1660 [03:11<00:02,  8.53it/s, loss=0.344, v_num=0000]\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Epoch 0:  99%|█████████▊| 1637/1660 [03:11<00:02,  8.53it/s, loss=0.344, v_num=0000]\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Validation DataLoader 0:  82%|████████▏ | 103/125 [00:07<00:01, 14.09it/s]\u001b[A\n",
      "Epoch 0:  99%|█████████▊| 1638/1660 [03:11<00:02,  8.54it/s, loss=0.344, v_num=0000]\n",
      "Epoch 0:  99%|█████████▊| 1639/1660 [03:11<00:02,  8.54it/s, loss=0.344, v_num=0000]\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Validation DataLoader 0:  84%|████████▍ | 105/125 [00:07<00:01, 14.05it/s]\u001b[A\n",
      "Epoch 0:  99%|█████████▉| 1640/1660 [03:12<00:02,  8.54it/s, loss=0.344, v_num=0000]\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Epoch 0:  99%|█████████▉| 1641/1660 [03:12<00:02,  8.54it/s, loss=0.344, v_num=0000]\n",
      "Validation DataLoader 0:  86%|████████▌ | 107/125 [00:07<00:01, 14.12it/s]\u001b[A\n",
      "Epoch 0:  99%|█████████▉| 1642/1660 [03:12<00:02,  8.54it/s, loss=0.344, v_num=0000]\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Epoch 0:  99%|█████████▉| 1643/1660 [03:12<00:01,  8.55it/s, loss=0.344, v_num=0000]\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Validation DataLoader 0:  87%|████████▋ | 109/125 [00:07<00:01, 14.17it/s]\u001b[A\n",
      "Epoch 0:  99%|█████████▉| 1644/1660 [03:12<00:01,  8.55it/s, loss=0.344, v_num=0000]\n",
      "Epoch 0:  99%|█████████▉| 1645/1660 [03:12<00:01,  8.55it/s, loss=0.344, v_num=0000]\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Validation DataLoader 0:  89%|████████▉ | 111/125 [00:07<00:00, 14.13it/s]\u001b[A\n",
      "Epoch 0:  99%|█████████▉| 1646/1660 [03:12<00:01,  8.55it/s, loss=0.344, v_num=0000]\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Epoch 0:  99%|█████████▉| 1647/1660 [03:12<00:01,  8.55it/s, loss=0.344, v_num=0000]\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Validation DataLoader 0:  90%|█████████ | 113/125 [00:08<00:00, 14.01it/s]\u001b[A\n",
      "Epoch 0:  99%|█████████▉| 1648/1660 [03:12<00:01,  8.56it/s, loss=0.344, v_num=0000]\n",
      "Epoch 0:  99%|█████████▉| 1649/1660 [03:12<00:01,  8.56it/s, loss=0.344, v_num=0000]\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Validation DataLoader 0:  92%|█████████▏| 115/125 [00:08<00:00, 14.08it/s]\u001b[A\n",
      "Epoch 0:  99%|█████████▉| 1650/1660 [03:12<00:01,  8.56it/s, loss=0.344, v_num=0000]\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Epoch 0:  99%|█████████▉| 1651/1660 [03:12<00:01,  8.56it/s, loss=0.344, v_num=0000]\n",
      "Validation DataLoader 0:  94%|█████████▎| 117/125 [00:08<00:00, 14.13it/s]\u001b[A\n",
      "Epoch 0: 100%|█████████▉| 1652/1660 [03:12<00:00,  8.56it/s, loss=0.344, v_num=0000]\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Epoch 0: 100%|█████████▉| 1653/1660 [03:12<00:00,  8.57it/s, loss=0.344, v_num=0000]\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Validation DataLoader 0:  95%|█████████▌| 119/125 [00:08<00:00, 14.18it/s]\u001b[A\n",
      "Epoch 0: 100%|█████████▉| 1654/1660 [03:13<00:00,  8.57it/s, loss=0.344, v_num=0000]\n",
      "Epoch 0: 100%|█████████▉| 1655/1660 [03:13<00:00,  8.57it/s, loss=0.344, v_num=0000]\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Validation DataLoader 0:  97%|█████████▋| 121/125 [00:08<00:00, 14.09it/s]\u001b[A\n",
      "Epoch 0: 100%|█████████▉| 1656/1660 [03:13<00:00,  8.57it/s, loss=0.344, v_num=0000]\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Epoch 0: 100%|█████████▉| 1657/1660 [03:13<00:00,  8.57it/s, loss=0.344, v_num=0000]\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Validation DataLoader 0:  98%|█████████▊| 123/125 [00:08<00:00, 14.13it/s]\u001b[A\n",
      "Epoch 0: 100%|█████████▉| 1658/1660 [03:13<00:00,  8.58it/s, loss=0.344, v_num=0000]\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Epoch 0: 100%|█████████▉| 1659/1660 [03:13<00:00,  8.58it/s, loss=0.344, v_num=0000]\n",
      "Result for train_rp_86c32_00000:\n",
      "  acc: 0.509\n",
      "  date: 2022-06-20_12-03-34\n",
      "  done: false\n",
      "  experiment_id: c6f5e25ceb2844f99760bc29b12a9a21\n",
      "  hostname: facd02a90c00\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 172.17.0.5\n",
      "  pid: 17844\n",
      "  time_since_restore: 202.64174103736877\n",
      "  time_this_iter_s: 202.64174103736877\n",
      "  time_total_s: 202.64174103736877\n",
      "  timestamp: 1655726614\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 86c32_00000\n",
      "  warmup_time: 1.069762945175171\n",
      "  \n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 125/125 [00:08<00:00, 14.18it/s]\u001b[A\n",
      "Epoch 0: 100%|██████████| 1660/1660 [03:13<00:00,  8.58it/s, loss=0.344, v_num=0000]\n",
      "Epoch 0: 100%|██████████| 1660/1660 [03:13<00:00,  8.58it/s, loss=0.344, v_num=0000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-20 12:03:36,635\tERROR trial_runner.py:886 -- Trial train_rp_86c32_00000: Error processing event.\n",
      "NoneType: None\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m 2022-06-20 12:03:36,621\tERROR function_runner.py:286 -- Runner Thread raised error.\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m   File \"/home/dobby/.local/lib/python3.9/site-packages/ray/tune/function_runner.py\", line 277, in run\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m     self._entrypoint()\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m   File \"/home/dobby/.local/lib/python3.9/site-packages/ray/tune/function_runner.py\", line 349, in entrypoint\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m     return self._trainable_func(\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m   File \"/home/dobby/.local/lib/python3.9/site-packages/ray/util/tracing/tracing_helper.py\", line 462, in _resume_span\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m   File \"/home/dobby/.local/lib/python3.9/site-packages/ray/tune/function_runner.py\", line 645, in _trainable_func\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m     output = fn()\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m   File \"/home/dobby/.local/lib/python3.9/site-packages/ray/tune/utils/trainable.py\", line 419, in _inner\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m     inner(config, checkpoint_dir=None)\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m   File \"/home/dobby/.local/lib/python3.9/site-packages/ray/tune/utils/trainable.py\", line 410, in inner\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m     trainable(config, **fn_kwargs)\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m   File \"/tmp/ipykernel_14250/1043132461.py\", line 26, in train_rp\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m   File \"/home/dobby/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 770, in fit\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m     self._call_and_handle_interrupt(\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m   File \"/home/dobby/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 723, in _call_and_handle_interrupt\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m   File \"/home/dobby/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 811, in _fit_impl\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m     results = self._run(model, ckpt_path=self.ckpt_path)\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m   File \"/home/dobby/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 1236, in _run\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m     results = self._run_stage()\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m   File \"/home/dobby/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 1323, in _run_stage\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m     return self._run_train()\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m   File \"/home/dobby/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 1353, in _run_train\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m     self.fit_loop.run()\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m   File \"/home/dobby/.local/lib/python3.9/site-packages/pytorch_lightning/loops/base.py\", line 205, in run\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m     self.on_advance_end()\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m   File \"/home/dobby/.local/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py\", line 297, in on_advance_end\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m     self.trainer._call_callback_hooks(\"on_train_epoch_end\")\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m   File \"/home/dobby/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 1636, in _call_callback_hooks\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m     fn(self, self.lightning_module, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m   File \"/home/dobby/.local/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 308, in on_train_epoch_end\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m     self._save_topk_checkpoint(trainer, monitor_candidates)\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m   File \"/home/dobby/.local/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 381, in _save_topk_checkpoint\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m     self._save_none_monitor_checkpoint(trainer, monitor_candidates)\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m   File \"/home/dobby/.local/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 661, in _save_none_monitor_checkpoint\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m     self._save_checkpoint(trainer, filepath)\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m   File \"/home/dobby/.local/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 384, in _save_checkpoint\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m     trainer.save_checkpoint(filepath, self.save_weights_only)\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m   File \"/home/dobby/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 2467, in save_checkpoint\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m     self._checkpoint_connector.save_checkpoint(filepath, weights_only=weights_only, storage_options=storage_options)\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m   File \"/home/dobby/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py\", line 445, in save_checkpoint\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m     self.trainer.strategy.save_checkpoint(_checkpoint, filepath, storage_options=storage_options)\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m   File \"/home/dobby/.local/lib/python3.9/site-packages/pytorch_lightning/strategies/strategy.py\", line 418, in save_checkpoint\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m     self.checkpoint_io.save_checkpoint(checkpoint, filepath, storage_options=storage_options)\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m   File \"/home/dobby/.local/lib/python3.9/site-packages/pytorch_lightning/plugins/io/torch_plugin.py\", line 51, in save_checkpoint\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m     fs.makedirs(os.path.dirname(path), exist_ok=True)\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m   File \"/home/dobby/.local/lib/python3.9/site-packages/fsspec/implementations/local.py\", line 47, in makedirs\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m     os.makedirs(path, exist_ok=exist_ok)\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m   File \"/usr/lib/python3.9/os.py\", line 215, in makedirs\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m     makedirs(head, exist_ok=exist_ok)\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m   File \"/usr/lib/python3.9/os.py\", line 215, in makedirs\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m     makedirs(head, exist_ok=exist_ok)\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m   File \"/usr/lib/python3.9/os.py\", line 215, in makedirs\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m     makedirs(head, exist_ok=exist_ok)\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m   [Previous line repeated 1 more time]\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m   File \"/usr/lib/python3.9/os.py\", line 225, in makedirs\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m     mkdir(name, mode)\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m PermissionError: [Errno 13] Permission denied: '/512-RP-Mod'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for train_rp_86c32_00000:\n",
      "  acc: 0.509\n",
      "  date: 2022-06-20_12-03-34\n",
      "  done: false\n",
      "  experiment_id: c6f5e25ceb2844f99760bc29b12a9a21\n",
      "  experiment_tag: 0_learning_rate=0.0004,weight_decay=0.1000\n",
      "  hostname: facd02a90c00\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 172.17.0.5\n",
      "  pid: 17844\n",
      "  time_since_restore: 202.64174103736877\n",
      "  time_this_iter_s: 202.64174103736877\n",
      "  time_total_s: 202.64174103736877\n",
      "  timestamp: 1655726614\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 86c32_00000\n",
      "  warmup_time: 1.069762945175171\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-06-20 12:03:36 (running for 00:03:29.79)\n",
      "Memory usage on this node: 31.4/376.5 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/8 CPUs, 0/1 GPUs, 0.0/34.4 GiB heap, 0.0/17.2 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Current best trial: 86c32_00000 with acc=0.509 and parameters={'learning_rate': 0.00036894439804188814, 'weight_decay': 0.1}\n",
      "Result logdir: /home/dobby/ray_results/tune_rp\n",
      "Number of trials: 1/1 (1 ERROR)\n",
      "+----------------------+----------+------------------+-----------------+----------------+--------+------------------+-------+\n",
      "| Trial name           | status   | loc              |   learning_rate |   weight_decay |   iter |   total time (s) |   acc |\n",
      "|----------------------+----------+------------------+-----------------+----------------+--------+------------------+-------|\n",
      "| train_rp_86c32_00000 | ERROR    | 172.17.0.5:17844 |     0.000368944 |            0.1 |      1 |          202.642 | 0.509 |\n",
      "+----------------------+----------+------------------+-----------------+----------------+--------+------------------+-------+\n",
      "Number of errored trials: 1\n",
      "+----------------------+--------------+-------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name           |   # failures | error file                                                                                                                    |\n",
      "|----------------------+--------------+-------------------------------------------------------------------------------------------------------------------------------|\n",
      "| train_rp_86c32_00000 |            1 | /home/dobby/ray_results/tune_rp/train_rp_86c32_00000_0_learning_rate=0.0004,weight_decay=0.1000_2022-06-20_12-00-07/error.txt |\n",
      "+----------------------+--------------+-------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m wandb: \\ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m wandb: | 0.001 MB of 0.010 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m wandb: / 0.001 MB of 0.010 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m wandb: - 0.001 MB of 0.010 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m wandb: \\ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m wandb: \\ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)\n",
      "wandb:                                                                                uped)\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m wandb:    avg_val_accuracy ▁\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m wandb:        avg_val_loss ▁\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m wandb:               epoch ▁▁\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m wandb:          train_loss ▁\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m wandb: trainer/global_step ▁█\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m wandb:        val_accuracy ▁\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m wandb:            val_loss ▁\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m wandb:    avg_val_accuracy 0.509\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m wandb:        avg_val_loss 0.41228\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m wandb:               epoch 0\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m wandb:          train_loss 0.31222\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m wandb: trainer/global_step 95\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m wandb:        val_accuracy 0.509\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m wandb:            val_loss 0.41228\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m wandb: Synced train_rp_86c32_00000: https://wandb.ai/isadoraw/Optimization_Project/runs/86c32_00000\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(train_rp pid=17844)\u001b[0m wandb: Find logs at: ./wandb/run-20220620_120011-86c32_00000/logs\n"
     ]
    },
    {
     "ename": "TuneError",
     "evalue": "('Trials did not complete', [train_rp_86c32_00000])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTuneError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\isado\\transformersModerat\\Training\\T5\\t5_fine_tuning_rp_mod.ipynb Cell 25'\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/c%3A/Users/isado/transformersModerat/Training/T5/t5_fine_tuning_rp_mod.ipynb#ch0000056?line=6'>7</a>\u001b[0m config \u001b[39m=\u001b[39m {\n\u001b[1;32m      <a href='vscode-notebook-cell:/c%3A/Users/isado/transformersModerat/Training/T5/t5_fine_tuning_rp_mod.ipynb#ch0000056?line=7'>8</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mlearning_rate\u001b[39m\u001b[39m\"\u001b[39m: tune\u001b[39m.\u001b[39mloguniform(\u001b[39m1e-4\u001b[39m, \u001b[39m5e-3\u001b[39m), \n\u001b[1;32m      <a href='vscode-notebook-cell:/c%3A/Users/isado/transformersModerat/Training/T5/t5_fine_tuning_rp_mod.ipynb#ch0000056?line=8'>9</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mweight_decay\u001b[39m\u001b[39m\"\u001b[39m: tune\u001b[39m.\u001b[39mchoice([\u001b[39m0.01\u001b[39m, \u001b[39m0.1\u001b[39m]),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/isado/transformersModerat/Training/T5/t5_fine_tuning_rp_mod.ipynb#ch0000056?line=13'>14</a>\u001b[0m         }\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/isado/transformersModerat/Training/T5/t5_fine_tuning_rp_mod.ipynb#ch0000056?line=14'>15</a>\u001b[0m }\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/isado/transformersModerat/Training/T5/t5_fine_tuning_rp_mod.ipynb#ch0000056?line=16'>17</a>\u001b[0m trainable \u001b[39m=\u001b[39m tune\u001b[39m.\u001b[39mwith_parameters(\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/isado/transformersModerat/Training/T5/t5_fine_tuning_rp_mod.ipynb#ch0000056?line=17'>18</a>\u001b[0m     train_rp\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/isado/transformersModerat/Training/T5/t5_fine_tuning_rp_mod.ipynb#ch0000056?line=18'>19</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell:/c%3A/Users/isado/transformersModerat/Training/T5/t5_fine_tuning_rp_mod.ipynb#ch0000056?line=20'>21</a>\u001b[0m analysis \u001b[39m=\u001b[39m tune\u001b[39m.\u001b[39;49mrun(\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/isado/transformersModerat/Training/T5/t5_fine_tuning_rp_mod.ipynb#ch0000056?line=21'>22</a>\u001b[0m     trainable,\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/isado/transformersModerat/Training/T5/t5_fine_tuning_rp_mod.ipynb#ch0000056?line=22'>23</a>\u001b[0m     resources_per_trial\u001b[39m=\u001b[39;49m{\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/isado/transformersModerat/Training/T5/t5_fine_tuning_rp_mod.ipynb#ch0000056?line=23'>24</a>\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m\"\u001b[39;49m:\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/isado/transformersModerat/Training/T5/t5_fine_tuning_rp_mod.ipynb#ch0000056?line=24'>25</a>\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mgpu\u001b[39;49m\u001b[39m\"\u001b[39;49m:\u001b[39m1\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/isado/transformersModerat/Training/T5/t5_fine_tuning_rp_mod.ipynb#ch0000056?line=25'>26</a>\u001b[0m     },\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/isado/transformersModerat/Training/T5/t5_fine_tuning_rp_mod.ipynb#ch0000056?line=26'>27</a>\u001b[0m     metric\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39macc\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/isado/transformersModerat/Training/T5/t5_fine_tuning_rp_mod.ipynb#ch0000056?line=27'>28</a>\u001b[0m     mode\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmin\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/isado/transformersModerat/Training/T5/t5_fine_tuning_rp_mod.ipynb#ch0000056?line=28'>29</a>\u001b[0m     config\u001b[39m=\u001b[39;49mconfig,\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/isado/transformersModerat/Training/T5/t5_fine_tuning_rp_mod.ipynb#ch0000056?line=29'>30</a>\u001b[0m     name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtune_rp\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/isado/transformersModerat/Training/T5/t5_fine_tuning_rp_mod.ipynb#ch0000056?line=30'>31</a>\u001b[0m     \u001b[39m# callbacks=[wandb_logger]\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/isado/transformersModerat/Training/T5/t5_fine_tuning_rp_mod.ipynb#ch0000056?line=31'>32</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/isado/transformersModerat/Training/T5/t5_fine_tuning_rp_mod.ipynb#ch0000056?line=33'>34</a>\u001b[0m \u001b[39mprint\u001b[39m(analysis\u001b[39m.\u001b[39mbest_config)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/ray/tune/tune.py:741\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, local_dir, search_alg, scheduler, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, server_port, resume, reuse_actors, trial_executor, raise_on_failed_trial, callbacks, max_concurrent_trials, _experiment_checkpoint_dir, loggers, _remote)\u001b[0m\n\u001b[1;32m    739\u001b[0m \u001b[39mif\u001b[39;00m incomplete_trials:\n\u001b[1;32m    740\u001b[0m     \u001b[39mif\u001b[39;00m raise_on_failed_trial \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m state[\u001b[39m\"\u001b[39m\u001b[39msignal\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m--> 741\u001b[0m         \u001b[39mraise\u001b[39;00m TuneError(\u001b[39m\"\u001b[39m\u001b[39mTrials did not complete\u001b[39m\u001b[39m\"\u001b[39m, incomplete_trials)\n\u001b[1;32m    742\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    743\u001b[0m         logger\u001b[39m.\u001b[39merror(\u001b[39m\"\u001b[39m\u001b[39mTrials did not complete: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, incomplete_trials)\n",
      "\u001b[0;31mTuneError\u001b[0m: ('Trials did not complete', [train_rp_86c32_00000])"
     ]
    }
   ],
   "source": [
    "from ray import tune\n",
    "from ray.tune.logger import DEFAULT_LOGGERS\n",
    "from ray.tune.integration.wandb import WandbLogger\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "config = {\n",
    "    \"learning_rate\": tune.loguniform(1e-4, 5e-3), \n",
    "    \"weight_decay\": tune.choice([0.01, 0.1]),\n",
    "    # wandb configuration\n",
    "    \"wandb\": {\n",
    "            \"project\": \"Optimization_Project\",\n",
    "            # \"log_config\": True\n",
    "        }\n",
    "}\n",
    "\n",
    "trainable = tune.with_parameters(\n",
    "    train_rp\n",
    ")\n",
    "\n",
    "analysis = tune.run(\n",
    "    trainable,\n",
    "    resources_per_trial={\n",
    "        \"cpu\":1,\n",
    "        \"gpu\":1\n",
    "    },\n",
    "    metric=\"acc\",\n",
    "    mode=\"min\",\n",
    "    config=config,\n",
    "    name=\"tune_rp\",\n",
    "    # callbacks=[wandb_logger]\n",
    ")\n",
    "\n",
    "print(analysis.best_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Less fancy hyperparameter search :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33misadoraw\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.18"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/dobby/wandb/run-20220620_203600-30ei2mip</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/isadoraw/RP-Mod-GermanT5-oscar-german-small-el32-hyperparameter-tuning/runs/30ei2mip\" target=\"_blank\">learning_rate-0.002-weight_decay-0.1</a></strong> to <a href=\"https://wandb.ai/isadoraw/RP-Mod-GermanT5-oscar-german-small-el32-hyperparameter-tuning\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'checkpoint_callback' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\isado\\transformersModerat\\Training\\T5\\t5_fine_tuning_rp_mod.ipynb Cell 26'\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/isado/transformersModerat/Training/T5/t5_fine_tuning_rp_mod.ipynb#ch0000026?line=11'>12</a>\u001b[0m args_dict \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/isado/transformersModerat/Training/T5/t5_fine_tuning_rp_mod.ipynb#ch0000026?line=12'>13</a>\u001b[0m     data_dir\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m# path for data files\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/isado/transformersModerat/Training/T5/t5_fine_tuning_rp_mod.ipynb#ch0000026?line=13'>14</a>\u001b[0m     output_dir\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m/GermanT5-RP-Mod/t5-efficient-oscar-german-small-el32/\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m# path to save the checkpoints\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/isado/transformersModerat/Training/T5/t5_fine_tuning_rp_mod.ipynb#ch0000026?line=31'>32</a>\u001b[0m     seed\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/isado/transformersModerat/Training/T5/t5_fine_tuning_rp_mod.ipynb#ch0000026?line=32'>33</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/isado/transformersModerat/Training/T5/t5_fine_tuning_rp_mod.ipynb#ch0000026?line=33'>34</a>\u001b[0m args \u001b[39m=\u001b[39m argparse\u001b[39m.\u001b[39mNamespace(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39margs_dict)\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/isado/transformersModerat/Training/T5/t5_fine_tuning_rp_mod.ipynb#ch0000026?line=35'>36</a>\u001b[0m train_params \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/isado/transformersModerat/Training/T5/t5_fine_tuning_rp_mod.ipynb#ch0000026?line=36'>37</a>\u001b[0m     accumulate_grad_batches\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39mgradient_accumulation_steps,\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/isado/transformersModerat/Training/T5/t5_fine_tuning_rp_mod.ipynb#ch0000026?line=37'>38</a>\u001b[0m     auto_lr_find\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/isado/transformersModerat/Training/T5/t5_fine_tuning_rp_mod.ipynb#ch0000026?line=38'>39</a>\u001b[0m     gpus\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39mn_gpu,\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/isado/transformersModerat/Training/T5/t5_fine_tuning_rp_mod.ipynb#ch0000026?line=39'>40</a>\u001b[0m     max_epochs\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39mnum_train_epochs,\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/isado/transformersModerat/Training/T5/t5_fine_tuning_rp_mod.ipynb#ch0000026?line=40'>41</a>\u001b[0m     default_root_dir\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m/home/dobby/RP-Mod/t5-efficient-oscar-german-small-el32\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/isado/transformersModerat/Training/T5/t5_fine_tuning_rp_mod.ipynb#ch0000026?line=41'>42</a>\u001b[0m     \u001b[39m# early_stop_callback=False,\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/isado/transformersModerat/Training/T5/t5_fine_tuning_rp_mod.ipynb#ch0000026?line=42'>43</a>\u001b[0m     precision\u001b[39m=\u001b[39m \u001b[39m16\u001b[39m \u001b[39mif\u001b[39;00m args\u001b[39m.\u001b[39mfp_16 \u001b[39melse\u001b[39;00m \u001b[39m32\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/isado/transformersModerat/Training/T5/t5_fine_tuning_rp_mod.ipynb#ch0000026?line=43'>44</a>\u001b[0m     amp_level\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39mopt_level,\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/isado/transformersModerat/Training/T5/t5_fine_tuning_rp_mod.ipynb#ch0000026?line=44'>45</a>\u001b[0m     gradient_clip_val\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39mmax_grad_norm,\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/isado/transformersModerat/Training/T5/t5_fine_tuning_rp_mod.ipynb#ch0000026?line=45'>46</a>\u001b[0m     \u001b[39m# checkpoint_callback=checkpoint_callback,\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/isado/transformersModerat/Training/T5/t5_fine_tuning_rp_mod.ipynb#ch0000026?line=46'>47</a>\u001b[0m     logger\u001b[39m=\u001b[39mwandb_logger,\n\u001b[0;32m---> <a href='vscode-notebook-cell:/c%3A/Users/isado/transformersModerat/Training/T5/t5_fine_tuning_rp_mod.ipynb#ch0000026?line=47'>48</a>\u001b[0m     enable_checkpointing\u001b[39m=\u001b[39mcheckpoint_callback,\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/isado/transformersModerat/Training/T5/t5_fine_tuning_rp_mod.ipynb#ch0000026?line=48'>49</a>\u001b[0m     callbacks\u001b[39m=\u001b[39m[raytuner_callback],\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/isado/transformersModerat/Training/T5/t5_fine_tuning_rp_mod.ipynb#ch0000026?line=49'>50</a>\u001b[0m     \u001b[39m# callbacks=[LoggingCallback()],\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/isado/transformersModerat/Training/T5/t5_fine_tuning_rp_mod.ipynb#ch0000026?line=50'>51</a>\u001b[0m     amp_backend\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mapex\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/isado/transformersModerat/Training/T5/t5_fine_tuning_rp_mod.ipynb#ch0000026?line=51'>52</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/isado/transformersModerat/Training/T5/t5_fine_tuning_rp_mod.ipynb#ch0000026?line=53'>54</a>\u001b[0m model \u001b[39m=\u001b[39m T5FineTuner(args, train_dataset, val_dataset)\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/isado/transformersModerat/Training/T5/t5_fine_tuning_rp_mod.ipynb#ch0000026?line=54'>55</a>\u001b[0m trainer \u001b[39m=\u001b[39m pl\u001b[39m.\u001b[39mTrainer(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtrain_params)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'checkpoint_callback' is not defined"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import wandb\n",
    "possible_learning_rates = [2e-3, 9e-4]\n",
    "possible_weight_decays = [0.1, 0.01, 1]\n",
    "\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    dirpath=args.output_dir, filename=\"checkpoint\", monitor=\"val_accuracy\", mode=\"max\", save_top_k=5\n",
    ")\n",
    "\n",
    "for lr in possible_learning_rates:\n",
    "    for wd in possible_weight_decays:\n",
    "        wandb.finish()\n",
    "        wandb_logger = WandbLogger(project=f\"{args.dataset_name}-GermanT5-oscar-german-small-el32-hyperparameter-tuning\", \n",
    "        name=f\"learning_rate-{lr}-weight_decay-{wd}\")\n",
    "\n",
    "        args_dict = dict(\n",
    "            data_dir=\"\", # path for data files\n",
    "            output_dir=\"/GermanT5-RP-Mod/t5-efficient-oscar-german-small-el32/\", # path to save the checkpoints\n",
    "            model_name_or_path=\"GermanT5/t5-efficient-oscar-german-small-el32\",\n",
    "            tokenizer_name_or_path=\"GermanT5/t5-efficient-oscar-german-small-el32\",\n",
    "            dataset_name=\"RP-Mod\",\n",
    "            max_seq_length=512,\n",
    "            learning_rate=lr,\n",
    "            weight_decay=wd,\n",
    "            adam_epsilon=1e-8,\n",
    "            warmup_steps=0,\n",
    "            train_batch_size=8,\n",
    "            eval_batch_size=8,\n",
    "            num_train_epochs=10,\n",
    "            gradient_accumulation_steps=16,\n",
    "            n_gpu=1,\n",
    "            early_stop_callback=False,\n",
    "            fp_16=False, # if you want to enable 16-bit training then install apex and set this to true\n",
    "            opt_level='O1', # you can find out more on optimisation levels here https://nvidia.github.io/apex/amp.html#opt-levels-and-properties\n",
    "            max_grad_norm=0.5, # if you enable 16-bit training then set this to a sensible value, 0.5 is a good default\n",
    "            seed=42,\n",
    "        )\n",
    "        args = argparse.Namespace(**args_dict)\n",
    "\n",
    "        train_params = dict(\n",
    "            accumulate_grad_batches=args.gradient_accumulation_steps,\n",
    "            auto_lr_find=True,\n",
    "            gpus=args.n_gpu,\n",
    "            max_epochs=args.num_train_epochs,\n",
    "            default_root_dir=f\"/home/dobby/RP-Mod/t5-efficient-oscar-german-small-el32\",\n",
    "            # early_stop_callback=False,\n",
    "            precision= 16 if args.fp_16 else 32,\n",
    "            amp_level=args.opt_level,\n",
    "            gradient_clip_val=args.max_grad_norm,\n",
    "            # checkpoint_callback=checkpoint_callback,\n",
    "            logger=wandb_logger,\n",
    "            enable_checkpointing=checkpoint_callback,\n",
    "            callbacks=[raytuner_callback],\n",
    "            # callbacks=[LoggingCallback()],\n",
    "            amp_backend=\"apex\"\n",
    "        )\n",
    "\n",
    "        model = T5FineTuner(args, train_dataset, val_dataset)\n",
    "        trainer = pl.Trainer(**train_params)\n",
    "\n",
    "        trainer.fit(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Begin Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "model = T5FineTuner(args, train_dataset, val_dataset)\n",
    "trainer = pl.Trainer(**train_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(data_dir='', output_dir='/GermanT5-RP-Mod/t5-efficient-oscar-german-small-el32/', model_name_or_path='GermanT5/t5-efficient-oscar-german-small-el32', tokenizer_name_or_path='GermanT5/t5-efficient-oscar-german-small-el32', dataset_name='RP-Mod', max_seq_length=512, learning_rate=0.0019, weight_decay=0.1, adam_epsilon=1e-08, warmup_steps=0, train_batch_size=8, eval_batch_size=8, num_train_epochs=10, gradient_accumulation_steps=16, n_gpu=1, early_stop_callback=False, fp_16=False, opt_level='O1', max_grad_norm=0.5, seed=42)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dobby/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:117: UserWarning: When using `Trainer(accumulate_grad_batches != 1)` and overriding `LightningModule.optimizer_{step,zero_grad}`, the hooks will not be called on every batch (rather, they are called on every optimization step).\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/dobby/.local/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\n",
      "  | Name  | Type                       | Params\n",
      "-----------------------------------------------------\n",
      "0 | model | T5ForConditionalGeneration | 142 M \n",
      "-----------------------------------------------------\n",
      "142 M     Trainable params\n",
      "0         Non-trainable params\n",
      "142 M     Total params\n",
      "569.289   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f433cacae6747c7a81819e33221f9f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "377d1f4141fa40ef9d831e828769ede2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dobby/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:151: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=<pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint object at 0x7f1fa979cc10>)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=<pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint object at 0x7f1fa979cc10>)`.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/dobby/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:117: UserWarning: When using `Trainer(accumulate_grad_batches != 1)` and overriding `LightningModule.optimizer_{step,zero_grad}`, the hooks will not be called on every batch (rather, they are called on every optimization step).\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/dobby/.local/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8165bf50856942c89a4484e9a4710d07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /home/dobby/.lr_find_eabaf0f2-c58f-481b-8134-e84ef263372d.ckpt\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = T5FineTuner(args, train_dataset, val_dataset)\n",
    "trainer = pl.Trainer(**train_params)\n",
    "\n",
    "\n",
    "lr_finder = trainer.tuner.lr_find(model)\n",
    "\n",
    "# Results can be found in\n",
    "# print(lr_finder.results)\n",
    "\n",
    "# Plot with\n",
    "fig = lr_finder.plot(suggest=True)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'new_lr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\isado\\transformersModerat\\Training\\T5\\t5_fine_tuning_rp_mod.ipynb Cell 18'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/c%3A/Users/isado/transformersModerat/Training/T5/t5_fine_tuning_rp_mod.ipynb#ch0000020?line=0'>1</a>\u001b[0m new_lr\n",
      "\u001b[0;31mNameError\u001b[0m: name 'new_lr' is not defined"
     ]
    }
   ],
   "source": [
    "new_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_lr = lr_finder.suggestion()\n",
    "\n",
    "# model.hparams.learning_rate = new_lr\n",
    "\n",
    "# trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0019054607179632484"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                       | Params\n",
      "-----------------------------------------------------\n",
      "0 | model | T5ForConditionalGeneration | 222 M \n",
      "-----------------------------------------------------\n",
      "222 M     Trainable params\n",
      "0         Non-trainable params\n",
      "222 M     Total params\n",
      "891.614   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "695f7baebb8b496eb0903f40d5e5d4f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42ee3ddd40eb4f7f8677e986892db0ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26a0b9e3065040b7928177cd80784427",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dobby/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:286: UserWarning: The ``compute`` method of metric _ResultMetric was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02cf6f7a55c94347a26c305f28715fc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.save_pretrained('t5_german_small_rp_mod_3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 113])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = test_dataset\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "it = iter(loader)\n",
    "\n",
    "batch = next(it)\n",
    "batch[\"source_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[23574, 22822,  2940,  ...,     0,     0,     0],\n",
       "        [23574, 22822,   114,  ...,     0,     0,     0],\n",
       "        [23574, 22822,     8,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [23574, 22822,  2903,  ...,     0,     0,     0],\n",
       "        [23574, 22822,  6428,  ...,     0,     0,     0],\n",
       "        [23574, 22822,  1631,  ...,     0,     0,     0]], device='cuda:0')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"source_ids\"].cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "outs = model.model.generate(input_ids=batch['source_ids'], \n",
    "                              attention_mask=batch['source_mask'], \n",
    "                              max_length=2)\n",
    "\n",
    "dec = [tokenizer.decode(ids) for ids in outs]\n",
    "\n",
    "texts = [tokenizer.decode(ids) for ids in batch['source_ids']]\n",
    "targets = [tokenizer.decode(ids) for ids in batch['target_ids']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: classification Sollte man das nicht auch für den einen oder anderen auffälligen Fan\n",
      "einführen</s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "<pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "<pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "<pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "<pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "<pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "<pad> <pad>\n",
      "\n",
      "Actual sentiment: problematisch</s>\n",
      "Predicted sentiment: <pad> unproblematisch\n",
      "=====================================================================\n",
      "\n",
      "Review: classification Wir sollten ganz schnell den eisernen Vorhang wieder hochziehen Ein Großteil\n",
      "der Osteuropäer hat aus der kommunistischen Ära nichts gelernt se können mit der ihnen geschenkten\n",
      "Freiheit einfach nicht umgehen</s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "<pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "<pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "<pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "<pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\n",
      "Actual sentiment: problematisch</s>\n",
      "Predicted sentiment: <pad> problematisch\n",
      "=====================================================================\n",
      "\n",
      "Review: classification <unk> Stoll Samstag 25 Januar 2020 1037 Uhr und nicht wie die bunte Schar der\n",
      "Opportunisten schweigt ja diese Bunte Schar zieht sich quer durch Politik wie Fäulnis durch einen\n",
      "Apfel</s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "<pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "<pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "<pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "<pad> <pad> <pad> <pad> <pad>\n",
      "\n",
      "Actual sentiment: problematisch</s>\n",
      "Predicted sentiment: <pad> problematisch\n",
      "=====================================================================\n",
      "\n",
      "Review: classification Die Grünen lassen endlich ihre Maske fallen Hinweis zur nächsten Wahl Nur die\n",
      "dümmsten Kälber wählen ihren Schlächter selber Grüne Bündnis 90 Linke braucht kein Mensch</s> <pad>\n",
      "<pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "<pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "<pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "<pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "<pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\n",
      "Actual sentiment: problematisch</s>\n",
      "Predicted sentiment: <pad> problematisch\n",
      "=====================================================================\n",
      "\n",
      "Review: classification Der Typus Fahrer ist mir bekannt ohne den Name zu kenne</s> <pad> <pad> <pad>\n",
      "<pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "<pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "<pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "<pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "<pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "<pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\n",
      "Actual sentiment: problematisch</s>\n",
      "Predicted sentiment: <pad> problematisch\n",
      "=====================================================================\n",
      "\n",
      "Review: classification Ich würde mir ja gerne europäische Smartphones mit europäischem\n",
      "Betriebssystem kaufen Gibt es aber nicht Also bleibt mir nichts anderes übrig als eines mit Andorid\n",
      "oder MicrosoftBetriebssystem zu kaufen Allerdings von großen Herstellern und keine\n",
      "Nischenprodukte</s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "<pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "<pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "<pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "<pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\n",
      "Actual sentiment: unproblematisch</s>\n",
      "Predicted sentiment: <pad> unproblematisch\n",
      "=====================================================================\n",
      "\n",
      "Review: classification Sehr geehrter Herr Clages Sie benutzen das Auto um in die Stadt zu kommen\n",
      "wieviel Protokolle haben sie schon erhalten bei deim Tempo 30 Wahn Ich habe jedenfalls keine Lust\n",
      "mehr weder mit FahrradAuto oder Bus in die Stadt Mönchengladbach zu fahren Leider werde ich alles im\n",
      "Internet kaufen Das begründet vielleicht auch die vielen Leerstände in MG und Rheydt Weiter so</s>\n",
      "<pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "<pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "<pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\n",
      "Actual sentiment: unproblematisch</s>\n",
      "Predicted sentiment: <pad> unproblematisch\n",
      "=====================================================================\n",
      "\n",
      "Review: classification Ich bin mir da nicht ganz sicher aber solltemüßte die Überschrift nicht zum\n",
      "zweiten Mal heissen</s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "<pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "<pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "<pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "<pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "<pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\n",
      "Actual sentiment: unproblematisch</s>\n",
      "Predicted sentiment: <pad> problematisch\n",
      "=====================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(8):\n",
    "    lines = textwrap.wrap(\"Review:\\n%s\\n\" % texts[i], width=100)\n",
    "    print(\"\\n\".join(lines))\n",
    "    print(\"\\nActual sentiment: %s\" % targets[i])\n",
    "    print(\"Predicted sentiment: %s\" % dec[i])\n",
    "    print(\"=====================================================================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "962d8f882d0a4e5f856f216262f67bfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loader = DataLoader(dataset, batch_size=32, num_workers=4)\n",
    "model.model.eval()\n",
    "outputs = []\n",
    "targets = []\n",
    "for batch in tqdm(loader):\n",
    "  # outs = model.model.generate(input_ids=batch['source_ids'].cuda(), \n",
    "  #                             attention_mask=batch['source_mask'].cuda(), \n",
    "  #                             max_length=2)\n",
    "\n",
    "  outs = model.model.generate(input_ids=batch['source_ids'], \n",
    "                              attention_mask=batch['source_mask'], \n",
    "                              max_length=2)\n",
    "\n",
    "  dec = [tokenizer.decode(ids) for ids in outs]\n",
    "  target = [tokenizer.decode(ids) for ids in batch[\"target_ids\"]]\n",
    "  \n",
    "  outputs.extend(dec)\n",
    "  targets.extend(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1000)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_outputs = [s[6:] for s in outputs]\n",
    "new_targets = [s[:-4] for s in targets]\n",
    "\n",
    "len(new_outputs), len(new_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, out in enumerate(new_outputs):\n",
    "  if out not in ['problematisch', 'unproblematisch']:\n",
    "    print(i, 'detected invalid prediction')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.688"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(new_targets, new_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12282"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "  problematisch       0.67      0.82      0.74       530\n",
      "unproblematisch       0.73      0.53      0.62       470\n",
      "\n",
      "       accuracy                           0.69      1000\n",
      "      macro avg       0.70      0.68      0.68      1000\n",
      "   weighted avg       0.70      0.69      0.68      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(new_targets, new_outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "984131e02ccb42938a3717fb48967b49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.736\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "  problematisch       0.74      0.75      0.74       512\n",
      "unproblematisch       0.73      0.72      0.73       488\n",
      "\n",
      "       accuracy                           0.74      1000\n",
      "      macro avg       0.74      0.74      0.74      1000\n",
      "   weighted avg       0.74      0.74      0.74      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def compute_accuracy(dataset, model):\n",
    "    loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    it = iter(loader)\n",
    "\n",
    "    batch = next(it)\n",
    "    batch[\"source_ids\"].shape\n",
    "\n",
    "    model.model.eval()\n",
    "    outputs = []\n",
    "    targets = []\n",
    "    for batch in tqdm(loader):\n",
    "\n",
    "        outs = model.model.generate(input_ids=batch['source_ids'], \n",
    "                                    attention_mask=batch['source_mask'], \n",
    "                                    max_length=2)\n",
    "\n",
    "        dec = [tokenizer.decode(ids) for ids in outs]\n",
    "        target = [tokenizer.decode(ids) for ids in batch[\"target_ids\"]]\n",
    "        \n",
    "        outputs.extend(dec)\n",
    "        targets.extend(target)\n",
    "    \n",
    "    new_outputs = [s[6:] for s in outputs]\n",
    "    new_targets = [s[:-4] for s in targets]\n",
    "\n",
    "    for i, out in enumerate(new_outputs):\n",
    "        if out not in ['problematisch', 'unproblematisch']:\n",
    "            print(i, 'detected invalid prediction')\n",
    "    \n",
    "    print(metrics.accuracy_score(new_targets, new_outputs))\n",
    "\n",
    "    print(metrics.classification_report(new_targets, new_outputs))\n",
    "\n",
    "    return new_targets, new_outputs\n",
    "\n",
    "\n",
    "targets, outputs = compute_accuracy(test_dataset, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_curve = metrics.precision_recall_curve(targets, outputs, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b4120961d704338a9d22edc1a00b05b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.736\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "  problematisch       0.74      0.75      0.74       512\n",
      "unproblematisch       0.73      0.72      0.73       488\n",
      "\n",
      "       accuracy                           0.74      1000\n",
      "      macro avg       0.74      0.74      0.74      1000\n",
      "   weighted avg       0.74      0.74      0.74      1000\n",
      "\n"
     ]
    },
    {
     "ename": "UFuncTypeError",
     "evalue": "ufunc 'subtract' did not contain a loop with signature matching types (dtype('<U15'), dtype('<U15')) -> None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUFuncTypeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\isado\\transformersModerat\\Training\\T5\\t5_fine_tuning_rp_mod.ipynb Cell 44'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/c%3A/Users/isado/transformersModerat/Training/T5/t5_fine_tuning_rp_mod.ipynb#ch0000043?line=0'>1</a>\u001b[0m compute_accuracy(test_dataset, model)\n",
      "\u001b[1;32mc:\\Users\\isado\\transformersModerat\\Training\\T5\\t5_fine_tuning_rp_mod.ipynb Cell 43'\u001b[0m in \u001b[0;36mcompute_accuracy\u001b[0;34m(dataset, model)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/isado/transformersModerat/Training/T5/t5_fine_tuning_rp_mod.ipynb#ch0000042?line=30'>31</a>\u001b[0m \u001b[39mprint\u001b[39m(metrics\u001b[39m.\u001b[39maccuracy_score(new_targets, new_outputs))\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/isado/transformersModerat/Training/T5/t5_fine_tuning_rp_mod.ipynb#ch0000042?line=32'>33</a>\u001b[0m \u001b[39mprint\u001b[39m(metrics\u001b[39m.\u001b[39mclassification_report(new_targets, new_outputs))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/c%3A/Users/isado/transformersModerat/Training/T5/t5_fine_tuning_rp_mod.ipynb#ch0000042?line=34'>35</a>\u001b[0m \u001b[39mreturn\u001b[39;00m metrics\u001b[39m.\u001b[39;49mprecision_recall_curve(new_targets, new_outputs, pos_label\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mproblematisch\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_ranking.py:858\u001b[0m, in \u001b[0;36mprecision_recall_curve\u001b[0;34m(y_true, probas_pred, pos_label, sample_weight)\u001b[0m\n\u001b[1;32m    779\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprecision_recall_curve\u001b[39m(y_true, probas_pred, \u001b[39m*\u001b[39m, pos_label\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    780\u001b[0m     \u001b[39m\"\"\"Compute precision-recall pairs for different probability thresholds.\u001b[39;00m\n\u001b[1;32m    781\u001b[0m \n\u001b[1;32m    782\u001b[0m \u001b[39m    Note: this implementation is restricted to the binary classification task.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    856\u001b[0m \n\u001b[1;32m    857\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 858\u001b[0m     fps, tps, thresholds \u001b[39m=\u001b[39m _binary_clf_curve(\n\u001b[1;32m    859\u001b[0m         y_true, probas_pred, pos_label\u001b[39m=\u001b[39;49mpos_label, sample_weight\u001b[39m=\u001b[39;49msample_weight\n\u001b[1;32m    860\u001b[0m     )\n\u001b[1;32m    862\u001b[0m     precision \u001b[39m=\u001b[39m tps \u001b[39m/\u001b[39m (tps \u001b[39m+\u001b[39m fps)\n\u001b[1;32m    863\u001b[0m     precision[np\u001b[39m.\u001b[39misnan(precision)] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_ranking.py:765\u001b[0m, in \u001b[0;36m_binary_clf_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[1;32m    760\u001b[0m     weight \u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m\n\u001b[1;32m    762\u001b[0m \u001b[39m# y_score typically has many tied values. Here we extract\u001b[39;00m\n\u001b[1;32m    763\u001b[0m \u001b[39m# the indices associated with the distinct values. We also\u001b[39;00m\n\u001b[1;32m    764\u001b[0m \u001b[39m# concatenate a value for the end of the curve.\u001b[39;00m\n\u001b[0;32m--> 765\u001b[0m distinct_value_indices \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mwhere(np\u001b[39m.\u001b[39;49mdiff(y_score))[\u001b[39m0\u001b[39m]\n\u001b[1;32m    766\u001b[0m threshold_idxs \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mr_[distinct_value_indices, y_true\u001b[39m.\u001b[39msize \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m]\n\u001b[1;32m    768\u001b[0m \u001b[39m# accumulate the true positives with decreasing threshold\u001b[39;00m\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdiff\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/numpy/lib/function_base.py:1423\u001b[0m, in \u001b[0;36mdiff\u001b[0;34m(a, n, axis, prepend, append)\u001b[0m\n\u001b[1;32m   1421\u001b[0m op \u001b[39m=\u001b[39m not_equal \u001b[39mif\u001b[39;00m a\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m np\u001b[39m.\u001b[39mbool_ \u001b[39melse\u001b[39;00m subtract\n\u001b[1;32m   1422\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n):\n\u001b[0;32m-> 1423\u001b[0m     a \u001b[39m=\u001b[39m op(a[slice1], a[slice2])\n\u001b[1;32m   1425\u001b[0m \u001b[39mreturn\u001b[39;00m a\n",
      "\u001b[0;31mUFuncTypeError\u001b[0m: ufunc 'subtract' did not contain a loop with signature matching types (dtype('<U15'), dtype('<U15')) -> None"
     ]
    }
   ],
   "source": [
    "compute_accuracy(test_dataset, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt to Retrieve Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_callback.best_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go to this link: https://pytorch-lightning.readthedocs.io/en/stable/common/checkpointing.html\n",
    "\n",
    "new_model = T5FineTuner.load_from_checkpoint(checkpoint_path=\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

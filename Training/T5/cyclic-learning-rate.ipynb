{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jun 23 12:19:00 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.103.01   Driver Version: 470.103.01   CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Quadro RTX 6000     Off  | 00000000:D8:00.0 Off |                  Off |\n",
      "| 35%   41C    P8     7W / 260W |  16622MiB / 24220MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mDatasets\u001b[0m/                                 \u001b[01;34mnltk_data\u001b[0m/\n",
      "\u001b[01;34mGermanT5-RP-Mod\u001b[0m/                          \u001b[01;34mray_results\u001b[0m/\n",
      "\u001b[01;34mRP-Mod\u001b[0m/                                   \u001b[01;34mresults\u001b[0m/\n",
      "\u001b[01;34mRP-Mod-GermanT5-oscar-german-small-el32\u001b[0m/  \u001b[01;34mt5_base_imdb_sentiment\u001b[0m/\n",
      "T5FineTuner.py                            \u001b[01;34mt5_german_small_rp_mod\u001b[0m/\n",
      "\u001b[01;34m__pycache__\u001b[0m/                              \u001b[01;34mt5_german_small_rp_mod_2\u001b[0m/\n",
      "\u001b[01;34maclImdb\u001b[0m/                                  \u001b[01;34mt5_german_small_rp_mod_3\u001b[0m/\n",
      "aclImdb_v1.tar.gz                         \u001b[01;34mt5_imdb_sentiment\u001b[0m/\n",
      "\u001b[01;34marguments_test_dir\u001b[0m/                       utils-0.py\n",
      "create_t5_embeddings.ipynb                utils.ipynb\n",
      "import-notebook-as-module.py              \u001b[01;34mwandb\u001b[0m/\n",
      "\u001b[01;34mlightning_logs\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\isado\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import fine_tuning_t5_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\isado\\transformersModerat\\Training\\T5\\cyclic-learning-rate.ipynb Cell 2'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/isado/transformersModerat/Training/T5/cyclic-learning-rate.ipynb#ch0000005?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mutils\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jun 23 11:08:09 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 511.79       Driver Version: 511.79       CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0  On |                  N/A |\n",
      "| N/A   52C    P8    19W /  N/A |    219MiB /  8192MiB |      6%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1760    C+G                                   N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/dobby/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "[nltk_data] Downloading package punkt to /home/dobby/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from T5FineTuner import T5FineTuner, RPDataset\n",
    "from utils import get_folds\n",
    "import torch\n",
    "import argparse\n",
    "from transformers import T5Tokenizer\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import wandb\n",
    "\n",
    "DATASET = \"RP-Crowd-3\"\n",
    "MODEL_NAME_OR_PATH = \"GermanT5/t5-efficient-oscar-german-small-el32\"\n",
    "WANDB_PROJECT_NAME = \"rp-crowd-3-folds-cyclic-learning-t5-efficient-small-el32\"\n",
    "OUTPUT_DIR = \"./GermanT5-RP-Mod/t5-efficient-oscar-german-small-el32/\"\n",
    "SOURCE = f\"./Datasets/{DATASET}-folds.csv\"\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME_OR_PATH)\n",
    "train_inputs, train_targets, val_inputs, val_targets = get_folds(SOURCE)\n",
    "\n",
    "train_dataset = RPDataset(tokenizer, train_inputs, train_targets)\n",
    "valid_dataset = RPDataset(tokenizer, val_inputs, val_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/dobby/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import random\n",
    "import re\n",
    "from itertools import chain\n",
    "from string import punctuation\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import csv\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# eval packages\n",
    "import textwrap\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn import metrics\n",
    "\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from transformers import (\n",
    "    # AdamW,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "\n",
    "def set_seed(seed):\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  torch.manual_seed(seed)\n",
    "  if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "\n",
    "\n",
    "class T5FineTuner(pl.LightningModule):\n",
    "  def __init__(self, hparams):\n",
    "    super(T5FineTuner, self).__init__()\n",
    "    # self.hparams.save_hyperparameters(hparams)\n",
    "    self.save_hyperparameters(hparams)\n",
    "    \n",
    "    self.model = T5ForConditionalGeneration.from_pretrained(hparams.model_name_or_path)\n",
    "    self.tokenizer = T5Tokenizer.from_pretrained(hparams.tokenizer_name_or_path)\n",
    "    self.train_dataset = self.hparams.train_dataset\n",
    "    self.val_dataset = self.hparams.val_dataset\n",
    "    \n",
    "  def is_logger(self):\n",
    "    return self.trainer.global_rank <= 0\n",
    "  \n",
    "\n",
    "  def forward(\n",
    "      self, input_ids, attention_mask=None, decoder_input_ids=None, decoder_attention_mask=None, labels=None\n",
    "  ):\n",
    "    return self.model(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        decoder_input_ids=decoder_input_ids,\n",
    "        decoder_attention_mask=decoder_attention_mask,\n",
    "        labels=labels,\n",
    "        # lm_labels=lm_labels,\n",
    "    )\n",
    "\n",
    "  def _step(self, batch):\n",
    "    labels = batch[\"target_ids\"]\n",
    "    labels[labels[:, :] == self.tokenizer.pad_token_id] = -100\n",
    "\n",
    "    outputs = self(\n",
    "        input_ids=batch[\"source_ids\"],\n",
    "        attention_mask=batch[\"source_mask\"],\n",
    "        labels=labels,\n",
    "        # lm_labels=lm_labels,\n",
    "        decoder_attention_mask=batch['target_mask']\n",
    "    )\n",
    "\n",
    "    loss = outputs[0]\n",
    "\n",
    "    return loss\n",
    "  \n",
    "  def get_accuracy(self, batch):\n",
    "    labels = batch[\"target_ids\"]\n",
    "\n",
    "    outs = self.model.generate(input_ids=batch[\"source_ids\"], \n",
    "                                attention_mask=batch[\"source_mask\"], \n",
    "                                max_length=2)\n",
    "\n",
    "    dec = [self.tokenizer.decode(ids) for ids in outs]\n",
    "    target = [self.tokenizer.decode(label) for label in labels]\n",
    "\n",
    "    new_outputs = [s[6:] for s in dec]\n",
    "    new_targets = [s[:-4] for s in target]\n",
    "\n",
    "    accuracy_score = metrics.accuracy_score(new_targets, new_outputs)\n",
    "    f1 = metrics.f1_score(new_targets, new_outputs, average=\"micro\")\n",
    "    # rec = metrics.recall_score(new_targets, new_outputs, average=\"micros\")\n",
    "    return accuracy_score, f1\n",
    "  # def computer_accuracy(self):\n",
    "  #   self.model.model.eval()\n",
    "\n",
    "  def training_step(self, batch, batch_idx):\n",
    "    loss = self._step(batch)\n",
    "    self.log(\"train_loss\", loss)\n",
    "    return {\"loss\": loss}\n",
    "  \n",
    "  def training_epoch_end(self, outputs):\n",
    "    avg_train_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n",
    "    tensorboard_logs = {\"avg_train_loss\": avg_train_loss}\n",
    "    self.log(\"avg_train_loss\", avg_train_loss)\n",
    "    \n",
    "  def validation_step(self, batch, batch_idx):\n",
    "    loss = self._step(batch)\n",
    "    accuracy, f1 = torch.tensor(self.get_accuracy(batch))\n",
    "    self.log(\"val_loss\", loss, logger=True)\n",
    "    self.log(\"val_accuracy\", accuracy, logger=True)\n",
    "    return {\"val_loss\": loss, \"val_accuracy\": accuracy}\n",
    "  \n",
    "  def validation_epoch_end(self, outputs):\n",
    "    avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "    avg_accuracy = torch.stack([x[\"val_accuracy\"] for x in outputs]).mean()\n",
    "    # tensorboard_logs = {\"val_loss\": avg_loss}\n",
    "    self.log(\"avg_val_loss\", avg_loss)\n",
    "    self.log(\"avg_val_accuracy\", avg_accuracy)\n",
    "    # self.log(\"log\", tensorboard_logs)\n",
    "    # self.log(\"progress_bar\", tensorboard_logs)\n",
    "    # self.log({\"avg_val_loss\": avg_loss, \n",
    "    #           \"log\": tensorboard_logs,\n",
    "    #           'progress_bar': tensorboard_logs}, logger=True, prog_bar=True)\n",
    "    return {\"avg_val_loss\": avg_loss, \"avg_val_accuracy\": avg_accuracy}\n",
    "\n",
    "  def configure_optimizers(self):\n",
    "    \"Prepare optimizer and schedule (linear warmup and decay)\"\n",
    "\n",
    "    model = self.model\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": self.hparams.weight_decay,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=self.hparams.learning_rate, eps=self.hparams.adam_epsilon)\n",
    "    self.opt = optimizer\n",
    "    # lr_schedulers = {\"scheduler\": }\n",
    "    return [optimizer]\n",
    "  \n",
    "  def optimizer_step(self, \n",
    "                      epoch,\n",
    "                      batch_idx,\n",
    "                      optimizer,\n",
    "                      optimizer_idx,\n",
    "                      second_order_closure=None,\n",
    "                      on_tpu=None,\n",
    "                      using_native_amp=None,\n",
    "                      using_lbfgs=None):\n",
    "    # if self.trainer.use_tpu:\n",
    "    #   xm.optimizer_step(optimizer)\n",
    "    # else:\n",
    "    optimizer.step(closure=second_order_closure)\n",
    "    optimizer.zero_grad()\n",
    "    self.lr_scheduler.step()\n",
    "  \n",
    "  def get_tqdm_dict(self):\n",
    "    tqdm_dict = {\"loss\": \"{:.3f}\".format(self.trainer.avg_loss), \"lr\": self.lr_scheduler.get_last_lr()[-1]}\n",
    "\n",
    "    return tqdm_dict\n",
    "\n",
    "  def train_dataloader(self):\n",
    "    # train_dataset = get_dataset(tokenizer=self.tokenizer, type_path=\"train\", args=self.hparams)\n",
    "    dataloader = DataLoader(self.hparams.train_dataset, batch_size=self.hparams.train_batch_size, drop_last=True, shuffle=True, num_workers=4)\n",
    "    t_total = (\n",
    "        (len(dataloader.dataset) // (self.hparams.train_batch_size * max(1, self.hparams.n_gpu)))\n",
    "        // self.hparams.gradient_accumulation_steps\n",
    "        * float(self.hparams.num_train_epochs)\n",
    "    )\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        self.opt, num_warmup_steps=self.hparams.warmup_ratio * t_total, num_training_steps=t_total\n",
    "    )\n",
    "    # cyclic learning rate scheduler\n",
    "    # up = len(self.train_dataset)//(self.hparams.train_batch_size * self.hparams.gradient_accumulation_steps)\n",
    "    # scheduler = torch.optim.lr_scheduler.CyclicLR(self.opt, base_lr=4e-5, max_lr=4e-3, step_size_up=up, cycle_momentum=False)\n",
    "    \n",
    "    self.lr_scheduler = scheduler\n",
    "    return dataloader\n",
    "\n",
    "  def val_dataloader(self):\n",
    "    # val_dataset = get_dataset(tokenizer=self.tokenizer, type_path=\"val\", args=self.hparams)\n",
    "    return DataLoader(self.hparams.val_dataset, batch_size=self.hparams.eval_batch_size, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/dobby/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:117: UserWarning: When using `Trainer(accumulate_grad_batches != 1)` and overriding `LightningModule.optimizer_{step,zero_grad}`, the hooks will not be called on every batch (rather, they are called on every optimization step).\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "478b1612277747cbadf1e87b1bb04dfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /home/dobby/RP-Mod/t5-efficient-oscar-german-small-el32/.lr_find_2067ec90-9146-4ff9-a749-bf9de27cc522.ckpt\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = T5FineTuner(args)\n",
    "trainer = pl.Trainer(**train_params)\n",
    "\n",
    "\n",
    "lr_finder = trainer.tuner.lr_find(model)\n",
    "\n",
    "# Results can be found in\n",
    "# print(lr_finder.results)\n",
    "\n",
    "# Plot with\n",
    "fig = lr_finder.plot(suggest=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10830/3692713026.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT5FineTuner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtrain_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "model = T5FineTuner(args)\n",
    "trainer = pl.Trainer(**train_params)\n",
    "\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fe211e19e704544aca95ef423aa1eb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">cyclic-learning_rate-no-monitor-weight_decay-0.1</strong>: <a href=\"https://wandb.ai/isadoraw/rp-crowd-3-folds-cyclic-learning-t5-efficient-small-el32/runs/3l7ffffo\" target=\"_blank\">https://wandb.ai/isadoraw/rp-crowd-3-folds-cyclic-learning-t5-efficient-small-el32/runs/3l7ffffo</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220623_160954-3l7ffffo/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.19 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.18"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/dobby/wandb/run-20220623_161456-3khmleq1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/isadoraw/rp-crowd-3-folds-cyclic-learning-t5-efficient-small-el32/runs/3khmleq1\" target=\"_blank\">cyclic-learning_rate-no-monitor-weight_decay-0.1</a></strong> to <a href=\"https://wandb.ai/isadoraw/rp-crowd-3-folds-cyclic-learning-t5-efficient-small-el32\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/dobby/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:117: UserWarning: When using `Trainer(accumulate_grad_batches != 1)` and overriding `LightningModule.optimizer_{step,zero_grad}`, the hooks will not be called on every batch (rather, they are called on every optimization step).\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                       | Params\n",
      "-----------------------------------------------------\n",
      "0 | model | T5ForConditionalGeneration | 142 M \n",
      "-----------------------------------------------------\n",
      "142 M     Trainable params\n",
      "0         Non-trainable params\n",
      "142 M     Total params\n",
      "569.289   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e66871f7686749368fe1d2ac9e464fd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9f88909c58f45f8a5c4d76ff77c616b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59983c42a7da4b1f8bc3d20bc8db4d9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96eea60cb8e54056b09184b83ec0bdb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "680c49683d404639b5ef19f8f2fd788c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "441a62c0325940fb9e50bbd4284ee680",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eab257cb0e544099ebf350749ee4821",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50df3c871718412abaea426b070c57fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "522563f914e243489661c2e8dbf3bd2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26a7bc484888493e8cdb767f9a0d13cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91479084bade47248825e13bb3b3a4e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd356c89ab1a4c7a98875f011fbd2917",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd131e125a97438486bc9f43765bd5af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70f84b45711a4d69a52605bfe4d38629",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2999ce88a1024320bbbc3d17887a621f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d3af9a9d4244d8b94ea00407a3cc04e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8a3d562b4224e9794cf377214ccfaa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "wd = 0.1\n",
    "lr = 4e-4\n",
    "args_dict = dict(\n",
    "            data_dir=\"\", # path for data files\n",
    "            output_dir=f\"./GermanT5-RP-Mod/t5-efficient-oscar-german-small-el32/\", # path to save the checkpoints\n",
    "            model_name_or_path=\"GermanT5/t5-efficient-oscar-german-small-el32\",\n",
    "            tokenizer_name_or_path=\"GermanT5/t5-efficient-oscar-german-small-el32\",\n",
    "            dataset_name=\"RP-Mod\",\n",
    "            max_seq_length=512,\n",
    "            learning_rate=lr,\n",
    "            weight_decay=wd,\n",
    "            adam_epsilon=1e-8,\n",
    "            # warmup_steps=100,\n",
    "            warmup_ratio=0.3,\n",
    "            train_batch_size=8,\n",
    "            eval_batch_size=8,\n",
    "            num_train_epochs=15,\n",
    "            gradient_accumulation_steps=16,\n",
    "            n_gpu=1,\n",
    "            early_stop_callback=False,\n",
    "            fp_16=False, # if you want to enable 16-bit training then install apex and set this to true\n",
    "            opt_level='O1', # you can find out more on optimisation levels here https://nvidia.github.io/apex/amp.html#opt-levels-and-properties\n",
    "            max_grad_norm=0.5, # if you enable 16-bit training then set this to a sensible value, 0.5 is a good default\n",
    "            seed=42,\n",
    "            train_dataset=train_dataset, \n",
    "            val_dataset=valid_dataset\n",
    "        )\n",
    "\n",
    "args = argparse.Namespace(**args_dict)\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "            dirpath=OUTPUT_DIR + f\"warm-up-{args.warmup_ratio}-wd-{wd}\", filename=\"{epoch}-{val_accuracy:.2f}-{val_loss:.2f}\", monitor=\"val_accuracy\", mode=\"max\", save_top_k=5\n",
    "        )\n",
    "\n",
    "wandb.finish()\n",
    "wandb_logger = WandbLogger(project=WANDB_PROJECT_NAME, \n",
    "name=f\"cyclic-learning_rate-no-monitor-weight_decay-{wd}\")\n",
    "\n",
    "train_params = dict(\n",
    "            accumulate_grad_batches=args.gradient_accumulation_steps,\n",
    "            auto_lr_find=True,\n",
    "            gpus=args.n_gpu,\n",
    "            max_epochs=args.num_train_epochs,\n",
    "            default_root_dir=f\"/home/dobby/RP-Mod/t5-efficient-oscar-german-small-el32\",\n",
    "            # early_stop_callback=False,\n",
    "            precision= 16 if args.fp_16 else 32,\n",
    "            amp_level=args.opt_level,\n",
    "            gradient_clip_val=args.max_grad_norm,\n",
    "            # checkpoint_callback=checkpoint_callback,\n",
    "            logger=wandb_logger,\n",
    "            enable_checkpointing=checkpoint_callback,\n",
    "            callbacks=[\n",
    "                checkpoint_callback, \n",
    "                # LearningRateMonitor(logging_interval=\"step\")\n",
    "                ],\n",
    "            # callbacks=[raytuner_callback],\n",
    "            # callbacks=[LoggingCallback()],\n",
    "            amp_backend=\"apex\"\n",
    "        )\n",
    "\n",
    "model = T5FineTuner(args)\n",
    "trainer = pl.Trainer(**train_params)\n",
    "\n",
    "trainer.fit(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jun 23 12:24:10 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.103.01   Driver Version: 470.103.01   CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Quadro RTX 6000     Off  | 00000000:D8:00.0 Off |                  Off |\n",
      "| 35%   47C    P2    60W / 260W |  22867MiB / 24220MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "822c11fc1520dc863eb71ed954e7df7c13a4f43f6dead24a015d5e39da55e1c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
